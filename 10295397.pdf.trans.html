<!DOCTYPE html>

<head>
  <meta charset="utf-8">
  <script>
  !function(t,e){"object"==typeof exports&&"undefined"!=typeof module?module.exports=e():"function"==typeof define&&define.amd?define(e):(t="undefined"!=typeof globalThis?globalThis:t||self).Vue=e()}(this,(function(){"use strict";var t=Object.freeze({}),e=Array.isArray;function n(t){return null==t}function r(t){return null!=t}function o(t){return!0===t}function i(t){return"string"==typeof t||"number"==typeof t||"symbol"==typeof t||"boolean"==typeof t}function a(t){return"function"==typeof t}function s(t){return null!==t&&"object"==typeof t}var c=Object.prototype.toString;function u(t){return"[object Object]"===c.call(t)}function l(t){var e=parseFloat(String(t));return e>=0&&Math.floor(e)===e&&isFinite(t)}function f(t){return r(t)&&"function"==typeof t.then&&"function"==typeof t.catch}function d(t){return null==t?"":Array.isArray(t)||u(t)&&t.toString===c?JSON.stringify(t,null,2):String(t)}function p(t){var e=parseFloat(t);return isNaN(e)?t:e}function v(t,e){for(var n=Object.create(null),r=t.split(","),o=0;o<r.length;o++)n[r[o]]=!0;return e?function(t){return n[t.toLowerCase()]}:function(t){return n[t]}}var h=v("slot,component",!0),m=v("key,ref,slot,slot-scope,is");function g(t,e){var n=t.length;if(n){if(e===t[n-1])return void(t.length=n-1);var r=t.indexOf(e);if(r>-1)return t.splice(r,1)}}var y=Object.prototype.hasOwnProperty;function _(t,e){return y.call(t,e)}function b(t){var e=Object.create(null);return function(n){return e[n]||(e[n]=t(n))}}var $=/-(\w)/g,w=b((function(t){return t.replace($,(function(t,e){return e?e.toUpperCase():""}))})),x=b((function(t){return t.charAt(0).toUpperCase()+t.slice(1)})),C=/\B([A-Z])/g,k=b((function(t){return t.replace(C,"-$1").toLowerCase()}));var S=Function.prototype.bind?function(t,e){return t.bind(e)}:function(t,e){function n(n){var r=arguments.length;return r?r>1?t.apply(e,arguments):t.call(e,n):t.call(e)}return n._length=t.length,n};function O(t,e){e=e||0;for(var n=t.length-e,r=new Array(n);n--;)r[n]=t[n+e];return r}function T(t,e){for(var n in e)t[n]=e[n];return t}function A(t){for(var e={},n=0;n<t.length;n++)t[n]&&T(e,t[n]);return e}function j(t,e,n){}var E=function(t,e,n){return!1},N=function(t){return t};function P(t,e){if(t===e)return!0;var n=s(t),r=s(e);if(!n||!r)return!n&&!r&&String(t)===String(e);try{var o=Array.isArray(t),i=Array.isArray(e);if(o&&i)return t.length===e.length&&t.every((function(t,n){return P(t,e[n])}));if(t instanceof Date&&e instanceof Date)return t.getTime()===e.getTime();if(o||i)return!1;var a=Object.keys(t),c=Object.keys(e);return a.length===c.length&&a.every((function(n){return P(t[n],e[n])}))}catch(t){return!1}}function D(t,e){for(var n=0;n<t.length;n++)if(P(t[n],e))return n;return-1}function M(t){var e=!1;return function(){e||(e=!0,t.apply(this,arguments))}}function I(t,e){return t===e?0===t&&1/t!=1/e:t==t||e==e}var L="data-server-rendered",R=["component","directive","filter"],F=["beforeCreate","created","beforeMount","mounted","beforeUpdate","updated","beforeDestroy","destroyed","activated","deactivated","errorCaptured","serverPrefetch","renderTracked","renderTriggered"],H={optionMergeStrategies:Object.create(null),silent:!1,productionTip:!1,devtools:!1,performance:!1,errorHandler:null,warnHandler:null,ignoredElements:[],keyCodes:Object.create(null),isReservedTag:E,isReservedAttr:E,isUnknownElement:E,getTagNamespace:j,parsePlatformTagName:N,mustUseProp:E,async:!0,_lifecycleHooks:F},B=/a-zA-Z\u00B7\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u037D\u037F-\u1FFF\u200C-\u200D\u203F-\u2040\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD/;function U(t){var e=(t+"").charCodeAt(0);return 36===e||95===e}function z(t,e,n,r){Object.defineProperty(t,e,{value:n,enumerable:!!r,writable:!0,configurable:!0})}var V=new RegExp("[^".concat(B.source,".$_\\d]"));var K="__proto__"in{},J="undefined"!=typeof window,q=J&&window.navigator.userAgent.toLowerCase(),W=q&&/msie|trident/.test(q),Z=q&&q.indexOf("msie 9.0")>0,G=q&&q.indexOf("edge/")>0;q&&q.indexOf("android");var X=q&&/iphone|ipad|ipod|ios/.test(q);q&&/chrome\/\d+/.test(q),q&&/phantomjs/.test(q);var Y,Q=q&&q.match(/firefox\/(\d+)/),tt={}.watch,et=!1;if(J)try{var nt={};Object.defineProperty(nt,"passive",{get:function(){et=!0}}),window.addEventListener("test-passive",null,nt)}catch(t){}var rt=function(){return void 0===Y&&(Y=!J&&"undefined"!=typeof global&&(global.process&&"server"===global.process.env.VUE_ENV)),Y},ot=J&&window.__VUE_DEVTOOLS_GLOBAL_HOOK__;function it(t){return"function"==typeof t&&/native code/.test(t.toString())}var at,st="undefined"!=typeof Symbol&&it(Symbol)&&"undefined"!=typeof Reflect&&it(Reflect.ownKeys);at="undefined"!=typeof Set&&it(Set)?Set:function(){function t(){this.set=Object.create(null)}return t.prototype.has=function(t){return!0===this.set[t]},t.prototype.add=function(t){this.set[t]=!0},t.prototype.clear=function(){this.set=Object.create(null)},t}();var ct=null;function ut(t){void 0===t&&(t=null),t||ct&&ct._scope.off(),ct=t,t&&t._scope.on()}var lt=function(){function t(t,e,n,r,o,i,a,s){this.tag=t,this.data=e,this.children=n,this.text=r,this.elm=o,this.ns=void 0,this.context=i,this.fnContext=void 0,this.fnOptions=void 0,this.fnScopeId=void 0,this.key=e&&e.key,this.componentOptions=a,this.componentInstance=void 0,this.parent=void 0,this.raw=!1,this.isStatic=!1,this.isRootInsert=!0,this.isComment=!1,this.isCloned=!1,this.isOnce=!1,this.asyncFactory=s,this.asyncMeta=void 0,this.isAsyncPlaceholder=!1}return Object.defineProperty(t.prototype,"child",{get:function(){return this.componentInstance},enumerable:!1,configurable:!0}),t}(),ft=function(t){void 0===t&&(t="");var e=new lt;return e.text=t,e.isComment=!0,e};function dt(t){return new lt(void 0,void 0,void 0,String(t))}function pt(t){var e=new lt(t.tag,t.data,t.children&&t.children.slice(),t.text,t.elm,t.context,t.componentOptions,t.asyncFactory);return e.ns=t.ns,e.isStatic=t.isStatic,e.key=t.key,e.isComment=t.isComment,e.fnContext=t.fnContext,e.fnOptions=t.fnOptions,e.fnScopeId=t.fnScopeId,e.asyncMeta=t.asyncMeta,e.isCloned=!0,e}var vt=0,ht=[],mt=function(){function t(){this._pending=!1,this.id=vt++,this.subs=[]}return t.prototype.addSub=function(t){this.subs.push(t)},t.prototype.removeSub=function(t){this.subs[this.subs.indexOf(t)]=null,this._pending||(this._pending=!0,ht.push(this))},t.prototype.depend=function(e){t.target&&t.target.addDep(this)},t.prototype.notify=function(t){for(var e=this.subs.filter((function(t){return t})),n=0,r=e.length;n<r;n++){e[n].update()}},t}();mt.target=null;var gt=[];function yt(t){gt.push(t),mt.target=t}function _t(){gt.pop(),mt.target=gt[gt.length-1]}var bt=Array.prototype,$t=Object.create(bt);["push","pop","shift","unshift","splice","sort","reverse"].forEach((function(t){var e=bt[t];z($t,t,(function(){for(var n=[],r=0;r<arguments.length;r++)n[r]=arguments[r];var o,i=e.apply(this,n),a=this.__ob__;switch(t){case"push":case"unshift":o=n;break;case"splice":o=n.slice(2)}return o&&a.observeArray(o),a.dep.notify(),i}))}));var wt=Object.getOwnPropertyNames($t),xt={},Ct=!0;function kt(t){Ct=t}var St={notify:j,depend:j,addSub:j,removeSub:j},Ot=function(){function t(t,n,r){if(void 0===n&&(n=!1),void 0===r&&(r=!1),this.value=t,this.shallow=n,this.mock=r,this.dep=r?St:new mt,this.vmCount=0,z(t,"__ob__",this),e(t)){if(!r)if(K)t.__proto__=$t;else for(var o=0,i=wt.length;o<i;o++){z(t,s=wt[o],$t[s])}n||this.observeArray(t)}else{var a=Object.keys(t);for(o=0;o<a.length;o++){var s;At(t,s=a[o],xt,void 0,n,r)}}}return t.prototype.observeArray=function(t){for(var e=0,n=t.length;e<n;e++)Tt(t[e],!1,this.mock)},t}();function Tt(t,n,r){return t&&_(t,"__ob__")&&t.__ob__ instanceof Ot?t.__ob__:!Ct||!r&&rt()||!e(t)&&!u(t)||!Object.isExtensible(t)||t.__v_skip||Ft(t)||t instanceof lt?void 0:new Ot(t,n,r)}function At(t,n,r,o,i,a){var s=new mt,c=Object.getOwnPropertyDescriptor(t,n);if(!c||!1!==c.configurable){var u=c&&c.get,l=c&&c.set;u&&!l||r!==xt&&2!==arguments.length||(r=t[n]);var f=!i&&Tt(r,!1,a);return Object.defineProperty(t,n,{enumerable:!0,configurable:!0,get:function(){var n=u?u.call(t):r;return mt.target&&(s.depend(),f&&(f.dep.depend(),e(n)&&Nt(n))),Ft(n)&&!i?n.value:n},set:function(e){var n=u?u.call(t):r;if(I(n,e)){if(l)l.call(t,e);else{if(u)return;if(!i&&Ft(n)&&!Ft(e))return void(n.value=e);r=e}f=!i&&Tt(e,!1,a),s.notify()}}}),s}}function jt(t,n,r){if(!Lt(t)){var o=t.__ob__;return e(t)&&l(n)?(t.length=Math.max(t.length,n),t.splice(n,1,r),o&&!o.shallow&&o.mock&&Tt(r,!1,!0),r):n in t&&!(n in Object.prototype)?(t[n]=r,r):t._isVue||o&&o.vmCount?r:o?(At(o.value,n,r,void 0,o.shallow,o.mock),o.dep.notify(),r):(t[n]=r,r)}}function Et(t,n){if(e(t)&&l(n))t.splice(n,1);else{var r=t.__ob__;t._isVue||r&&r.vmCount||Lt(t)||_(t,n)&&(delete t[n],r&&r.dep.notify())}}function Nt(t){for(var n=void 0,r=0,o=t.length;r<o;r++)(n=t[r])&&n.__ob__&&n.__ob__.dep.depend(),e(n)&&Nt(n)}function Pt(t){return Dt(t,!0),z(t,"__v_isShallow",!0),t}function Dt(t,e){Lt(t)||Tt(t,e,rt())}function Mt(t){return Lt(t)?Mt(t.__v_raw):!(!t||!t.__ob__)}function It(t){return!(!t||!t.__v_isShallow)}function Lt(t){return!(!t||!t.__v_isReadonly)}var Rt="__v_isRef";function Ft(t){return!(!t||!0!==t.__v_isRef)}function Ht(t,e){if(Ft(t))return t;var n={};return z(n,Rt,!0),z(n,"__v_isShallow",e),z(n,"dep",At(n,"value",t,null,e,rt())),n}function Bt(t,e,n){Object.defineProperty(t,n,{enumerable:!0,configurable:!0,get:function(){var t=e[n];if(Ft(t))return t.value;var r=t&&t.__ob__;return r&&r.dep.depend(),t},set:function(t){var r=e[n];Ft(r)&&!Ft(t)?r.value=t:e[n]=t}})}function Ut(t,e,n){var r=t[e];if(Ft(r))return r;var o={get value(){var r=t[e];return void 0===r?n:r},set value(n){t[e]=n}};return z(o,Rt,!0),o}function zt(t){return Vt(t,!1)}function Vt(t,e){if(!u(t))return t;if(Lt(t))return t;var n=e?"__v_rawToShallowReadonly":"__v_rawToReadonly",r=t[n];if(r)return r;var o=Object.create(Object.getPrototypeOf(t));z(t,n,o),z(o,"__v_isReadonly",!0),z(o,"__v_raw",t),Ft(t)&&z(o,Rt,!0),(e||It(t))&&z(o,"__v_isShallow",!0);for(var i=Object.keys(t),a=0;a<i.length;a++)Kt(o,t,i[a],e);return o}function Kt(t,e,n,r){Object.defineProperty(t,n,{enumerable:!0,configurable:!0,get:function(){var t=e[n];return r||!u(t)?t:zt(t)},set:function(){}})}var Jt=b((function(t){var e="&"===t.charAt(0),n="~"===(t=e?t.slice(1):t).charAt(0),r="!"===(t=n?t.slice(1):t).charAt(0);return{name:t=r?t.slice(1):t,once:n,capture:r,passive:e}}));function qt(t,n){function r(){var t=r.fns;if(!e(t))return dn(t,null,arguments,n,"v-on handler");for(var o=t.slice(),i=0;i<o.length;i++)dn(o[i],null,arguments,n,"v-on handler")}return r.fns=t,r}function Wt(t,e,r,i,a,s){var c,u,l,f;for(c in t)u=t[c],l=e[c],f=Jt(c),n(u)||(n(l)?(n(u.fns)&&(u=t[c]=qt(u,s)),o(f.once)&&(u=t[c]=a(f.name,u,f.capture)),r(f.name,u,f.capture,f.passive,f.params)):u!==l&&(l.fns=u,t[c]=l));for(c in e)n(t[c])&&i((f=Jt(c)).name,e[c],f.capture)}function Zt(t,e,i){var a;t instanceof lt&&(t=t.data.hook||(t.data.hook={}));var s=t[e];function c(){i.apply(this,arguments),g(a.fns,c)}n(s)?a=qt([c]):r(s.fns)&&o(s.merged)?(a=s).fns.push(c):a=qt([s,c]),a.merged=!0,t[e]=a}function Gt(t,e,n,o,i){if(r(e)){if(_(e,n))return t[n]=e[n],i||delete e[n],!0;if(_(e,o))return t[n]=e[o],i||delete e[o],!0}return!1}function Xt(t){return i(t)?[dt(t)]:e(t)?Qt(t):void 0}function Yt(t){return r(t)&&r(t.text)&&!1===t.isComment}function Qt(t,a){var s,c,u,l,f=[];for(s=0;s<t.length;s++)n(c=t[s])||"boolean"==typeof c||(l=f[u=f.length-1],e(c)?c.length>0&&(Yt((c=Qt(c,"".concat(a||"","_").concat(s)))[0])&&Yt(l)&&(f[u]=dt(l.text+c[0].text),c.shift()),f.push.apply(f,c)):i(c)?Yt(l)?f[u]=dt(l.text+c):""!==c&&f.push(dt(c)):Yt(c)&&Yt(l)?f[u]=dt(l.text+c.text):(o(t._isVList)&&r(c.tag)&&n(c.key)&&r(a)&&(c.key="__vlist".concat(a,"_").concat(s,"__")),f.push(c)));return f}function te(t,n,c,u,l,f){return(e(c)||i(c))&&(l=u,u=c,c=void 0),o(f)&&(l=2),function(t,n,o,i,c){if(r(o)&&r(o.__ob__))return ft();r(o)&&r(o.is)&&(n=o.is);if(!n)return ft();e(i)&&a(i[0])&&((o=o||{}).scopedSlots={default:i[0]},i.length=0);2===c?i=Xt(i):1===c&&(i=function(t){for(var n=0;n<t.length;n++)if(e(t[n]))return Array.prototype.concat.apply([],t);return t}(i));var u,l;if("string"==typeof n){var f=void 0;l=t.$vnode&&t.$vnode.ns||H.getTagNamespace(n),u=H.isReservedTag(n)?new lt(H.parsePlatformTagName(n),o,i,void 0,void 0,t):o&&o.pre||!r(f=yr(t.$options,"components",n))?new lt(n,o,i,void 0,void 0,t):cr(f,o,t,i,n)}else u=cr(n,o,t,i);return e(u)?u:r(u)?(r(l)&&ee(u,l),r(o)&&function(t){s(t.style)&&Bn(t.style);s(t.class)&&Bn(t.class)}(o),u):ft()}(t,n,c,u,l)}function ee(t,e,i){if(t.ns=e,"foreignObject"===t.tag&&(e=void 0,i=!0),r(t.children))for(var a=0,s=t.children.length;a<s;a++){var c=t.children[a];r(c.tag)&&(n(c.ns)||o(i)&&"svg"!==c.tag)&&ee(c,e,i)}}function ne(t,n){var o,i,a,c,u=null;if(e(t)||"string"==typeof t)for(u=new Array(t.length),o=0,i=t.length;o<i;o++)u[o]=n(t[o],o);else if("number"==typeof t)for(u=new Array(t),o=0;o<t;o++)u[o]=n(o+1,o);else if(s(t))if(st&&t[Symbol.iterator]){u=[];for(var l=t[Symbol.iterator](),f=l.next();!f.done;)u.push(n(f.value,u.length)),f=l.next()}else for(a=Object.keys(t),u=new Array(a.length),o=0,i=a.length;o<i;o++)c=a[o],u[o]=n(t[c],c,o);return r(u)||(u=[]),u._isVList=!0,u}function re(t,e,n,r){var o,i=this.$scopedSlots[t];i?(n=n||{},r&&(n=T(T({},r),n)),o=i(n)||(a(e)?e():e)):o=this.$slots[t]||(a(e)?e():e);var s=n&&n.slot;return s?this.$createElement("template",{slot:s},o):o}function oe(t){return yr(this.$options,"filters",t)||N}function ie(t,n){return e(t)?-1===t.indexOf(n):t!==n}function ae(t,e,n,r,o){var i=H.keyCodes[e]||n;return o&&r&&!H.keyCodes[e]?ie(o,r):i?ie(i,t):r?k(r)!==e:void 0===t}function se(t,n,r,o,i){if(r)if(s(r)){e(r)&&(r=A(r));var a=void 0,c=function(e){if("class"===e||"style"===e||m(e))a=t;else{var s=t.attrs&&t.attrs.type;a=o||H.mustUseProp(n,s,e)?t.domProps||(t.domProps={}):t.attrs||(t.attrs={})}var c=w(e),u=k(e);c in a||u in a||(a[e]=r[e],i&&((t.on||(t.on={}))["update:".concat(e)]=function(t){r[e]=t}))};for(var u in r)c(u)}else;return t}function ce(t,e){var n=this._staticTrees||(this._staticTrees=[]),r=n[t];return r&&!e||le(r=n[t]=this.$options.staticRenderFns[t].call(this._renderProxy,this._c,this),"__static__".concat(t),!1),r}function ue(t,e,n){return le(t,"__once__".concat(e).concat(n?"_".concat(n):""),!0),t}function le(t,n,r){if(e(t))for(var o=0;o<t.length;o++)t[o]&&"string"!=typeof t[o]&&fe(t[o],"".concat(n,"_").concat(o),r);else fe(t,n,r)}function fe(t,e,n){t.isStatic=!0,t.key=e,t.isOnce=n}function de(t,e){if(e)if(u(e)){var n=t.on=t.on?T({},t.on):{};for(var r in e){var o=n[r],i=e[r];n[r]=o?[].concat(o,i):i}}else;return t}function pe(t,n,r,o){n=n||{$stable:!r};for(var i=0;i<t.length;i++){var a=t[i];e(a)?pe(a,n,r):a&&(a.proxy&&(a.fn.proxy=!0),n[a.key]=a.fn)}return o&&(n.$key=o),n}function ve(t,e){for(var n=0;n<e.length;n+=2){var r=e[n];"string"==typeof r&&r&&(t[e[n]]=e[n+1])}return t}function he(t,e){return"string"==typeof t?e+t:t}function me(t){t._o=ue,t._n=p,t._s=d,t._l=ne,t._t=re,t._q=P,t._i=D,t._m=ce,t._f=oe,t._k=ae,t._b=se,t._v=dt,t._e=ft,t._u=pe,t._g=de,t._d=ve,t._p=he}function ge(t,e){if(!t||!t.length)return{};for(var n={},r=0,o=t.length;r<o;r++){var i=t[r],a=i.data;if(a&&a.attrs&&a.attrs.slot&&delete a.attrs.slot,i.context!==e&&i.fnContext!==e||!a||null==a.slot)(n.default||(n.default=[])).push(i);else{var s=a.slot,c=n[s]||(n[s]=[]);"template"===i.tag?c.push.apply(c,i.children||[]):c.push(i)}}for(var u in n)n[u].every(ye)&&delete n[u];return n}function ye(t){return t.isComment&&!t.asyncFactory||" "===t.text}function _e(t){return t.isComment&&t.asyncFactory}function be(e,n,r,o){var i,a=Object.keys(r).length>0,s=n?!!n.$stable:!a,c=n&&n.$key;if(n){if(n._normalized)return n._normalized;if(s&&o&&o!==t&&c===o.$key&&!a&&!o.$hasNormal)return o;for(var u in i={},n)n[u]&&"$"!==u[0]&&(i[u]=$e(e,r,u,n[u]))}else i={};for(var l in r)l in i||(i[l]=we(r,l));return n&&Object.isExtensible(n)&&(n._normalized=i),z(i,"$stable",s),z(i,"$key",c),z(i,"$hasNormal",a),i}function $e(t,n,r,o){var i=function(){var n=ct;ut(t);var r=arguments.length?o.apply(null,arguments):o({}),i=(r=r&&"object"==typeof r&&!e(r)?[r]:Xt(r))&&r[0];return ut(n),r&&(!i||1===r.length&&i.isComment&&!_e(i))?void 0:r};return o.proxy&&Object.defineProperty(n,r,{get:i,enumerable:!0,configurable:!0}),i}function we(t,e){return function(){return t[e]}}function xe(e){return{get attrs(){if(!e._attrsProxy){var n=e._attrsProxy={};z(n,"_v_attr_proxy",!0),Ce(n,e.$attrs,t,e,"$attrs")}return e._attrsProxy},get listeners(){e._listenersProxy||Ce(e._listenersProxy={},e.$listeners,t,e,"$listeners");return e._listenersProxy},get slots(){return function(t){t._slotsProxy||Se(t._slotsProxy={},t.$scopedSlots);return t._slotsProxy}(e)},emit:S(e.$emit,e),expose:function(t){t&&Object.keys(t).forEach((function(n){return Bt(e,t,n)}))}}}function Ce(t,e,n,r,o){var i=!1;for(var a in e)a in t?e[a]!==n[a]&&(i=!0):(i=!0,ke(t,a,r,o));for(var a in t)a in e||(i=!0,delete t[a]);return i}function ke(t,e,n,r){Object.defineProperty(t,e,{enumerable:!0,configurable:!0,get:function(){return n[r][e]}})}function Se(t,e){for(var n in e)t[n]=e[n];for(var n in t)n in e||delete t[n]}function Oe(){var t=ct;return t._setupContext||(t._setupContext=xe(t))}var Te,Ae=null;function je(t,e){return(t.__esModule||st&&"Module"===t[Symbol.toStringTag])&&(t=t.default),s(t)?e.extend(t):t}function Ee(t){if(e(t))for(var n=0;n<t.length;n++){var o=t[n];if(r(o)&&(r(o.componentOptions)||_e(o)))return o}}function Ne(t,e){Te.$on(t,e)}function Pe(t,e){Te.$off(t,e)}function De(t,e){var n=Te;return function r(){var o=e.apply(null,arguments);null!==o&&n.$off(t,r)}}function Me(t,e,n){Te=t,Wt(e,n||{},Ne,Pe,De,t),Te=void 0}var Ie=null;function Le(t){var e=Ie;return Ie=t,function(){Ie=e}}function Re(t){for(;t&&(t=t.$parent);)if(t._inactive)return!0;return!1}function Fe(t,e){if(e){if(t._directInactive=!1,Re(t))return}else if(t._directInactive)return;if(t._inactive||null===t._inactive){t._inactive=!1;for(var n=0;n<t.$children.length;n++)Fe(t.$children[n]);Be(t,"activated")}}function He(t,e){if(!(e&&(t._directInactive=!0,Re(t))||t._inactive)){t._inactive=!0;for(var n=0;n<t.$children.length;n++)He(t.$children[n]);Be(t,"deactivated")}}function Be(t,e,n,r){void 0===r&&(r=!0),yt();var o=ct;r&&ut(t);var i=t.$options[e],a="".concat(e," hook");if(i)for(var s=0,c=i.length;s<c;s++)dn(i[s],t,n||null,t,a);t._hasHookEvent&&t.$emit("hook:"+e),r&&ut(o),_t()}var Ue=[],ze=[],Ve={},Ke=!1,Je=!1,qe=0;var We=0,Ze=Date.now;if(J&&!W){var Ge=window.performance;Ge&&"function"==typeof Ge.now&&Ze()>document.createEvent("Event").timeStamp&&(Ze=function(){return Ge.now()})}var Xe=function(t,e){if(t.post){if(!e.post)return 1}else if(e.post)return-1;return t.id-e.id};function Ye(){var t,e;for(We=Ze(),Je=!0,Ue.sort(Xe),qe=0;qe<Ue.length;qe++)(t=Ue[qe]).before&&t.before(),e=t.id,Ve[e]=null,t.run();var n=ze.slice(),r=Ue.slice();qe=Ue.length=ze.length=0,Ve={},Ke=Je=!1,function(t){for(var e=0;e<t.length;e++)t[e]._inactive=!0,Fe(t[e],!0)}(n),function(t){var e=t.length;for(;e--;){var n=t[e],r=n.vm;r&&r._watcher===n&&r._isMounted&&!r._isDestroyed&&Be(r,"updated")}}(r),function(){for(var t=0;t<ht.length;t++){var e=ht[t];e.subs=e.subs.filter((function(t){return t})),e._pending=!1}ht.length=0}(),ot&&H.devtools&&ot.emit("flush")}function Qe(t){var e=t.id;if(null==Ve[e]&&(t!==mt.target||!t.noRecurse)){if(Ve[e]=!0,Je){for(var n=Ue.length-1;n>qe&&Ue[n].id>t.id;)n--;Ue.splice(n+1,0,t)}else Ue.push(t);Ke||(Ke=!0,Cn(Ye))}}var tn="watcher",en="".concat(tn," callback"),nn="".concat(tn," getter"),rn="".concat(tn," cleanup");function on(t,e){return cn(t,null,{flush:"post"})}var an,sn={};function cn(n,r,o){var i=void 0===o?t:o,s=i.immediate,c=i.deep,u=i.flush,l=void 0===u?"pre":u;i.onTrack,i.onTrigger;var f,d,p=ct,v=function(t,e,n){return void 0===n&&(n=null),dn(t,null,n,p,e)},h=!1,m=!1;if(Ft(n)?(f=function(){return n.value},h=It(n)):Mt(n)?(f=function(){return n.__ob__.dep.depend(),n},c=!0):e(n)?(m=!0,h=n.some((function(t){return Mt(t)||It(t)})),f=function(){return n.map((function(t){return Ft(t)?t.value:Mt(t)?Bn(t):a(t)?v(t,nn):void 0}))}):f=a(n)?r?function(){return v(n,nn)}:function(){if(!p||!p._isDestroyed)return d&&d(),v(n,tn,[y])}:j,r&&c){var g=f;f=function(){return Bn(g())}}var y=function(t){d=_.onStop=function(){v(t,rn)}};if(rt())return y=j,r?s&&v(r,en,[f(),m?[]:void 0,y]):f(),j;var _=new Vn(ct,f,j,{lazy:!0});_.noRecurse=!r;var b=m?[]:sn;return _.run=function(){if(_.active)if(r){var t=_.get();(c||h||(m?t.some((function(t,e){return I(t,b[e])})):I(t,b)))&&(d&&d(),v(r,en,[t,b===sn?void 0:b,y]),b=t)}else _.get()},"sync"===l?_.update=_.run:"post"===l?(_.post=!0,_.update=function(){return Qe(_)}):_.update=function(){if(p&&p===ct&&!p._isMounted){var t=p._preWatchers||(p._preWatchers=[]);t.indexOf(_)<0&&t.push(_)}else Qe(_)},r?s?_.run():b=_.get():"post"===l&&p?p.$once("hook:mounted",(function(){return _.get()})):_.get(),function(){_.teardown()}}var un=function(){function t(t){void 0===t&&(t=!1),this.detached=t,this.active=!0,this.effects=[],this.cleanups=[],this.parent=an,!t&&an&&(this.index=(an.scopes||(an.scopes=[])).push(this)-1)}return t.prototype.run=function(t){if(this.active){var e=an;try{return an=this,t()}finally{an=e}}},t.prototype.on=function(){an=this},t.prototype.off=function(){an=this.parent},t.prototype.stop=function(t){if(this.active){var e=void 0,n=void 0;for(e=0,n=this.effects.length;e<n;e++)this.effects[e].teardown();for(e=0,n=this.cleanups.length;e<n;e++)this.cleanups[e]();if(this.scopes)for(e=0,n=this.scopes.length;e<n;e++)this.scopes[e].stop(!0);if(!this.detached&&this.parent&&!t){var r=this.parent.scopes.pop();r&&r!==this&&(this.parent.scopes[this.index]=r,r.index=this.index)}this.parent=void 0,this.active=!1}},t}();function ln(t){var e=t._provided,n=t.$parent&&t.$parent._provided;return n===e?t._provided=Object.create(n):e}function fn(t,e,n){yt();try{if(e)for(var r=e;r=r.$parent;){var o=r.$options.errorCaptured;if(o)for(var i=0;i<o.length;i++)try{if(!1===o[i].call(r,t,e,n))return}catch(t){pn(t,r,"errorCaptured hook")}}pn(t,e,n)}finally{_t()}}function dn(t,e,n,r,o){var i;try{(i=n?t.apply(e,n):t.call(e))&&!i._isVue&&f(i)&&!i._handled&&(i.catch((function(t){return fn(t,r,o+" (Promise/async)")})),i._handled=!0)}catch(t){fn(t,r,o)}return i}function pn(t,e,n){if(H.errorHandler)try{return H.errorHandler.call(null,t,e,n)}catch(e){e!==t&&vn(e)}vn(t)}function vn(t,e,n){if(!J||"undefined"==typeof console)throw t;console.error(t)}var hn,mn=!1,gn=[],yn=!1;function _n(){yn=!1;var t=gn.slice(0);gn.length=0;for(var e=0;e<t.length;e++)t[e]()}if("undefined"!=typeof Promise&&it(Promise)){var bn=Promise.resolve();hn=function(){bn.then(_n),X&&setTimeout(j)},mn=!0}else if(W||"undefined"==typeof MutationObserver||!it(MutationObserver)&&"[object MutationObserverConstructor]"!==MutationObserver.toString())hn="undefined"!=typeof setImmediate&&it(setImmediate)?function(){setImmediate(_n)}:function(){setTimeout(_n,0)};else{var $n=1,wn=new MutationObserver(_n),xn=document.createTextNode(String($n));wn.observe(xn,{characterData:!0}),hn=function(){$n=($n+1)%2,xn.data=String($n)},mn=!0}function Cn(t,e){var n;if(gn.push((function(){if(t)try{t.call(e)}catch(t){fn(t,e,"nextTick")}else n&&n(e)})),yn||(yn=!0,hn()),!t&&"undefined"!=typeof Promise)return new Promise((function(t){n=t}))}function kn(t){return function(e,n){if(void 0===n&&(n=ct),n)return function(t,e,n){var r=t.$options;r[e]=vr(r[e],n)}(n,t,e)}}var Sn=kn("beforeMount"),On=kn("mounted"),Tn=kn("beforeUpdate"),An=kn("updated"),jn=kn("beforeDestroy"),En=kn("destroyed"),Nn=kn("activated"),Pn=kn("deactivated"),Dn=kn("serverPrefetch"),Mn=kn("renderTracked"),In=kn("renderTriggered"),Ln=kn("errorCaptured");var Rn="2.7.14";var Fn=Object.freeze({__proto__:null,version:Rn,defineComponent:function(t){return t},ref:function(t){return Ht(t,!1)},shallowRef:function(t){return Ht(t,!0)},isRef:Ft,toRef:Ut,toRefs:function(t){var n=e(t)?new Array(t.length):{};for(var r in t)n[r]=Ut(t,r);return n},unref:function(t){return Ft(t)?t.value:t},proxyRefs:function(t){if(Mt(t))return t;for(var e={},n=Object.keys(t),r=0;r<n.length;r++)Bt(e,t,n[r]);return e},customRef:function(t){var e=new mt,n=t((function(){e.depend()}),(function(){e.notify()})),r=n.get,o=n.set,i={get value(){return r()},set value(t){o(t)}};return z(i,Rt,!0),i},triggerRef:function(t){t.dep&&t.dep.notify()},reactive:function(t){return Dt(t,!1),t},isReactive:Mt,isReadonly:Lt,isShallow:It,isProxy:function(t){return Mt(t)||Lt(t)},shallowReactive:Pt,markRaw:function(t){return Object.isExtensible(t)&&z(t,"__v_skip",!0),t},toRaw:function t(e){var n=e&&e.__v_raw;return n?t(n):e},readonly:zt,shallowReadonly:function(t){return Vt(t,!0)},computed:function(t,e){var n,r,o=a(t);o?(n=t,r=j):(n=t.get,r=t.set);var i=rt()?null:new Vn(ct,n,j,{lazy:!0}),s={effect:i,get value(){return i?(i.dirty&&i.evaluate(),mt.target&&i.depend(),i.value):n()},set value(t){r(t)}};return z(s,Rt,!0),z(s,"__v_isReadonly",o),s},watch:function(t,e,n){return cn(t,e,n)},watchEffect:function(t,e){return cn(t,null,e)},watchPostEffect:on,watchSyncEffect:function(t,e){return cn(t,null,{flush:"sync"})},EffectScope:un,effectScope:function(t){return new un(t)},onScopeDispose:function(t){an&&an.cleanups.push(t)},getCurrentScope:function(){return an},provide:function(t,e){ct&&(ln(ct)[t]=e)},inject:function(t,e,n){void 0===n&&(n=!1);var r=ct;if(r){var o=r.$parent&&r.$parent._provided;if(o&&t in o)return o[t];if(arguments.length>1)return n&&a(e)?e.call(r):e}},h:function(t,e,n){return te(ct,t,e,n,2,!0)},getCurrentInstance:function(){return ct&&{proxy:ct}},useSlots:function(){return Oe().slots},useAttrs:function(){return Oe().attrs},useListeners:function(){return Oe().listeners},mergeDefaults:function(t,n){var r=e(t)?t.reduce((function(t,e){return t[e]={},t}),{}):t;for(var o in n){var i=r[o];i?e(i)||a(i)?r[o]={type:i,default:n[o]}:i.default=n[o]:null===i&&(r[o]={default:n[o]})}return r},nextTick:Cn,set:jt,del:Et,useCssModule:function(e){return t},useCssVars:function(t){if(J){var e=ct;e&&on((function(){var n=e.$el,r=t(e,e._setupProxy);if(n&&1===n.nodeType){var o=n.style;for(var i in r)o.setProperty("--".concat(i),r[i])}}))}},defineAsyncComponent:function(t){a(t)&&(t={loader:t});var e=t.loader,n=t.loadingComponent,r=t.errorComponent,o=t.delay,i=void 0===o?200:o,s=t.timeout;t.suspensible;var c=t.onError,u=null,l=0,f=function(){var t;return u||(t=u=e().catch((function(t){if(t=t instanceof Error?t:new Error(String(t)),c)return new Promise((function(e,n){c(t,(function(){return e((l++,u=null,f()))}),(function(){return n(t)}),l+1)}));throw t})).then((function(e){return t!==u&&u?u:(e&&(e.__esModule||"Module"===e[Symbol.toStringTag])&&(e=e.default),e)})))};return function(){return{component:f(),delay:i,timeout:s,error:r,loading:n}}},onBeforeMount:Sn,onMounted:On,onBeforeUpdate:Tn,onUpdated:An,onBeforeUnmount:jn,onUnmounted:En,onActivated:Nn,onDeactivated:Pn,onServerPrefetch:Dn,onRenderTracked:Mn,onRenderTriggered:In,onErrorCaptured:function(t,e){void 0===e&&(e=ct),Ln(t,e)}}),Hn=new at;function Bn(t){return Un(t,Hn),Hn.clear(),t}function Un(t,n){var r,o,i=e(t);if(!(!i&&!s(t)||t.__v_skip||Object.isFrozen(t)||t instanceof lt)){if(t.__ob__){var a=t.__ob__.dep.id;if(n.has(a))return;n.add(a)}if(i)for(r=t.length;r--;)Un(t[r],n);else if(Ft(t))Un(t.value,n);else for(r=(o=Object.keys(t)).length;r--;)Un(t[o[r]],n)}}var zn=0,Vn=function(){function t(t,e,n,r,o){!function(t,e){void 0===e&&(e=an),e&&e.active&&e.effects.push(t)}(this,an&&!an._vm?an:t?t._scope:void 0),(this.vm=t)&&o&&(t._watcher=this),r?(this.deep=!!r.deep,this.user=!!r.user,this.lazy=!!r.lazy,this.sync=!!r.sync,this.before=r.before):this.deep=this.user=this.lazy=this.sync=!1,this.cb=n,this.id=++zn,this.active=!0,this.post=!1,this.dirty=this.lazy,this.deps=[],this.newDeps=[],this.depIds=new at,this.newDepIds=new at,this.expression="",a(e)?this.getter=e:(this.getter=function(t){if(!V.test(t)){var e=t.split(".");return function(t){for(var n=0;n<e.length;n++){if(!t)return;t=t[e[n]]}return t}}}(e),this.getter||(this.getter=j)),this.value=this.lazy?void 0:this.get()}return t.prototype.get=function(){var t;yt(this);var e=this.vm;try{t=this.getter.call(e,e)}catch(t){if(!this.user)throw t;fn(t,e,'getter for watcher "'.concat(this.expression,'"'))}finally{this.deep&&Bn(t),_t(),this.cleanupDeps()}return t},t.prototype.addDep=function(t){var e=t.id;this.newDepIds.has(e)||(this.newDepIds.add(e),this.newDeps.push(t),this.depIds.has(e)||t.addSub(this))},t.prototype.cleanupDeps=function(){for(var t=this.deps.length;t--;){var e=this.deps[t];this.newDepIds.has(e.id)||e.removeSub(this)}var n=this.depIds;this.depIds=this.newDepIds,this.newDepIds=n,this.newDepIds.clear(),n=this.deps,this.deps=this.newDeps,this.newDeps=n,this.newDeps.length=0},t.prototype.update=function(){this.lazy?this.dirty=!0:this.sync?this.run():Qe(this)},t.prototype.run=function(){if(this.active){var t=this.get();if(t!==this.value||s(t)||this.deep){var e=this.value;if(this.value=t,this.user){var n='callback for watcher "'.concat(this.expression,'"');dn(this.cb,this.vm,[t,e],this.vm,n)}else this.cb.call(this.vm,t,e)}}},t.prototype.evaluate=function(){this.value=this.get(),this.dirty=!1},t.prototype.depend=function(){for(var t=this.deps.length;t--;)this.deps[t].depend()},t.prototype.teardown=function(){if(this.vm&&!this.vm._isBeingDestroyed&&g(this.vm._scope.effects,this),this.active){for(var t=this.deps.length;t--;)this.deps[t].removeSub(this);this.active=!1,this.onStop&&this.onStop()}},t}(),Kn={enumerable:!0,configurable:!0,get:j,set:j};function Jn(t,e,n){Kn.get=function(){return this[e][n]},Kn.set=function(t){this[e][n]=t},Object.defineProperty(t,n,Kn)}function qn(t){var n=t.$options;if(n.props&&function(t,e){var n=t.$options.propsData||{},r=t._props=Pt({}),o=t.$options._propKeys=[];t.$parent&&kt(!1);var i=function(i){o.push(i);var a=_r(i,e,n,t);At(r,i,a),i in t||Jn(t,"_props",i)};for(var a in e)i(a);kt(!0)}(t,n.props),function(t){var e=t.$options,n=e.setup;if(n){var r=t._setupContext=xe(t);ut(t),yt();var o=dn(n,null,[t._props||Pt({}),r],t,"setup");if(_t(),ut(),a(o))e.render=o;else if(s(o))if(t._setupState=o,o.__sfc){var i=t._setupProxy={};for(var c in o)"__sfc"!==c&&Bt(i,o,c)}else for(var c in o)U(c)||Bt(t,o,c)}}(t),n.methods&&function(t,e){for(var n in t.$options.props,e)t[n]="function"!=typeof e[n]?j:S(e[n],t)}(t,n.methods),n.data)!function(t){var e=t.$options.data;u(e=t._data=a(e)?function(t,e){yt();try{return t.call(e,e)}catch(t){return fn(t,e,"data()"),{}}finally{_t()}}(e,t):e||{})||(e={});var n=Object.keys(e),r=t.$options.props;t.$options.methods;var o=n.length;for(;o--;){var i=n[o];r&&_(r,i)||U(i)||Jn(t,"_data",i)}var s=Tt(e);s&&s.vmCount++}(t);else{var r=Tt(t._data={});r&&r.vmCount++}n.computed&&function(t,e){var n=t._computedWatchers=Object.create(null),r=rt();for(var o in e){var i=e[o],s=a(i)?i:i.get;r||(n[o]=new Vn(t,s||j,j,Wn)),o in t||Zn(t,o,i)}}(t,n.computed),n.watch&&n.watch!==tt&&function(t,n){for(var r in n){var o=n[r];if(e(o))for(var i=0;i<o.length;i++)Yn(t,r,o[i]);else Yn(t,r,o)}}(t,n.watch)}var Wn={lazy:!0};function Zn(t,e,n){var r=!rt();a(n)?(Kn.get=r?Gn(e):Xn(n),Kn.set=j):(Kn.get=n.get?r&&!1!==n.cache?Gn(e):Xn(n.get):j,Kn.set=n.set||j),Object.defineProperty(t,e,Kn)}function Gn(t){return function(){var e=this._computedWatchers&&this._computedWatchers[t];if(e)return e.dirty&&e.evaluate(),mt.target&&e.depend(),e.value}}function Xn(t){return function(){return t.call(this,this)}}function Yn(t,e,n,r){return u(n)&&(r=n,n=n.handler),"string"==typeof n&&(n=t[n]),t.$watch(e,n,r)}function Qn(t,e){if(t){for(var n=Object.create(null),r=st?Reflect.ownKeys(t):Object.keys(t),o=0;o<r.length;o++){var i=r[o];if("__ob__"!==i){var s=t[i].from;if(s in e._provided)n[i]=e._provided[s];else if("default"in t[i]){var c=t[i].default;n[i]=a(c)?c.call(e):c}}}return n}}var tr=0;function er(t){var e=t.options;if(t.super){var n=er(t.super);if(n!==t.superOptions){t.superOptions=n;var r=function(t){var e,n=t.options,r=t.sealedOptions;for(var o in n)n[o]!==r[o]&&(e||(e={}),e[o]=n[o]);return e}(t);r&&T(t.extendOptions,r),(e=t.options=gr(n,t.extendOptions)).name&&(e.components[e.name]=t)}}return e}function nr(n,r,i,a,s){var c,u=this,l=s.options;_(a,"_uid")?(c=Object.create(a))._original=a:(c=a,a=a._original);var f=o(l._compiled),d=!f;this.data=n,this.props=r,this.children=i,this.parent=a,this.listeners=n.on||t,this.injections=Qn(l.inject,a),this.slots=function(){return u.$slots||be(a,n.scopedSlots,u.$slots=ge(i,a)),u.$slots},Object.defineProperty(this,"scopedSlots",{enumerable:!0,get:function(){return be(a,n.scopedSlots,this.slots())}}),f&&(this.$options=l,this.$slots=this.slots(),this.$scopedSlots=be(a,n.scopedSlots,this.$slots)),l._scopeId?this._c=function(t,n,r,o){var i=te(c,t,n,r,o,d);return i&&!e(i)&&(i.fnScopeId=l._scopeId,i.fnContext=a),i}:this._c=function(t,e,n,r){return te(c,t,e,n,r,d)}}function rr(t,e,n,r,o){var i=pt(t);return i.fnContext=n,i.fnOptions=r,e.slot&&((i.data||(i.data={})).slot=e.slot),i}function or(t,e){for(var n in e)t[w(n)]=e[n]}function ir(t){return t.name||t.__name||t._componentTag}me(nr.prototype);var ar={init:function(t,e){if(t.componentInstance&&!t.componentInstance._isDestroyed&&t.data.keepAlive){var n=t;ar.prepatch(n,n)}else{(t.componentInstance=function(t,e){var n={_isComponent:!0,_parentVnode:t,parent:e},o=t.data.inlineTemplate;r(o)&&(n.render=o.render,n.staticRenderFns=o.staticRenderFns);return new t.componentOptions.Ctor(n)}(t,Ie)).$mount(e?t.elm:void 0,e)}},prepatch:function(e,n){var r=n.componentOptions;!function(e,n,r,o,i){var a=o.data.scopedSlots,s=e.$scopedSlots,c=!!(a&&!a.$stable||s!==t&&!s.$stable||a&&e.$scopedSlots.$key!==a.$key||!a&&e.$scopedSlots.$key),u=!!(i||e.$options._renderChildren||c),l=e.$vnode;e.$options._parentVnode=o,e.$vnode=o,e._vnode&&(e._vnode.parent=o),e.$options._renderChildren=i;var f=o.data.attrs||t;e._attrsProxy&&Ce(e._attrsProxy,f,l.data&&l.data.attrs||t,e,"$attrs")&&(u=!0),e.$attrs=f,r=r||t;var d=e.$options._parentListeners;if(e._listenersProxy&&Ce(e._listenersProxy,r,d||t,e,"$listeners"),e.$listeners=e.$options._parentListeners=r,Me(e,r,d),n&&e.$options.props){kt(!1);for(var p=e._props,v=e.$options._propKeys||[],h=0;h<v.length;h++){var m=v[h],g=e.$options.props;p[m]=_r(m,g,n,e)}kt(!0),e.$options.propsData=n}u&&(e.$slots=ge(i,o.context),e.$forceUpdate())}(n.componentInstance=e.componentInstance,r.propsData,r.listeners,n,r.children)},insert:function(t){var e,n=t.context,r=t.componentInstance;r._isMounted||(r._isMounted=!0,Be(r,"mounted")),t.data.keepAlive&&(n._isMounted?((e=r)._inactive=!1,ze.push(e)):Fe(r,!0))},destroy:function(t){var e=t.componentInstance;e._isDestroyed||(t.data.keepAlive?He(e,!0):e.$destroy())}},sr=Object.keys(ar);function cr(i,a,c,u,l){if(!n(i)){var d=c.$options._base;if(s(i)&&(i=d.extend(i)),"function"==typeof i){var p;if(n(i.cid)&&(i=function(t,e){if(o(t.error)&&r(t.errorComp))return t.errorComp;if(r(t.resolved))return t.resolved;var i=Ae;if(i&&r(t.owners)&&-1===t.owners.indexOf(i)&&t.owners.push(i),o(t.loading)&&r(t.loadingComp))return t.loadingComp;if(i&&!r(t.owners)){var a=t.owners=[i],c=!0,u=null,l=null;i.$on("hook:destroyed",(function(){return g(a,i)}));var d=function(t){for(var e=0,n=a.length;e<n;e++)a[e].$forceUpdate();t&&(a.length=0,null!==u&&(clearTimeout(u),u=null),null!==l&&(clearTimeout(l),l=null))},p=M((function(n){t.resolved=je(n,e),c?a.length=0:d(!0)})),v=M((function(e){r(t.errorComp)&&(t.error=!0,d(!0))})),h=t(p,v);return s(h)&&(f(h)?n(t.resolved)&&h.then(p,v):f(h.component)&&(h.component.then(p,v),r(h.error)&&(t.errorComp=je(h.error,e)),r(h.loading)&&(t.loadingComp=je(h.loading,e),0===h.delay?t.loading=!0:u=setTimeout((function(){u=null,n(t.resolved)&&n(t.error)&&(t.loading=!0,d(!1))}),h.delay||200)),r(h.timeout)&&(l=setTimeout((function(){l=null,n(t.resolved)&&v(null)}),h.timeout)))),c=!1,t.loading?t.loadingComp:t.resolved}}(p=i,d),void 0===i))return function(t,e,n,r,o){var i=ft();return i.asyncFactory=t,i.asyncMeta={data:e,context:n,children:r,tag:o},i}(p,a,c,u,l);a=a||{},er(i),r(a.model)&&function(t,n){var o=t.model&&t.model.prop||"value",i=t.model&&t.model.event||"input";(n.attrs||(n.attrs={}))[o]=n.model.value;var a=n.on||(n.on={}),s=a[i],c=n.model.callback;r(s)?(e(s)?-1===s.indexOf(c):s!==c)&&(a[i]=[c].concat(s)):a[i]=c}(i.options,a);var v=function(t,e,o){var i=e.options.props;if(!n(i)){var a={},s=t.attrs,c=t.props;if(r(s)||r(c))for(var u in i){var l=k(u);Gt(a,c,u,l,!0)||Gt(a,s,u,l,!1)}return a}}(a,i);if(o(i.options.functional))return function(n,o,i,a,s){var c=n.options,u={},l=c.props;if(r(l))for(var f in l)u[f]=_r(f,l,o||t);else r(i.attrs)&&or(u,i.attrs),r(i.props)&&or(u,i.props);var d=new nr(i,u,s,a,n),p=c.render.call(null,d._c,d);if(p instanceof lt)return rr(p,i,d.parent,c);if(e(p)){for(var v=Xt(p)||[],h=new Array(v.length),m=0;m<v.length;m++)h[m]=rr(v[m],i,d.parent,c);return h}}(i,v,a,c,u);var h=a.on;if(a.on=a.nativeOn,o(i.options.abstract)){var m=a.slot;a={},m&&(a.slot=m)}!function(t){for(var e=t.hook||(t.hook={}),n=0;n<sr.length;n++){var r=sr[n],o=e[r],i=ar[r];o===i||o&&o._merged||(e[r]=o?ur(i,o):i)}}(a);var y=ir(i.options)||l;return new lt("vue-component-".concat(i.cid).concat(y?"-".concat(y):""),a,void 0,void 0,void 0,c,{Ctor:i,propsData:v,listeners:h,tag:l,children:u},p)}}}function ur(t,e){var n=function(n,r){t(n,r),e(n,r)};return n._merged=!0,n}var lr=j,fr=H.optionMergeStrategies;function dr(t,e,n){if(void 0===n&&(n=!0),!e)return t;for(var r,o,i,a=st?Reflect.ownKeys(e):Object.keys(e),s=0;s<a.length;s++)"__ob__"!==(r=a[s])&&(o=t[r],i=e[r],n&&_(t,r)?o!==i&&u(o)&&u(i)&&dr(o,i):jt(t,r,i));return t}function pr(t,e,n){return n?function(){var r=a(e)?e.call(n,n):e,o=a(t)?t.call(n,n):t;return r?dr(r,o):o}:e?t?function(){return dr(a(e)?e.call(this,this):e,a(t)?t.call(this,this):t)}:e:t}function vr(t,n){var r=n?t?t.concat(n):e(n)?n:[n]:t;return r?function(t){for(var e=[],n=0;n<t.length;n++)-1===e.indexOf(t[n])&&e.push(t[n]);return e}(r):r}function hr(t,e,n,r){var o=Object.create(t||null);return e?T(o,e):o}fr.data=function(t,e,n){return n?pr(t,e,n):e&&"function"!=typeof e?t:pr(t,e)},F.forEach((function(t){fr[t]=vr})),R.forEach((function(t){fr[t+"s"]=hr})),fr.watch=function(t,n,r,o){if(t===tt&&(t=void 0),n===tt&&(n=void 0),!n)return Object.create(t||null);if(!t)return n;var i={};for(var a in T(i,t),n){var s=i[a],c=n[a];s&&!e(s)&&(s=[s]),i[a]=s?s.concat(c):e(c)?c:[c]}return i},fr.props=fr.methods=fr.inject=fr.computed=function(t,e,n,r){if(!t)return e;var o=Object.create(null);return T(o,t),e&&T(o,e),o},fr.provide=function(t,e){return t?function(){var n=Object.create(null);return dr(n,a(t)?t.call(this):t),e&&dr(n,a(e)?e.call(this):e,!1),n}:e};var mr=function(t,e){return void 0===e?t:e};function gr(t,n,r){if(a(n)&&(n=n.options),function(t,n){var r=t.props;if(r){var o,i,a={};if(e(r))for(o=r.length;o--;)"string"==typeof(i=r[o])&&(a[w(i)]={type:null});else if(u(r))for(var s in r)i=r[s],a[w(s)]=u(i)?i:{type:i};t.props=a}}(n),function(t,n){var r=t.inject;if(r){var o=t.inject={};if(e(r))for(var i=0;i<r.length;i++)o[r[i]]={from:r[i]};else if(u(r))for(var a in r){var s=r[a];o[a]=u(s)?T({from:a},s):{from:s}}}}(n),function(t){var e=t.directives;if(e)for(var n in e){var r=e[n];a(r)&&(e[n]={bind:r,update:r})}}(n),!n._base&&(n.extends&&(t=gr(t,n.extends,r)),n.mixins))for(var o=0,i=n.mixins.length;o<i;o++)t=gr(t,n.mixins[o],r);var s,c={};for(s in t)l(s);for(s in n)_(t,s)||l(s);function l(e){var o=fr[e]||mr;c[e]=o(t[e],n[e],r,e)}return c}function yr(t,e,n,r){if("string"==typeof n){var o=t[e];if(_(o,n))return o[n];var i=w(n);if(_(o,i))return o[i];var a=x(i);return _(o,a)?o[a]:o[n]||o[i]||o[a]}}function _r(t,e,n,r){var o=e[t],i=!_(n,t),s=n[t],c=xr(Boolean,o.type);if(c>-1)if(i&&!_(o,"default"))s=!1;else if(""===s||s===k(t)){var u=xr(String,o.type);(u<0||c<u)&&(s=!0)}if(void 0===s){s=function(t,e,n){if(!_(e,"default"))return;var r=e.default;if(t&&t.$options.propsData&&void 0===t.$options.propsData[n]&&void 0!==t._props[n])return t._props[n];return a(r)&&"Function"!==$r(e.type)?r.call(t):r}(r,o,t);var l=Ct;kt(!0),Tt(s),kt(l)}return s}var br=/^\s*function (\w+)/;function $r(t){var e=t&&t.toString().match(br);return e?e[1]:""}function wr(t,e){return $r(t)===$r(e)}function xr(t,n){if(!e(n))return wr(n,t)?0:-1;for(var r=0,o=n.length;r<o;r++)if(wr(n[r],t))return r;return-1}function Cr(t){this._init(t)}function kr(t){t.cid=0;var e=1;t.extend=function(t){t=t||{};var n=this,r=n.cid,o=t._Ctor||(t._Ctor={});if(o[r])return o[r];var i=ir(t)||ir(n.options),a=function(t){this._init(t)};return(a.prototype=Object.create(n.prototype)).constructor=a,a.cid=e++,a.options=gr(n.options,t),a.super=n,a.options.props&&function(t){var e=t.options.props;for(var n in e)Jn(t.prototype,"_props",n)}(a),a.options.computed&&function(t){var e=t.options.computed;for(var n in e)Zn(t.prototype,n,e[n])}(a),a.extend=n.extend,a.mixin=n.mixin,a.use=n.use,R.forEach((function(t){a[t]=n[t]})),i&&(a.options.components[i]=a),a.superOptions=n.options,a.extendOptions=t,a.sealedOptions=T({},a.options),o[r]=a,a}}function Sr(t){return t&&(ir(t.Ctor.options)||t.tag)}function Or(t,n){return e(t)?t.indexOf(n)>-1:"string"==typeof t?t.split(",").indexOf(n)>-1:(r=t,"[object RegExp]"===c.call(r)&&t.test(n));var r}function Tr(t,e){var n=t.cache,r=t.keys,o=t._vnode;for(var i in n){var a=n[i];if(a){var s=a.name;s&&!e(s)&&Ar(n,i,r,o)}}}function Ar(t,e,n,r){var o=t[e];!o||r&&o.tag===r.tag||o.componentInstance.$destroy(),t[e]=null,g(n,e)}!function(e){e.prototype._init=function(e){var n=this;n._uid=tr++,n._isVue=!0,n.__v_skip=!0,n._scope=new un(!0),n._scope._vm=!0,e&&e._isComponent?function(t,e){var n=t.$options=Object.create(t.constructor.options),r=e._parentVnode;n.parent=e.parent,n._parentVnode=r;var o=r.componentOptions;n.propsData=o.propsData,n._parentListeners=o.listeners,n._renderChildren=o.children,n._componentTag=o.tag,e.render&&(n.render=e.render,n.staticRenderFns=e.staticRenderFns)}(n,e):n.$options=gr(er(n.constructor),e||{},n),n._renderProxy=n,n._self=n,function(t){var e=t.$options,n=e.parent;if(n&&!e.abstract){for(;n.$options.abstract&&n.$parent;)n=n.$parent;n.$children.push(t)}t.$parent=n,t.$root=n?n.$root:t,t.$children=[],t.$refs={},t._provided=n?n._provided:Object.create(null),t._watcher=null,t._inactive=null,t._directInactive=!1,t._isMounted=!1,t._isDestroyed=!1,t._isBeingDestroyed=!1}(n),function(t){t._events=Object.create(null),t._hasHookEvent=!1;var e=t.$options._parentListeners;e&&Me(t,e)}(n),function(e){e._vnode=null,e._staticTrees=null;var n=e.$options,r=e.$vnode=n._parentVnode,o=r&&r.context;e.$slots=ge(n._renderChildren,o),e.$scopedSlots=r?be(e.$parent,r.data.scopedSlots,e.$slots):t,e._c=function(t,n,r,o){return te(e,t,n,r,o,!1)},e.$createElement=function(t,n,r,o){return te(e,t,n,r,o,!0)};var i=r&&r.data;At(e,"$attrs",i&&i.attrs||t,null,!0),At(e,"$listeners",n._parentListeners||t,null,!0)}(n),Be(n,"beforeCreate",void 0,!1),function(t){var e=Qn(t.$options.inject,t);e&&(kt(!1),Object.keys(e).forEach((function(n){At(t,n,e[n])})),kt(!0))}(n),qn(n),function(t){var e=t.$options.provide;if(e){var n=a(e)?e.call(t):e;if(!s(n))return;for(var r=ln(t),o=st?Reflect.ownKeys(n):Object.keys(n),i=0;i<o.length;i++){var c=o[i];Object.defineProperty(r,c,Object.getOwnPropertyDescriptor(n,c))}}}(n),Be(n,"created"),n.$options.el&&n.$mount(n.$options.el)}}(Cr),function(t){var e={get:function(){return this._data}},n={get:function(){return this._props}};Object.defineProperty(t.prototype,"$data",e),Object.defineProperty(t.prototype,"$props",n),t.prototype.$set=jt,t.prototype.$delete=Et,t.prototype.$watch=function(t,e,n){var r=this;if(u(e))return Yn(r,t,e,n);(n=n||{}).user=!0;var o=new Vn(r,t,e,n);if(n.immediate){var i='callback for immediate watcher "'.concat(o.expression,'"');yt(),dn(e,r,[o.value],r,i),_t()}return function(){o.teardown()}}}(Cr),function(t){var n=/^hook:/;t.prototype.$on=function(t,r){var o=this;if(e(t))for(var i=0,a=t.length;i<a;i++)o.$on(t[i],r);else(o._events[t]||(o._events[t]=[])).push(r),n.test(t)&&(o._hasHookEvent=!0);return o},t.prototype.$once=function(t,e){var n=this;function r(){n.$off(t,r),e.apply(n,arguments)}return r.fn=e,n.$on(t,r),n},t.prototype.$off=function(t,n){var r=this;if(!arguments.length)return r._events=Object.create(null),r;if(e(t)){for(var o=0,i=t.length;o<i;o++)r.$off(t[o],n);return r}var a,s=r._events[t];if(!s)return r;if(!n)return r._events[t]=null,r;for(var c=s.length;c--;)if((a=s[c])===n||a.fn===n){s.splice(c,1);break}return r},t.prototype.$emit=function(t){var e=this,n=e._events[t];if(n){n=n.length>1?O(n):n;for(var r=O(arguments,1),o='event handler for "'.concat(t,'"'),i=0,a=n.length;i<a;i++)dn(n[i],e,r,e,o)}return e}}(Cr),function(t){t.prototype._update=function(t,e){var n=this,r=n.$el,o=n._vnode,i=Le(n);n._vnode=t,n.$el=o?n.__patch__(o,t):n.__patch__(n.$el,t,e,!1),i(),r&&(r.__vue__=null),n.$el&&(n.$el.__vue__=n);for(var a=n;a&&a.$vnode&&a.$parent&&a.$vnode===a.$parent._vnode;)a.$parent.$el=a.$el,a=a.$parent},t.prototype.$forceUpdate=function(){this._watcher&&this._watcher.update()},t.prototype.$destroy=function(){var t=this;if(!t._isBeingDestroyed){Be(t,"beforeDestroy"),t._isBeingDestroyed=!0;var e=t.$parent;!e||e._isBeingDestroyed||t.$options.abstract||g(e.$children,t),t._scope.stop(),t._data.__ob__&&t._data.__ob__.vmCount--,t._isDestroyed=!0,t.__patch__(t._vnode,null),Be(t,"destroyed"),t.$off(),t.$el&&(t.$el.__vue__=null),t.$vnode&&(t.$vnode.parent=null)}}}(Cr),function(t){me(t.prototype),t.prototype.$nextTick=function(t){return Cn(t,this)},t.prototype._render=function(){var t,n=this,r=n.$options,o=r.render,i=r._parentVnode;i&&n._isMounted&&(n.$scopedSlots=be(n.$parent,i.data.scopedSlots,n.$slots,n.$scopedSlots),n._slotsProxy&&Se(n._slotsProxy,n.$scopedSlots)),n.$vnode=i;try{ut(n),Ae=n,t=o.call(n._renderProxy,n.$createElement)}catch(e){fn(e,n,"render"),t=n._vnode}finally{Ae=null,ut()}return e(t)&&1===t.length&&(t=t[0]),t instanceof lt||(t=ft()),t.parent=i,t}}(Cr);var jr=[String,RegExp,Array],Er={name:"keep-alive",abstract:!0,props:{include:jr,exclude:jr,max:[String,Number]},methods:{cacheVNode:function(){var t=this,e=t.cache,n=t.keys,r=t.vnodeToCache,o=t.keyToCache;if(r){var i=r.tag,a=r.componentInstance,s=r.componentOptions;e[o]={name:Sr(s),tag:i,componentInstance:a},n.push(o),this.max&&n.length>parseInt(this.max)&&Ar(e,n[0],n,this._vnode),this.vnodeToCache=null}}},created:function(){this.cache=Object.create(null),this.keys=[]},destroyed:function(){for(var t in this.cache)Ar(this.cache,t,this.keys)},mounted:function(){var t=this;this.cacheVNode(),this.$watch("include",(function(e){Tr(t,(function(t){return Or(e,t)}))})),this.$watch("exclude",(function(e){Tr(t,(function(t){return!Or(e,t)}))}))},updated:function(){this.cacheVNode()},render:function(){var t=this.$slots.default,e=Ee(t),n=e&&e.componentOptions;if(n){var r=Sr(n),o=this.include,i=this.exclude;if(o&&(!r||!Or(o,r))||i&&r&&Or(i,r))return e;var a=this.cache,s=this.keys,c=null==e.key?n.Ctor.cid+(n.tag?"::".concat(n.tag):""):e.key;a[c]?(e.componentInstance=a[c].componentInstance,g(s,c),s.push(c)):(this.vnodeToCache=e,this.keyToCache=c),e.data.keepAlive=!0}return e||t&&t[0]}},Nr={KeepAlive:Er};!function(t){var e={get:function(){return H}};Object.defineProperty(t,"config",e),t.util={warn:lr,extend:T,mergeOptions:gr,defineReactive:At},t.set=jt,t.delete=Et,t.nextTick=Cn,t.observable=function(t){return Tt(t),t},t.options=Object.create(null),R.forEach((function(e){t.options[e+"s"]=Object.create(null)})),t.options._base=t,T(t.options.components,Nr),function(t){t.use=function(t){var e=this._installedPlugins||(this._installedPlugins=[]);if(e.indexOf(t)>-1)return this;var n=O(arguments,1);return n.unshift(this),a(t.install)?t.install.apply(t,n):a(t)&&t.apply(null,n),e.push(t),this}}(t),function(t){t.mixin=function(t){return this.options=gr(this.options,t),this}}(t),kr(t),function(t){R.forEach((function(e){t[e]=function(t,n){return n?("component"===e&&u(n)&&(n.name=n.name||t,n=this.options._base.extend(n)),"directive"===e&&a(n)&&(n={bind:n,update:n}),this.options[e+"s"][t]=n,n):this.options[e+"s"][t]}}))}(t)}(Cr),Object.defineProperty(Cr.prototype,"$isServer",{get:rt}),Object.defineProperty(Cr.prototype,"$ssrContext",{get:function(){return this.$vnode&&this.$vnode.ssrContext}}),Object.defineProperty(Cr,"FunctionalRenderContext",{value:nr}),Cr.version=Rn;var Pr=v("style,class"),Dr=v("input,textarea,option,select,progress"),Mr=function(t,e,n){return"value"===n&&Dr(t)&&"button"!==e||"selected"===n&&"option"===t||"checked"===n&&"input"===t||"muted"===n&&"video"===t},Ir=v("contenteditable,draggable,spellcheck"),Lr=v("events,caret,typing,plaintext-only"),Rr=v("allowfullscreen,async,autofocus,autoplay,checked,compact,controls,declare,default,defaultchecked,defaultmuted,defaultselected,defer,disabled,enabled,formnovalidate,hidden,indeterminate,inert,ismap,itemscope,loop,multiple,muted,nohref,noresize,noshade,novalidate,nowrap,open,pauseonexit,readonly,required,reversed,scoped,seamless,selected,sortable,truespeed,typemustmatch,visible"),Fr="http://www.w3.org/1999/xlink",Hr=function(t){return":"===t.charAt(5)&&"xlink"===t.slice(0,5)},Br=function(t){return Hr(t)?t.slice(6,t.length):""},Ur=function(t){return null==t||!1===t};function zr(t){for(var e=t.data,n=t,o=t;r(o.componentInstance);)(o=o.componentInstance._vnode)&&o.data&&(e=Vr(o.data,e));for(;r(n=n.parent);)n&&n.data&&(e=Vr(e,n.data));return function(t,e){if(r(t)||r(e))return Kr(t,Jr(e));return""}(e.staticClass,e.class)}function Vr(t,e){return{staticClass:Kr(t.staticClass,e.staticClass),class:r(t.class)?[t.class,e.class]:e.class}}function Kr(t,e){return t?e?t+" "+e:t:e||""}function Jr(t){return Array.isArray(t)?function(t){for(var e,n="",o=0,i=t.length;o<i;o++)r(e=Jr(t[o]))&&""!==e&&(n&&(n+=" "),n+=e);return n}(t):s(t)?function(t){var e="";for(var n in t)t[n]&&(e&&(e+=" "),e+=n);return e}(t):"string"==typeof t?t:""}var qr={svg:"http://www.w3.org/2000/svg",math:"http://www.w3.org/1998/Math/MathML"},Wr=v("html,body,base,head,link,meta,style,title,address,article,aside,footer,header,h1,h2,h3,h4,h5,h6,hgroup,nav,section,div,dd,dl,dt,figcaption,figure,picture,hr,img,li,main,ol,p,pre,ul,a,b,abbr,bdi,bdo,br,cite,code,data,dfn,em,i,kbd,mark,q,rp,rt,rtc,ruby,s,samp,small,span,strong,sub,sup,time,u,var,wbr,area,audio,map,track,video,embed,object,param,source,canvas,script,noscript,del,ins,caption,col,colgroup,table,thead,tbody,td,th,tr,button,datalist,fieldset,form,input,label,legend,meter,optgroup,option,output,progress,select,textarea,details,dialog,menu,menuitem,summary,content,element,shadow,template,blockquote,iframe,tfoot"),Zr=v("svg,animate,circle,clippath,cursor,defs,desc,ellipse,filter,font-face,foreignobject,g,glyph,image,line,marker,mask,missing-glyph,path,pattern,polygon,polyline,rect,switch,symbol,text,textpath,tspan,use,view",!0),Gr=function(t){return Wr(t)||Zr(t)};function Xr(t){return Zr(t)?"svg":"math"===t?"math":void 0}var Yr=Object.create(null);var Qr=v("text,number,password,search,email,tel,url");function to(t){if("string"==typeof t){var e=document.querySelector(t);return e||document.createElement("div")}return t}var eo=Object.freeze({__proto__:null,createElement:function(t,e){var n=document.createElement(t);return"select"!==t||e.data&&e.data.attrs&&void 0!==e.data.attrs.multiple&&n.setAttribute("multiple","multiple"),n},createElementNS:function(t,e){return document.createElementNS(qr[t],e)},createTextNode:function(t){return document.createTextNode(t)},createComment:function(t){return document.createComment(t)},insertBefore:function(t,e,n){t.insertBefore(e,n)},removeChild:function(t,e){t.removeChild(e)},appendChild:function(t,e){t.appendChild(e)},parentNode:function(t){return t.parentNode},nextSibling:function(t){return t.nextSibling},tagName:function(t){return t.tagName},setTextContent:function(t,e){t.textContent=e},setStyleScope:function(t,e){t.setAttribute(e,"")}}),no={create:function(t,e){ro(e)},update:function(t,e){t.data.ref!==e.data.ref&&(ro(t,!0),ro(e))},destroy:function(t){ro(t,!0)}};function ro(t,n){var o=t.data.ref;if(r(o)){var i=t.context,s=t.componentInstance||t.elm,c=n?null:s,u=n?void 0:s;if(a(o))dn(o,i,[c],i,"template ref function");else{var l=t.data.refInFor,f="string"==typeof o||"number"==typeof o,d=Ft(o),p=i.$refs;if(f||d)if(l){var v=f?p[o]:o.value;n?e(v)&&g(v,s):e(v)?v.includes(s)||v.push(s):f?(p[o]=[s],oo(i,o,p[o])):o.value=[s]}else if(f){if(n&&p[o]!==s)return;p[o]=u,oo(i,o,c)}else if(d){if(n&&o.value!==s)return;o.value=c}}}}function oo(t,e,n){var r=t._setupState;r&&_(r,e)&&(Ft(r[e])?r[e].value=n:r[e]=n)}var io=new lt("",{},[]),ao=["create","activate","update","remove","destroy"];function so(t,e){return t.key===e.key&&t.asyncFactory===e.asyncFactory&&(t.tag===e.tag&&t.isComment===e.isComment&&r(t.data)===r(e.data)&&function(t,e){if("input"!==t.tag)return!0;var n,o=r(n=t.data)&&r(n=n.attrs)&&n.type,i=r(n=e.data)&&r(n=n.attrs)&&n.type;return o===i||Qr(o)&&Qr(i)}(t,e)||o(t.isAsyncPlaceholder)&&n(e.asyncFactory.error))}function co(t,e,n){var o,i,a={};for(o=e;o<=n;++o)r(i=t[o].key)&&(a[i]=o);return a}var uo={create:lo,update:lo,destroy:function(t){lo(t,io)}};function lo(t,e){(t.data.directives||e.data.directives)&&function(t,e){var n,r,o,i=t===io,a=e===io,s=po(t.data.directives,t.context),c=po(e.data.directives,e.context),u=[],l=[];for(n in c)r=s[n],o=c[n],r?(o.oldValue=r.value,o.oldArg=r.arg,ho(o,"update",e,t),o.def&&o.def.componentUpdated&&l.push(o)):(ho(o,"bind",e,t),o.def&&o.def.inserted&&u.push(o));if(u.length){var f=function(){for(var n=0;n<u.length;n++)ho(u[n],"inserted",e,t)};i?Zt(e,"insert",f):f()}l.length&&Zt(e,"postpatch",(function(){for(var n=0;n<l.length;n++)ho(l[n],"componentUpdated",e,t)}));if(!i)for(n in s)c[n]||ho(s[n],"unbind",t,t,a)}(t,e)}var fo=Object.create(null);function po(t,e){var n,r,o=Object.create(null);if(!t)return o;for(n=0;n<t.length;n++){if((r=t[n]).modifiers||(r.modifiers=fo),o[vo(r)]=r,e._setupState&&e._setupState.__sfc){var i=r.def||yr(e,"_setupState","v-"+r.name);r.def="function"==typeof i?{bind:i,update:i}:i}r.def=r.def||yr(e.$options,"directives",r.name)}return o}function vo(t){return t.rawName||"".concat(t.name,".").concat(Object.keys(t.modifiers||{}).join("."))}function ho(t,e,n,r,o){var i=t.def&&t.def[e];if(i)try{i(n.elm,t,n,r,o)}catch(r){fn(r,n.context,"directive ".concat(t.name," ").concat(e," hook"))}}var mo=[no,uo];function go(t,e){var i=e.componentOptions;if(!(r(i)&&!1===i.Ctor.options.inheritAttrs||n(t.data.attrs)&&n(e.data.attrs))){var a,s,c=e.elm,u=t.data.attrs||{},l=e.data.attrs||{};for(a in(r(l.__ob__)||o(l._v_attr_proxy))&&(l=e.data.attrs=T({},l)),l)s=l[a],u[a]!==s&&yo(c,a,s,e.data.pre);for(a in(W||G)&&l.value!==u.value&&yo(c,"value",l.value),u)n(l[a])&&(Hr(a)?c.removeAttributeNS(Fr,Br(a)):Ir(a)||c.removeAttribute(a))}}function yo(t,e,n,r){r||t.tagName.indexOf("-")>-1?_o(t,e,n):Rr(e)?Ur(n)?t.removeAttribute(e):(n="allowfullscreen"===e&&"EMBED"===t.tagName?"true":e,t.setAttribute(e,n)):Ir(e)?t.setAttribute(e,function(t,e){return Ur(e)||"false"===e?"false":"contenteditable"===t&&Lr(e)?e:"true"}(e,n)):Hr(e)?Ur(n)?t.removeAttributeNS(Fr,Br(e)):t.setAttributeNS(Fr,e,n):_o(t,e,n)}function _o(t,e,n){if(Ur(n))t.removeAttribute(e);else{if(W&&!Z&&"TEXTAREA"===t.tagName&&"placeholder"===e&&""!==n&&!t.__ieph){var r=function(e){e.stopImmediatePropagation(),t.removeEventListener("input",r)};t.addEventListener("input",r),t.__ieph=!0}t.setAttribute(e,n)}}var bo={create:go,update:go};function $o(t,e){var o=e.elm,i=e.data,a=t.data;if(!(n(i.staticClass)&&n(i.class)&&(n(a)||n(a.staticClass)&&n(a.class)))){var s=zr(e),c=o._transitionClasses;r(c)&&(s=Kr(s,Jr(c))),s!==o._prevClass&&(o.setAttribute("class",s),o._prevClass=s)}}var wo,xo,Co,ko,So,Oo,To={create:$o,update:$o},Ao=/[\w).+\-_$\]]/;function jo(t){var e,n,r,o,i,a=!1,s=!1,c=!1,u=!1,l=0,f=0,d=0,p=0;for(r=0;r<t.length;r++)if(n=e,e=t.charCodeAt(r),a)39===e&&92!==n&&(a=!1);else if(s)34===e&&92!==n&&(s=!1);else if(c)96===e&&92!==n&&(c=!1);else if(u)47===e&&92!==n&&(u=!1);else if(124!==e||124===t.charCodeAt(r+1)||124===t.charCodeAt(r-1)||l||f||d){switch(e){case 34:s=!0;break;case 39:a=!0;break;case 96:c=!0;break;case 40:d++;break;case 41:d--;break;case 91:f++;break;case 93:f--;break;case 123:l++;break;case 125:l--}if(47===e){for(var v=r-1,h=void 0;v>=0&&" "===(h=t.charAt(v));v--);h&&Ao.test(h)||(u=!0)}}else void 0===o?(p=r+1,o=t.slice(0,r).trim()):m();function m(){(i||(i=[])).push(t.slice(p,r).trim()),p=r+1}if(void 0===o?o=t.slice(0,r).trim():0!==p&&m(),i)for(r=0;r<i.length;r++)o=Eo(o,i[r]);return o}function Eo(t,e){var n=e.indexOf("(");if(n<0)return'_f("'.concat(e,'")(').concat(t,")");var r=e.slice(0,n),o=e.slice(n+1);return'_f("'.concat(r,'")(').concat(t).concat(")"!==o?","+o:o)}function No(t,e){console.error("[Vue compiler]: ".concat(t))}function Po(t,e){return t?t.map((function(t){return t[e]})).filter((function(t){return t})):[]}function Do(t,e,n,r,o){(t.props||(t.props=[])).push(zo({name:e,value:n,dynamic:o},r)),t.plain=!1}function Mo(t,e,n,r,o){(o?t.dynamicAttrs||(t.dynamicAttrs=[]):t.attrs||(t.attrs=[])).push(zo({name:e,value:n,dynamic:o},r)),t.plain=!1}function Io(t,e,n,r){t.attrsMap[e]=n,t.attrsList.push(zo({name:e,value:n},r))}function Lo(t,e,n,r,o,i,a,s){(t.directives||(t.directives=[])).push(zo({name:e,rawName:n,value:r,arg:o,isDynamicArg:i,modifiers:a},s)),t.plain=!1}function Ro(t,e,n){return n?"_p(".concat(e,',"').concat(t,'")'):t+e}function Fo(e,n,r,o,i,a,s,c){var u;(o=o||t).right?c?n="(".concat(n,")==='click'?'contextmenu':(").concat(n,")"):"click"===n&&(n="contextmenu",delete o.right):o.middle&&(c?n="(".concat(n,")==='click'?'mouseup':(").concat(n,")"):"click"===n&&(n="mouseup")),o.capture&&(delete o.capture,n=Ro("!",n,c)),o.once&&(delete o.once,n=Ro("~",n,c)),o.passive&&(delete o.passive,n=Ro("&",n,c)),o.native?(delete o.native,u=e.nativeEvents||(e.nativeEvents={})):u=e.events||(e.events={});var l=zo({value:r.trim(),dynamic:c},s);o!==t&&(l.modifiers=o);var f=u[n];Array.isArray(f)?i?f.unshift(l):f.push(l):u[n]=f?i?[l,f]:[f,l]:l,e.plain=!1}function Ho(t,e,n){var r=Bo(t,":"+e)||Bo(t,"v-bind:"+e);if(null!=r)return jo(r);if(!1!==n){var o=Bo(t,e);if(null!=o)return JSON.stringify(o)}}function Bo(t,e,n){var r;if(null!=(r=t.attrsMap[e]))for(var o=t.attrsList,i=0,a=o.length;i<a;i++)if(o[i].name===e){o.splice(i,1);break}return n&&delete t.attrsMap[e],r}function Uo(t,e){for(var n=t.attrsList,r=0,o=n.length;r<o;r++){var i=n[r];if(e.test(i.name))return n.splice(r,1),i}}function zo(t,e){return e&&(null!=e.start&&(t.start=e.start),null!=e.end&&(t.end=e.end)),t}function Vo(t,e,n){var r=n||{},o=r.number,i="$$v",a=i;r.trim&&(a="(typeof ".concat(i," === 'string'")+"? ".concat(i,".trim()")+": ".concat(i,")")),o&&(a="_n(".concat(a,")"));var s=Ko(e,a);t.model={value:"(".concat(e,")"),expression:JSON.stringify(e),callback:"function (".concat(i,") {").concat(s,"}")}}function Ko(t,e){var n=function(t){if(t=t.trim(),wo=t.length,t.indexOf("[")<0||t.lastIndexOf("]")<wo-1)return(ko=t.lastIndexOf("."))>-1?{exp:t.slice(0,ko),key:'"'+t.slice(ko+1)+'"'}:{exp:t,key:null};xo=t,ko=So=Oo=0;for(;!qo();)Wo(Co=Jo())?Go(Co):91===Co&&Zo(Co);return{exp:t.slice(0,So),key:t.slice(So+1,Oo)}}(t);return null===n.key?"".concat(t,"=").concat(e):"$set(".concat(n.exp,", ").concat(n.key,", ").concat(e,")")}function Jo(){return xo.charCodeAt(++ko)}function qo(){return ko>=wo}function Wo(t){return 34===t||39===t}function Zo(t){var e=1;for(So=ko;!qo();)if(Wo(t=Jo()))Go(t);else if(91===t&&e++,93===t&&e--,0===e){Oo=ko;break}}function Go(t){for(var e=t;!qo()&&(t=Jo())!==e;);}var Xo,Yo="__r";function Qo(t,e,n){var r=Xo;return function o(){var i=e.apply(null,arguments);null!==i&&ni(t,o,n,r)}}var ti=mn&&!(Q&&Number(Q[1])<=53);function ei(t,e,n,r){if(ti){var o=We,i=e;e=i._wrapper=function(t){if(t.target===t.currentTarget||t.timeStamp>=o||t.timeStamp<=0||t.target.ownerDocument!==document)return i.apply(this,arguments)}}Xo.addEventListener(t,e,et?{capture:n,passive:r}:n)}function ni(t,e,n,r){(r||Xo).removeEventListener(t,e._wrapper||e,n)}function ri(t,e){if(!n(t.data.on)||!n(e.data.on)){var o=e.data.on||{},i=t.data.on||{};Xo=e.elm||t.elm,function(t){if(r(t.__r)){var e=W?"change":"input";t[e]=[].concat(t.__r,t[e]||[]),delete t.__r}r(t.__c)&&(t.change=[].concat(t.__c,t.change||[]),delete t.__c)}(o),Wt(o,i,ei,ni,Qo,e.context),Xo=void 0}}var oi,ii={create:ri,update:ri,destroy:function(t){return ri(t,io)}};function ai(t,e){if(!n(t.data.domProps)||!n(e.data.domProps)){var i,a,s=e.elm,c=t.data.domProps||{},u=e.data.domProps||{};for(i in(r(u.__ob__)||o(u._v_attr_proxy))&&(u=e.data.domProps=T({},u)),c)i in u||(s[i]="");for(i in u){if(a=u[i],"textContent"===i||"innerHTML"===i){if(e.children&&(e.children.length=0),a===c[i])continue;1===s.childNodes.length&&s.removeChild(s.childNodes[0])}if("value"===i&&"PROGRESS"!==s.tagName){s._value=a;var l=n(a)?"":String(a);si(s,l)&&(s.value=l)}else if("innerHTML"===i&&Zr(s.tagName)&&n(s.innerHTML)){(oi=oi||document.createElement("div")).innerHTML="<svg>".concat(a,"</svg>");for(var f=oi.firstChild;s.firstChild;)s.removeChild(s.firstChild);for(;f.firstChild;)s.appendChild(f.firstChild)}else if(a!==c[i])try{s[i]=a}catch(t){}}}}function si(t,e){return!t.composing&&("OPTION"===t.tagName||function(t,e){var n=!0;try{n=document.activeElement!==t}catch(t){}return n&&t.value!==e}(t,e)||function(t,e){var n=t.value,o=t._vModifiers;if(r(o)){if(o.number)return p(n)!==p(e);if(o.trim)return n.trim()!==e.trim()}return n!==e}(t,e))}var ci={create:ai,update:ai},ui=b((function(t){var e={},n=/:(.+)/;return t.split(/;(?![^(]*\))/g).forEach((function(t){if(t){var r=t.split(n);r.length>1&&(e[r[0].trim()]=r[1].trim())}})),e}));function li(t){var e=fi(t.style);return t.staticStyle?T(t.staticStyle,e):e}function fi(t){return Array.isArray(t)?A(t):"string"==typeof t?ui(t):t}var di,pi=/^--/,vi=/\s*!important$/,hi=function(t,e,n){if(pi.test(e))t.style.setProperty(e,n);else if(vi.test(n))t.style.setProperty(k(e),n.replace(vi,""),"important");else{var r=gi(e);if(Array.isArray(n))for(var o=0,i=n.length;o<i;o++)t.style[r]=n[o];else t.style[r]=n}},mi=["Webkit","Moz","ms"],gi=b((function(t){if(di=di||document.createElement("div").style,"filter"!==(t=w(t))&&t in di)return t;for(var e=t.charAt(0).toUpperCase()+t.slice(1),n=0;n<mi.length;n++){var r=mi[n]+e;if(r in di)return r}}));function yi(t,e){var o=e.data,i=t.data;if(!(n(o.staticStyle)&&n(o.style)&&n(i.staticStyle)&&n(i.style))){var a,s,c=e.elm,u=i.staticStyle,l=i.normalizedStyle||i.style||{},f=u||l,d=fi(e.data.style)||{};e.data.normalizedStyle=r(d.__ob__)?T({},d):d;var p=function(t,e){var n,r={};if(e)for(var o=t;o.componentInstance;)(o=o.componentInstance._vnode)&&o.data&&(n=li(o.data))&&T(r,n);(n=li(t.data))&&T(r,n);for(var i=t;i=i.parent;)i.data&&(n=li(i.data))&&T(r,n);return r}(e,!0);for(s in f)n(p[s])&&hi(c,s,"");for(s in p)(a=p[s])!==f[s]&&hi(c,s,null==a?"":a)}}var _i={create:yi,update:yi},bi=/\s+/;function $i(t,e){if(e&&(e=e.trim()))if(t.classList)e.indexOf(" ")>-1?e.split(bi).forEach((function(e){return t.classList.add(e)})):t.classList.add(e);else{var n=" ".concat(t.getAttribute("class")||""," ");n.indexOf(" "+e+" ")<0&&t.setAttribute("class",(n+e).trim())}}function wi(t,e){if(e&&(e=e.trim()))if(t.classList)e.indexOf(" ")>-1?e.split(bi).forEach((function(e){return t.classList.remove(e)})):t.classList.remove(e),t.classList.length||t.removeAttribute("class");else{for(var n=" ".concat(t.getAttribute("class")||""," "),r=" "+e+" ";n.indexOf(r)>=0;)n=n.replace(r," ");(n=n.trim())?t.setAttribute("class",n):t.removeAttribute("class")}}function xi(t){if(t){if("object"==typeof t){var e={};return!1!==t.css&&T(e,Ci(t.name||"v")),T(e,t),e}return"string"==typeof t?Ci(t):void 0}}var Ci=b((function(t){return{enterClass:"".concat(t,"-enter"),enterToClass:"".concat(t,"-enter-to"),enterActiveClass:"".concat(t,"-enter-active"),leaveClass:"".concat(t,"-leave"),leaveToClass:"".concat(t,"-leave-to"),leaveActiveClass:"".concat(t,"-leave-active")}})),ki=J&&!Z,Si="transition",Oi="animation",Ti="transition",Ai="transitionend",ji="animation",Ei="animationend";ki&&(void 0===window.ontransitionend&&void 0!==window.onwebkittransitionend&&(Ti="WebkitTransition",Ai="webkitTransitionEnd"),void 0===window.onanimationend&&void 0!==window.onwebkitanimationend&&(ji="WebkitAnimation",Ei="webkitAnimationEnd"));var Ni=J?window.requestAnimationFrame?window.requestAnimationFrame.bind(window):setTimeout:function(t){return t()};function Pi(t){Ni((function(){Ni(t)}))}function Di(t,e){var n=t._transitionClasses||(t._transitionClasses=[]);n.indexOf(e)<0&&(n.push(e),$i(t,e))}function Mi(t,e){t._transitionClasses&&g(t._transitionClasses,e),wi(t,e)}function Ii(t,e,n){var r=Ri(t,e),o=r.type,i=r.timeout,a=r.propCount;if(!o)return n();var s=o===Si?Ai:Ei,c=0,u=function(){t.removeEventListener(s,l),n()},l=function(e){e.target===t&&++c>=a&&u()};setTimeout((function(){c<a&&u()}),i+1),t.addEventListener(s,l)}var Li=/\b(transform|all)(,|$)/;function Ri(t,e){var n,r=window.getComputedStyle(t),o=(r[Ti+"Delay"]||"").split(", "),i=(r[Ti+"Duration"]||"").split(", "),a=Fi(o,i),s=(r[ji+"Delay"]||"").split(", "),c=(r[ji+"Duration"]||"").split(", "),u=Fi(s,c),l=0,f=0;return e===Si?a>0&&(n=Si,l=a,f=i.length):e===Oi?u>0&&(n=Oi,l=u,f=c.length):f=(n=(l=Math.max(a,u))>0?a>u?Si:Oi:null)?n===Si?i.length:c.length:0,{type:n,timeout:l,propCount:f,hasTransform:n===Si&&Li.test(r[Ti+"Property"])}}function Fi(t,e){for(;t.length<e.length;)t=t.concat(t);return Math.max.apply(null,e.map((function(e,n){return Hi(e)+Hi(t[n])})))}function Hi(t){return 1e3*Number(t.slice(0,-1).replace(",","."))}function Bi(t,e){var o=t.elm;r(o._leaveCb)&&(o._leaveCb.cancelled=!0,o._leaveCb());var i=xi(t.data.transition);if(!n(i)&&!r(o._enterCb)&&1===o.nodeType){for(var c=i.css,u=i.type,l=i.enterClass,f=i.enterToClass,d=i.enterActiveClass,v=i.appearClass,h=i.appearToClass,m=i.appearActiveClass,g=i.beforeEnter,y=i.enter,_=i.afterEnter,b=i.enterCancelled,$=i.beforeAppear,w=i.appear,x=i.afterAppear,C=i.appearCancelled,k=i.duration,S=Ie,O=Ie.$vnode;O&&O.parent;)S=O.context,O=O.parent;var T=!S._isMounted||!t.isRootInsert;if(!T||w||""===w){var A=T&&v?v:l,j=T&&m?m:d,E=T&&h?h:f,N=T&&$||g,P=T&&a(w)?w:y,D=T&&x||_,I=T&&C||b,L=p(s(k)?k.enter:k),R=!1!==c&&!Z,F=Vi(P),H=o._enterCb=M((function(){R&&(Mi(o,E),Mi(o,j)),H.cancelled?(R&&Mi(o,A),I&&I(o)):D&&D(o),o._enterCb=null}));t.data.show||Zt(t,"insert",(function(){var e=o.parentNode,n=e&&e._pending&&e._pending[t.key];n&&n.tag===t.tag&&n.elm._leaveCb&&n.elm._leaveCb(),P&&P(o,H)})),N&&N(o),R&&(Di(o,A),Di(o,j),Pi((function(){Mi(o,A),H.cancelled||(Di(o,E),F||(zi(L)?setTimeout(H,L):Ii(o,u,H)))}))),t.data.show&&(e&&e(),P&&P(o,H)),R||F||H()}}}function Ui(t,e){var o=t.elm;r(o._enterCb)&&(o._enterCb.cancelled=!0,o._enterCb());var i=xi(t.data.transition);if(n(i)||1!==o.nodeType)return e();if(!r(o._leaveCb)){var a=i.css,c=i.type,u=i.leaveClass,l=i.leaveToClass,f=i.leaveActiveClass,d=i.beforeLeave,v=i.leave,h=i.afterLeave,m=i.leaveCancelled,g=i.delayLeave,y=i.duration,_=!1!==a&&!Z,b=Vi(v),$=p(s(y)?y.leave:y),w=o._leaveCb=M((function(){o.parentNode&&o.parentNode._pending&&(o.parentNode._pending[t.key]=null),_&&(Mi(o,l),Mi(o,f)),w.cancelled?(_&&Mi(o,u),m&&m(o)):(e(),h&&h(o)),o._leaveCb=null}));g?g(x):x()}function x(){w.cancelled||(!t.data.show&&o.parentNode&&((o.parentNode._pending||(o.parentNode._pending={}))[t.key]=t),d&&d(o),_&&(Di(o,u),Di(o,f),Pi((function(){Mi(o,u),w.cancelled||(Di(o,l),b||(zi($)?setTimeout(w,$):Ii(o,c,w)))}))),v&&v(o,w),_||b||w())}}function zi(t){return"number"==typeof t&&!isNaN(t)}function Vi(t){if(n(t))return!1;var e=t.fns;return r(e)?Vi(Array.isArray(e)?e[0]:e):(t._length||t.length)>1}function Ki(t,e){!0!==e.data.show&&Bi(e)}var Ji=function(t){var a,s,c={},u=t.modules,l=t.nodeOps;for(a=0;a<ao.length;++a)for(c[ao[a]]=[],s=0;s<u.length;++s)r(u[s][ao[a]])&&c[ao[a]].push(u[s][ao[a]]);function f(t){var e=l.parentNode(t);r(e)&&l.removeChild(e,t)}function d(t,e,n,i,a,s,u){if(r(t.elm)&&r(s)&&(t=s[u]=pt(t)),t.isRootInsert=!a,!function(t,e,n,i){var a=t.data;if(r(a)){var s=r(t.componentInstance)&&a.keepAlive;if(r(a=a.hook)&&r(a=a.init)&&a(t,!1),r(t.componentInstance))return p(t,e),h(n,t.elm,i),o(s)&&function(t,e,n,o){var i,a=t;for(;a.componentInstance;)if(r(i=(a=a.componentInstance._vnode).data)&&r(i=i.transition)){for(i=0;i<c.activate.length;++i)c.activate[i](io,a);e.push(a);break}h(n,t.elm,o)}(t,e,n,i),!0}}(t,e,n,i)){var f=t.data,d=t.children,v=t.tag;r(v)?(t.elm=t.ns?l.createElementNS(t.ns,v):l.createElement(v,t),_(t),m(t,d,e),r(f)&&y(t,e),h(n,t.elm,i)):o(t.isComment)?(t.elm=l.createComment(t.text),h(n,t.elm,i)):(t.elm=l.createTextNode(t.text),h(n,t.elm,i))}}function p(t,e){r(t.data.pendingInsert)&&(e.push.apply(e,t.data.pendingInsert),t.data.pendingInsert=null),t.elm=t.componentInstance.$el,g(t)?(y(t,e),_(t)):(ro(t),e.push(t))}function h(t,e,n){r(t)&&(r(n)?l.parentNode(n)===t&&l.insertBefore(t,e,n):l.appendChild(t,e))}function m(t,n,r){if(e(n))for(var o=0;o<n.length;++o)d(n[o],r,t.elm,null,!0,n,o);else i(t.text)&&l.appendChild(t.elm,l.createTextNode(String(t.text)))}function g(t){for(;t.componentInstance;)t=t.componentInstance._vnode;return r(t.tag)}function y(t,e){for(var n=0;n<c.create.length;++n)c.create[n](io,t);r(a=t.data.hook)&&(r(a.create)&&a.create(io,t),r(a.insert)&&e.push(t))}function _(t){var e;if(r(e=t.fnScopeId))l.setStyleScope(t.elm,e);else for(var n=t;n;)r(e=n.context)&&r(e=e.$options._scopeId)&&l.setStyleScope(t.elm,e),n=n.parent;r(e=Ie)&&e!==t.context&&e!==t.fnContext&&r(e=e.$options._scopeId)&&l.setStyleScope(t.elm,e)}function b(t,e,n,r,o,i){for(;r<=o;++r)d(n[r],i,t,e,!1,n,r)}function $(t){var e,n,o=t.data;if(r(o))for(r(e=o.hook)&&r(e=e.destroy)&&e(t),e=0;e<c.destroy.length;++e)c.destroy[e](t);if(r(e=t.children))for(n=0;n<t.children.length;++n)$(t.children[n])}function w(t,e,n){for(;e<=n;++e){var o=t[e];r(o)&&(r(o.tag)?(x(o),$(o)):f(o.elm))}}function x(t,e){if(r(e)||r(t.data)){var n,o=c.remove.length+1;for(r(e)?e.listeners+=o:e=function(t,e){function n(){0==--n.listeners&&f(t)}return n.listeners=e,n}(t.elm,o),r(n=t.componentInstance)&&r(n=n._vnode)&&r(n.data)&&x(n,e),n=0;n<c.remove.length;++n)c.remove[n](t,e);r(n=t.data.hook)&&r(n=n.remove)?n(t,e):e()}else f(t.elm)}function C(t,e,n,o){for(var i=n;i<o;i++){var a=e[i];if(r(a)&&so(t,a))return i}}function k(t,e,i,a,s,u){if(t!==e){r(e.elm)&&r(a)&&(e=a[s]=pt(e));var f=e.elm=t.elm;if(o(t.isAsyncPlaceholder))r(e.asyncFactory.resolved)?T(t.elm,e,i):e.isAsyncPlaceholder=!0;else if(o(e.isStatic)&&o(t.isStatic)&&e.key===t.key&&(o(e.isCloned)||o(e.isOnce)))e.componentInstance=t.componentInstance;else{var p,v=e.data;r(v)&&r(p=v.hook)&&r(p=p.prepatch)&&p(t,e);var h=t.children,m=e.children;if(r(v)&&g(e)){for(p=0;p<c.update.length;++p)c.update[p](t,e);r(p=v.hook)&&r(p=p.update)&&p(t,e)}n(e.text)?r(h)&&r(m)?h!==m&&function(t,e,o,i,a){for(var s,c,u,f=0,p=0,v=e.length-1,h=e[0],m=e[v],g=o.length-1,y=o[0],_=o[g],$=!a;f<=v&&p<=g;)n(h)?h=e[++f]:n(m)?m=e[--v]:so(h,y)?(k(h,y,i,o,p),h=e[++f],y=o[++p]):so(m,_)?(k(m,_,i,o,g),m=e[--v],_=o[--g]):so(h,_)?(k(h,_,i,o,g),$&&l.insertBefore(t,h.elm,l.nextSibling(m.elm)),h=e[++f],_=o[--g]):so(m,y)?(k(m,y,i,o,p),$&&l.insertBefore(t,m.elm,h.elm),m=e[--v],y=o[++p]):(n(s)&&(s=co(e,f,v)),n(c=r(y.key)?s[y.key]:C(y,e,f,v))?d(y,i,t,h.elm,!1,o,p):so(u=e[c],y)?(k(u,y,i,o,p),e[c]=void 0,$&&l.insertBefore(t,u.elm,h.elm)):d(y,i,t,h.elm,!1,o,p),y=o[++p]);f>v?b(t,n(o[g+1])?null:o[g+1].elm,o,p,g,i):p>g&&w(e,f,v)}(f,h,m,i,u):r(m)?(r(t.text)&&l.setTextContent(f,""),b(f,null,m,0,m.length-1,i)):r(h)?w(h,0,h.length-1):r(t.text)&&l.setTextContent(f,""):t.text!==e.text&&l.setTextContent(f,e.text),r(v)&&r(p=v.hook)&&r(p=p.postpatch)&&p(t,e)}}}function S(t,e,n){if(o(n)&&r(t.parent))t.parent.data.pendingInsert=e;else for(var i=0;i<e.length;++i)e[i].data.hook.insert(e[i])}var O=v("attrs,class,staticClass,staticStyle,key");function T(t,e,n,i){var a,s=e.tag,c=e.data,u=e.children;if(i=i||c&&c.pre,e.elm=t,o(e.isComment)&&r(e.asyncFactory))return e.isAsyncPlaceholder=!0,!0;if(r(c)&&(r(a=c.hook)&&r(a=a.init)&&a(e,!0),r(a=e.componentInstance)))return p(e,n),!0;if(r(s)){if(r(u))if(t.hasChildNodes())if(r(a=c)&&r(a=a.domProps)&&r(a=a.innerHTML)){if(a!==t.innerHTML)return!1}else{for(var l=!0,f=t.firstChild,d=0;d<u.length;d++){if(!f||!T(f,u[d],n,i)){l=!1;break}f=f.nextSibling}if(!l||f)return!1}else m(e,u,n);if(r(c)){var v=!1;for(var h in c)if(!O(h)){v=!0,y(e,n);break}!v&&c.class&&Bn(c.class)}}else t.data!==e.text&&(t.data=e.text);return!0}return function(t,e,i,a){if(!n(e)){var s,u=!1,f=[];if(n(t))u=!0,d(e,f);else{var p=r(t.nodeType);if(!p&&so(t,e))k(t,e,f,null,null,a);else{if(p){if(1===t.nodeType&&t.hasAttribute(L)&&(t.removeAttribute(L),i=!0),o(i)&&T(t,e,f))return S(e,f,!0),t;s=t,t=new lt(l.tagName(s).toLowerCase(),{},[],void 0,s)}var v=t.elm,h=l.parentNode(v);if(d(e,f,v._leaveCb?null:h,l.nextSibling(v)),r(e.parent))for(var m=e.parent,y=g(e);m;){for(var _=0;_<c.destroy.length;++_)c.destroy[_](m);if(m.elm=e.elm,y){for(var b=0;b<c.create.length;++b)c.create[b](io,m);var x=m.data.hook.insert;if(x.merged)for(var C=1;C<x.fns.length;C++)x.fns[C]()}else ro(m);m=m.parent}r(h)?w([t],0,0):r(t.tag)&&$(t)}}return S(e,f,u),e.elm}r(t)&&$(t)}}({nodeOps:eo,modules:[bo,To,ii,ci,_i,J?{create:Ki,activate:Ki,remove:function(t,e){!0!==t.data.show?Ui(t,e):e()}}:{}].concat(mo)});Z&&document.addEventListener("selectionchange",(function(){var t=document.activeElement;t&&t.vmodel&&ta(t,"input")}));var qi={inserted:function(t,e,n,r){"select"===n.tag?(r.elm&&!r.elm._vOptions?Zt(n,"postpatch",(function(){qi.componentUpdated(t,e,n)})):Wi(t,e,n.context),t._vOptions=[].map.call(t.options,Xi)):("textarea"===n.tag||Qr(t.type))&&(t._vModifiers=e.modifiers,e.modifiers.lazy||(t.addEventListener("compositionstart",Yi),t.addEventListener("compositionend",Qi),t.addEventListener("change",Qi),Z&&(t.vmodel=!0)))},componentUpdated:function(t,e,n){if("select"===n.tag){Wi(t,e,n.context);var r=t._vOptions,o=t._vOptions=[].map.call(t.options,Xi);if(o.some((function(t,e){return!P(t,r[e])})))(t.multiple?e.value.some((function(t){return Gi(t,o)})):e.value!==e.oldValue&&Gi(e.value,o))&&ta(t,"change")}}};function Wi(t,e,n){Zi(t,e),(W||G)&&setTimeout((function(){Zi(t,e)}),0)}function Zi(t,e,n){var r=e.value,o=t.multiple;if(!o||Array.isArray(r)){for(var i,a,s=0,c=t.options.length;s<c;s++)if(a=t.options[s],o)i=D(r,Xi(a))>-1,a.selected!==i&&(a.selected=i);else if(P(Xi(a),r))return void(t.selectedIndex!==s&&(t.selectedIndex=s));o||(t.selectedIndex=-1)}}function Gi(t,e){return e.every((function(e){return!P(e,t)}))}function Xi(t){return"_value"in t?t._value:t.value}function Yi(t){t.target.composing=!0}function Qi(t){t.target.composing&&(t.target.composing=!1,ta(t.target,"input"))}function ta(t,e){var n=document.createEvent("HTMLEvents");n.initEvent(e,!0,!0),t.dispatchEvent(n)}function ea(t){return!t.componentInstance||t.data&&t.data.transition?t:ea(t.componentInstance._vnode)}var na={bind:function(t,e,n){var r=e.value,o=(n=ea(n)).data&&n.data.transition,i=t.__vOriginalDisplay="none"===t.style.display?"":t.style.display;r&&o?(n.data.show=!0,Bi(n,(function(){t.style.display=i}))):t.style.display=r?i:"none"},update:function(t,e,n){var r=e.value;!r!=!e.oldValue&&((n=ea(n)).data&&n.data.transition?(n.data.show=!0,r?Bi(n,(function(){t.style.display=t.__vOriginalDisplay})):Ui(n,(function(){t.style.display="none"}))):t.style.display=r?t.__vOriginalDisplay:"none")},unbind:function(t,e,n,r,o){o||(t.style.display=t.__vOriginalDisplay)}},ra={model:qi,show:na},oa={name:String,appear:Boolean,css:Boolean,mode:String,type:String,enterClass:String,leaveClass:String,enterToClass:String,leaveToClass:String,enterActiveClass:String,leaveActiveClass:String,appearClass:String,appearActiveClass:String,appearToClass:String,duration:[Number,String,Object]};function ia(t){var e=t&&t.componentOptions;return e&&e.Ctor.options.abstract?ia(Ee(e.children)):t}function aa(t){var e={},n=t.$options;for(var r in n.propsData)e[r]=t[r];var o=n._parentListeners;for(var r in o)e[w(r)]=o[r];return e}function sa(t,e){if(/\d-keep-alive$/.test(e.tag))return t("keep-alive",{props:e.componentOptions.propsData})}var ca=function(t){return t.tag||_e(t)},ua=function(t){return"show"===t.name},la={name:"transition",props:oa,abstract:!0,render:function(t){var e=this,n=this.$slots.default;if(n&&(n=n.filter(ca)).length){var r=this.mode,o=n[0];if(function(t){for(;t=t.parent;)if(t.data.transition)return!0}(this.$vnode))return o;var a=ia(o);if(!a)return o;if(this._leaving)return sa(t,o);var s="__transition-".concat(this._uid,"-");a.key=null==a.key?a.isComment?s+"comment":s+a.tag:i(a.key)?0===String(a.key).indexOf(s)?a.key:s+a.key:a.key;var c=(a.data||(a.data={})).transition=aa(this),u=this._vnode,l=ia(u);if(a.data.directives&&a.data.directives.some(ua)&&(a.data.show=!0),l&&l.data&&!function(t,e){return e.key===t.key&&e.tag===t.tag}(a,l)&&!_e(l)&&(!l.componentInstance||!l.componentInstance._vnode.isComment)){var f=l.data.transition=T({},c);if("out-in"===r)return this._leaving=!0,Zt(f,"afterLeave",(function(){e._leaving=!1,e.$forceUpdate()})),sa(t,o);if("in-out"===r){if(_e(a))return u;var d,p=function(){d()};Zt(c,"afterEnter",p),Zt(c,"enterCancelled",p),Zt(f,"delayLeave",(function(t){d=t}))}}return o}}},fa=T({tag:String,moveClass:String},oa);delete fa.mode;var da={props:fa,beforeMount:function(){var t=this,e=this._update;this._update=function(n,r){var o=Le(t);t.__patch__(t._vnode,t.kept,!1,!0),t._vnode=t.kept,o(),e.call(t,n,r)}},render:function(t){for(var e=this.tag||this.$vnode.data.tag||"span",n=Object.create(null),r=this.prevChildren=this.children,o=this.$slots.default||[],i=this.children=[],a=aa(this),s=0;s<o.length;s++){(l=o[s]).tag&&null!=l.key&&0!==String(l.key).indexOf("__vlist")&&(i.push(l),n[l.key]=l,(l.data||(l.data={})).transition=a)}if(r){var c=[],u=[];for(s=0;s<r.length;s++){var l;(l=r[s]).data.transition=a,l.data.pos=l.elm.getBoundingClientRect(),n[l.key]?c.push(l):u.push(l)}this.kept=t(e,null,c),this.removed=u}return t(e,null,i)},updated:function(){var t=this.prevChildren,e=this.moveClass||(this.name||"v")+"-move";t.length&&this.hasMove(t[0].elm,e)&&(t.forEach(pa),t.forEach(va),t.forEach(ha),this._reflow=document.body.offsetHeight,t.forEach((function(t){if(t.data.moved){var n=t.elm,r=n.style;Di(n,e),r.transform=r.WebkitTransform=r.transitionDuration="",n.addEventListener(Ai,n._moveCb=function t(r){r&&r.target!==n||r&&!/transform$/.test(r.propertyName)||(n.removeEventListener(Ai,t),n._moveCb=null,Mi(n,e))})}})))},methods:{hasMove:function(t,e){if(!ki)return!1;if(this._hasMove)return this._hasMove;var n=t.cloneNode();t._transitionClasses&&t._transitionClasses.forEach((function(t){wi(n,t)})),$i(n,e),n.style.display="none",this.$el.appendChild(n);var r=Ri(n);return this.$el.removeChild(n),this._hasMove=r.hasTransform}}};function pa(t){t.elm._moveCb&&t.elm._moveCb(),t.elm._enterCb&&t.elm._enterCb()}function va(t){t.data.newPos=t.elm.getBoundingClientRect()}function ha(t){var e=t.data.pos,n=t.data.newPos,r=e.left-n.left,o=e.top-n.top;if(r||o){t.data.moved=!0;var i=t.elm.style;i.transform=i.WebkitTransform="translate(".concat(r,"px,").concat(o,"px)"),i.transitionDuration="0s"}}var ma={Transition:la,TransitionGroup:da};Cr.config.mustUseProp=Mr,Cr.config.isReservedTag=Gr,Cr.config.isReservedAttr=Pr,Cr.config.getTagNamespace=Xr,Cr.config.isUnknownElement=function(t){if(!J)return!0;if(Gr(t))return!1;if(t=t.toLowerCase(),null!=Yr[t])return Yr[t];var e=document.createElement(t);return t.indexOf("-")>-1?Yr[t]=e.constructor===window.HTMLUnknownElement||e.constructor===window.HTMLElement:Yr[t]=/HTMLUnknownElement/.test(e.toString())},T(Cr.options.directives,ra),T(Cr.options.components,ma),Cr.prototype.__patch__=J?Ji:j,Cr.prototype.$mount=function(t,e){return function(t,e,n){var r;t.$el=e,t.$options.render||(t.$options.render=ft),Be(t,"beforeMount"),r=function(){t._update(t._render(),n)},new Vn(t,r,j,{before:function(){t._isMounted&&!t._isDestroyed&&Be(t,"beforeUpdate")}},!0),n=!1;var o=t._preWatchers;if(o)for(var i=0;i<o.length;i++)o[i].run();return null==t.$vnode&&(t._isMounted=!0,Be(t,"mounted")),t}(this,t=t&&J?to(t):void 0,e)},J&&setTimeout((function(){H.devtools&&ot&&ot.emit("init",Cr)}),0);var ga=/\{\{((?:.|\r?\n)+?)\}\}/g,ya=/[-.*+?^${}()|[\]\/\\]/g,_a=b((function(t){var e=t[0].replace(ya,"\\$&"),n=t[1].replace(ya,"\\$&");return new RegExp(e+"((?:.|\\n)+?)"+n,"g")}));var ba={staticKeys:["staticClass"],transformNode:function(t,e){e.warn;var n=Bo(t,"class");n&&(t.staticClass=JSON.stringify(n.replace(/\s+/g," ").trim()));var r=Ho(t,"class",!1);r&&(t.classBinding=r)},genData:function(t){var e="";return t.staticClass&&(e+="staticClass:".concat(t.staticClass,",")),t.classBinding&&(e+="class:".concat(t.classBinding,",")),e}};var $a,wa={staticKeys:["staticStyle"],transformNode:function(t,e){e.warn;var n=Bo(t,"style");n&&(t.staticStyle=JSON.stringify(ui(n)));var r=Ho(t,"style",!1);r&&(t.styleBinding=r)},genData:function(t){var e="";return t.staticStyle&&(e+="staticStyle:".concat(t.staticStyle,",")),t.styleBinding&&(e+="style:(".concat(t.styleBinding,"),")),e}},xa=function(t){return($a=$a||document.createElement("div")).innerHTML=t,$a.textContent},Ca=v("area,base,br,col,embed,frame,hr,img,input,isindex,keygen,link,meta,param,source,track,wbr"),ka=v("colgroup,dd,dt,li,options,p,td,tfoot,th,thead,tr,source"),Sa=v("address,article,aside,base,blockquote,body,caption,col,colgroup,dd,details,dialog,div,dl,dt,fieldset,figcaption,figure,footer,form,h1,h2,h3,h4,h5,h6,head,header,hgroup,hr,html,legend,li,menuitem,meta,optgroup,option,param,rp,rt,source,style,summary,tbody,td,tfoot,th,thead,title,tr,track"),Oa=/^\s*([^\s"'<>\/=]+)(?:\s*(=)\s*(?:"([^"]*)"+|'([^']*)'+|([^\s"'=<>`]+)))?/,Ta=/^\s*((?:v-[\w-]+:|@|:|#)\[[^=]+?\][^\s"'<>\/=]*)(?:\s*(=)\s*(?:"([^"]*)"+|'([^']*)'+|([^\s"'=<>`]+)))?/,Aa="[a-zA-Z_][\\-\\.0-9_a-zA-Z".concat(B.source,"]*"),ja="((?:".concat(Aa,"\\:)?").concat(Aa,")"),Ea=new RegExp("^<".concat(ja)),Na=/^\s*(\/?)>/,Pa=new RegExp("^<\\/".concat(ja,"[^>]*>")),Da=/^<!DOCTYPE [^>]+>/i,Ma=/^<!\--/,Ia=/^<!\[/,La=v("script,style,textarea",!0),Ra={},Fa={"&lt;":"<","&gt;":">","&quot;":'"',"&amp;":"&","&#10;":"\n","&#9;":"\t","&#39;":"'"},Ha=/&(?:lt|gt|quot|amp|#39);/g,Ba=/&(?:lt|gt|quot|amp|#39|#10|#9);/g,Ua=v("pre,textarea",!0),za=function(t,e){return t&&Ua(t)&&"\n"===e[0]};function Va(t,e){var n=e?Ba:Ha;return t.replace(n,(function(t){return Fa[t]}))}function Ka(t,e){for(var n,r,o=[],i=e.expectHTML,a=e.isUnaryTag||E,s=e.canBeLeftOpenTag||E,c=0,u=function(){if(n=t,r&&La(r)){var u=0,d=r.toLowerCase(),p=Ra[d]||(Ra[d]=new RegExp("([\\s\\S]*?)(</"+d+"[^>]*>)","i"));w=t.replace(p,(function(t,n,r){return u=r.length,La(d)||"noscript"===d||(n=n.replace(/<!\--([\s\S]*?)-->/g,"$1").replace(/<!\[CDATA\[([\s\S]*?)]]>/g,"$1")),za(d,n)&&(n=n.slice(1)),e.chars&&e.chars(n),""}));c+=t.length-w.length,t=w,f(d,c-u,c)}else{var v=t.indexOf("<");if(0===v){if(Ma.test(t)){var h=t.indexOf("--\x3e");if(h>=0)return e.shouldKeepComment&&e.comment&&e.comment(t.substring(4,h),c,c+h+3),l(h+3),"continue"}if(Ia.test(t)){var m=t.indexOf("]>");if(m>=0)return l(m+2),"continue"}var g=t.match(Da);if(g)return l(g[0].length),"continue";var y=t.match(Pa);if(y){var _=c;return l(y[0].length),f(y[1],_,c),"continue"}var b=function(){var e=t.match(Ea);if(e){var n={tagName:e[1],attrs:[],start:c};l(e[0].length);for(var r=void 0,o=void 0;!(r=t.match(Na))&&(o=t.match(Ta)||t.match(Oa));)o.start=c,l(o[0].length),o.end=c,n.attrs.push(o);if(r)return n.unarySlash=r[1],l(r[0].length),n.end=c,n}}();if(b)return function(t){var n=t.tagName,c=t.unarySlash;i&&("p"===r&&Sa(n)&&f(r),s(n)&&r===n&&f(n));for(var u=a(n)||!!c,l=t.attrs.length,d=new Array(l),p=0;p<l;p++){var v=t.attrs[p],h=v[3]||v[4]||v[5]||"",m="a"===n&&"href"===v[1]?e.shouldDecodeNewlinesForHref:e.shouldDecodeNewlines;d[p]={name:v[1],value:Va(h,m)}}u||(o.push({tag:n,lowerCasedTag:n.toLowerCase(),attrs:d,start:t.start,end:t.end}),r=n);e.start&&e.start(n,d,u,t.start,t.end)}(b),za(b.tagName,t)&&l(1),"continue"}var $=void 0,w=void 0,x=void 0;if(v>=0){for(w=t.slice(v);!(Pa.test(w)||Ea.test(w)||Ma.test(w)||Ia.test(w)||(x=w.indexOf("<",1))<0);)v+=x,w=t.slice(v);$=t.substring(0,v)}v<0&&($=t),$&&l($.length),e.chars&&$&&e.chars($,c-$.length,c)}if(t===n)return e.chars&&e.chars(t),"break"};t;){if("break"===u())break}function l(e){c+=e,t=t.substring(e)}function f(t,n,i){var a,s;if(null==n&&(n=c),null==i&&(i=c),t)for(s=t.toLowerCase(),a=o.length-1;a>=0&&o[a].lowerCasedTag!==s;a--);else a=0;if(a>=0){for(var u=o.length-1;u>=a;u--)e.end&&e.end(o[u].tag,n,i);o.length=a,r=a&&o[a-1].tag}else"br"===s?e.start&&e.start(t,[],!0,n,i):"p"===s&&(e.start&&e.start(t,[],!1,n,i),e.end&&e.end(t,n,i))}f()}var Ja,qa,Wa,Za,Ga,Xa,Ya,Qa,ts=/^@|^v-on:/,es=/^v-|^@|^:|^#/,ns=/([\s\S]*?)\s+(?:in|of)\s+([\s\S]*)/,rs=/,([^,\}\]]*)(?:,([^,\}\]]*))?$/,os=/^\(|\)$/g,is=/^\[.*\]$/,as=/:(.*)$/,ss=/^:|^\.|^v-bind:/,cs=/\.[^.\]]+(?=[^\]]*$)/g,us=/^v-slot(:|$)|^#/,ls=/[\r\n]/,fs=/[ \f\t\r\n]+/g,ds=b(xa),ps="_empty_";function vs(t,e,n){return{type:1,tag:t,attrsList:e,attrsMap:$s(e),rawAttrsMap:{},parent:n,children:[]}}function hs(t,e){Ja=e.warn||No,Xa=e.isPreTag||E,Ya=e.mustUseProp||E,Qa=e.getTagNamespace||E,e.isReservedTag,Wa=Po(e.modules,"transformNode"),Za=Po(e.modules,"preTransformNode"),Ga=Po(e.modules,"postTransformNode"),qa=e.delimiters;var n,r,o=[],i=!1!==e.preserveWhitespace,a=e.whitespace,s=!1,c=!1;function u(t){if(l(t),s||t.processed||(t=ms(t,e)),o.length||t===n||n.if&&(t.elseif||t.else)&&ys(n,{exp:t.elseif,block:t}),r&&!t.forbidden)if(t.elseif||t.else)a=t,u=function(t){for(var e=t.length;e--;){if(1===t[e].type)return t[e];t.pop()}}(r.children),u&&u.if&&ys(u,{exp:a.elseif,block:a});else{if(t.slotScope){var i=t.slotTarget||'"default"';(r.scopedSlots||(r.scopedSlots={}))[i]=t}r.children.push(t),t.parent=r}var a,u;t.children=t.children.filter((function(t){return!t.slotScope})),l(t),t.pre&&(s=!1),Xa(t.tag)&&(c=!1);for(var f=0;f<Ga.length;f++)Ga[f](t,e)}function l(t){if(!c)for(var e=void 0;(e=t.children[t.children.length-1])&&3===e.type&&" "===e.text;)t.children.pop()}return Ka(t,{warn:Ja,expectHTML:e.expectHTML,isUnaryTag:e.isUnaryTag,canBeLeftOpenTag:e.canBeLeftOpenTag,shouldDecodeNewlines:e.shouldDecodeNewlines,shouldDecodeNewlinesForHref:e.shouldDecodeNewlinesForHref,shouldKeepComment:e.comments,outputSourceRange:e.outputSourceRange,start:function(t,i,a,l,f){var d=r&&r.ns||Qa(t);W&&"svg"===d&&(i=function(t){for(var e=[],n=0;n<t.length;n++){var r=t[n];ws.test(r.name)||(r.name=r.name.replace(xs,""),e.push(r))}return e}(i));var p,v=vs(t,i,r);d&&(v.ns=d),"style"!==(p=v).tag&&("script"!==p.tag||p.attrsMap.type&&"text/javascript"!==p.attrsMap.type)||rt()||(v.forbidden=!0);for(var h=0;h<Za.length;h++)v=Za[h](v,e)||v;s||(!function(t){null!=Bo(t,"v-pre")&&(t.pre=!0)}(v),v.pre&&(s=!0)),Xa(v.tag)&&(c=!0),s?function(t){var e=t.attrsList,n=e.length;if(n)for(var r=t.attrs=new Array(n),o=0;o<n;o++)r[o]={name:e[o].name,value:JSON.stringify(e[o].value)},null!=e[o].start&&(r[o].start=e[o].start,r[o].end=e[o].end);else t.pre||(t.plain=!0)}(v):v.processed||(gs(v),function(t){var e=Bo(t,"v-if");if(e)t.if=e,ys(t,{exp:e,block:t});else{null!=Bo(t,"v-else")&&(t.else=!0);var n=Bo(t,"v-else-if");n&&(t.elseif=n)}}(v),function(t){null!=Bo(t,"v-once")&&(t.once=!0)}(v)),n||(n=v),a?u(v):(r=v,o.push(v))},end:function(t,e,n){var i=o[o.length-1];o.length-=1,r=o[o.length-1],u(i)},chars:function(t,e,n){if(r&&(!W||"textarea"!==r.tag||r.attrsMap.placeholder!==t)){var o,u=r.children;if(t=c||t.trim()?"script"===(o=r).tag||"style"===o.tag?t:ds(t):u.length?a?"condense"===a&&ls.test(t)?"":" ":i?" ":"":""){c||"condense"!==a||(t=t.replace(fs," "));var l=void 0,f=void 0;!s&&" "!==t&&(l=function(t,e){var n=e?_a(e):ga;if(n.test(t)){for(var r,o,i,a=[],s=[],c=n.lastIndex=0;r=n.exec(t);){(o=r.index)>c&&(s.push(i=t.slice(c,o)),a.push(JSON.stringify(i)));var u=jo(r[1].trim());a.push("_s(".concat(u,")")),s.push({"@binding":u}),c=o+r[0].length}return c<t.length&&(s.push(i=t.slice(c)),a.push(JSON.stringify(i))),{expression:a.join("+"),tokens:s}}}(t,qa))?f={type:2,expression:l.expression,tokens:l.tokens,text:t}:" "===t&&u.length&&" "===u[u.length-1].text||(f={type:3,text:t}),f&&u.push(f)}}},comment:function(t,e,n){if(r){var o={type:3,text:t,isComment:!0};r.children.push(o)}}}),n}function ms(t,e){var n,r;(r=Ho(n=t,"key"))&&(n.key=r),t.plain=!t.key&&!t.scopedSlots&&!t.attrsList.length,function(t){var e=Ho(t,"ref");e&&(t.ref=e,t.refInFor=function(t){var e=t;for(;e;){if(void 0!==e.for)return!0;e=e.parent}return!1}(t))}(t),function(t){var e;"template"===t.tag?(e=Bo(t,"scope"),t.slotScope=e||Bo(t,"slot-scope")):(e=Bo(t,"slot-scope"))&&(t.slotScope=e);var n=Ho(t,"slot");n&&(t.slotTarget='""'===n?'"default"':n,t.slotTargetDynamic=!(!t.attrsMap[":slot"]&&!t.attrsMap["v-bind:slot"]),"template"===t.tag||t.slotScope||Mo(t,"slot",n,function(t,e){return t.rawAttrsMap[":"+e]||t.rawAttrsMap["v-bind:"+e]||t.rawAttrsMap[e]}(t,"slot")));if("template"===t.tag){if(a=Uo(t,us)){var r=_s(a),o=r.name,i=r.dynamic;t.slotTarget=o,t.slotTargetDynamic=i,t.slotScope=a.value||ps}}else{var a;if(a=Uo(t,us)){var s=t.scopedSlots||(t.scopedSlots={}),c=_s(a),u=c.name,l=(i=c.dynamic,s[u]=vs("template",[],t));l.slotTarget=u,l.slotTargetDynamic=i,l.children=t.children.filter((function(t){if(!t.slotScope)return t.parent=l,!0})),l.slotScope=a.value||ps,t.children=[],t.plain=!1}}}(t),function(t){"slot"===t.tag&&(t.slotName=Ho(t,"name"))}(t),function(t){var e;(e=Ho(t,"is"))&&(t.component=e);null!=Bo(t,"inline-template")&&(t.inlineTemplate=!0)}(t);for(var o=0;o<Wa.length;o++)t=Wa[o](t,e)||t;return function(t){var e,n,r,o,i,a,s,c,u=t.attrsList;for(e=0,n=u.length;e<n;e++)if(r=o=u[e].name,i=u[e].value,es.test(r))if(t.hasBindings=!0,(a=bs(r.replace(es,"")))&&(r=r.replace(cs,"")),ss.test(r))r=r.replace(ss,""),i=jo(i),(c=is.test(r))&&(r=r.slice(1,-1)),a&&(a.prop&&!c&&"innerHtml"===(r=w(r))&&(r="innerHTML"),a.camel&&!c&&(r=w(r)),a.sync&&(s=Ko(i,"$event"),c?Fo(t,'"update:"+('.concat(r,")"),s,null,!1,0,u[e],!0):(Fo(t,"update:".concat(w(r)),s,null,!1,0,u[e]),k(r)!==w(r)&&Fo(t,"update:".concat(k(r)),s,null,!1,0,u[e])))),a&&a.prop||!t.component&&Ya(t.tag,t.attrsMap.type,r)?Do(t,r,i,u[e],c):Mo(t,r,i,u[e],c);else if(ts.test(r))r=r.replace(ts,""),(c=is.test(r))&&(r=r.slice(1,-1)),Fo(t,r,i,a,!1,0,u[e],c);else{var l=(r=r.replace(es,"")).match(as),f=l&&l[1];c=!1,f&&(r=r.slice(0,-(f.length+1)),is.test(f)&&(f=f.slice(1,-1),c=!0)),Lo(t,r,o,i,f,c,a,u[e])}else Mo(t,r,JSON.stringify(i),u[e]),!t.component&&"muted"===r&&Ya(t.tag,t.attrsMap.type,r)&&Do(t,r,"true",u[e])}(t),t}function gs(t){var e;if(e=Bo(t,"v-for")){var n=function(t){var e=t.match(ns);if(!e)return;var n={};n.for=e[2].trim();var r=e[1].trim().replace(os,""),o=r.match(rs);o?(n.alias=r.replace(rs,"").trim(),n.iterator1=o[1].trim(),o[2]&&(n.iterator2=o[2].trim())):n.alias=r;return n}(e);n&&T(t,n)}}function ys(t,e){t.ifConditions||(t.ifConditions=[]),t.ifConditions.push(e)}function _s(t){var e=t.name.replace(us,"");return e||"#"!==t.name[0]&&(e="default"),is.test(e)?{name:e.slice(1,-1),dynamic:!0}:{name:'"'.concat(e,'"'),dynamic:!1}}function bs(t){var e=t.match(cs);if(e){var n={};return e.forEach((function(t){n[t.slice(1)]=!0})),n}}function $s(t){for(var e={},n=0,r=t.length;n<r;n++)e[t[n].name]=t[n].value;return e}var ws=/^xmlns:NS\d+/,xs=/^NS\d+:/;function Cs(t){return vs(t.tag,t.attrsList.slice(),t.parent)}var ks=[ba,wa,{preTransformNode:function(t,e){if("input"===t.tag){var n=t.attrsMap;if(!n["v-model"])return;var r=void 0;if((n[":type"]||n["v-bind:type"])&&(r=Ho(t,"type")),n.type||r||!n["v-bind"]||(r="(".concat(n["v-bind"],").type")),r){var o=Bo(t,"v-if",!0),i=o?"&&(".concat(o,")"):"",a=null!=Bo(t,"v-else",!0),s=Bo(t,"v-else-if",!0),c=Cs(t);gs(c),Io(c,"type","checkbox"),ms(c,e),c.processed=!0,c.if="(".concat(r,")==='checkbox'")+i,ys(c,{exp:c.if,block:c});var u=Cs(t);Bo(u,"v-for",!0),Io(u,"type","radio"),ms(u,e),ys(c,{exp:"(".concat(r,")==='radio'")+i,block:u});var l=Cs(t);return Bo(l,"v-for",!0),Io(l,":type",r),ms(l,e),ys(c,{exp:o,block:l}),a?c.else=!0:s&&(c.elseif=s),c}}}}];var Ss,Os,Ts={model:function(t,e,n){var r=e.value,o=e.modifiers,i=t.tag,a=t.attrsMap.type;if(t.component)return Vo(t,r,o),!1;if("select"===i)!function(t,e,n){var r=n&&n.number,o='Array.prototype.filter.call($event.target.options,function(o){return o.selected}).map(function(o){var val = "_value" in o ? o._value : o.value;'+"return ".concat(r?"_n(val)":"val","})"),i="$event.target.multiple ? $$selectedVal : $$selectedVal[0]",a="var $$selectedVal = ".concat(o,";");a="".concat(a," ").concat(Ko(e,i)),Fo(t,"change",a,null,!0)}(t,r,o);else if("input"===i&&"checkbox"===a)!function(t,e,n){var r=n&&n.number,o=Ho(t,"value")||"null",i=Ho(t,"true-value")||"true",a=Ho(t,"false-value")||"false";Do(t,"checked","Array.isArray(".concat(e,")")+"?_i(".concat(e,",").concat(o,")>-1")+("true"===i?":(".concat(e,")"):":_q(".concat(e,",").concat(i,")"))),Fo(t,"change","var $$a=".concat(e,",")+"$$el=$event.target,"+"$$c=$$el.checked?(".concat(i,"):(").concat(a,");")+"if(Array.isArray($$a)){"+"var $$v=".concat(r?"_n("+o+")":o,",")+"$$i=_i($$a,$$v);"+"if($$el.checked){$$i<0&&(".concat(Ko(e,"$$a.concat([$$v])"),")}")+"else{$$i>-1&&(".concat(Ko(e,"$$a.slice(0,$$i).concat($$a.slice($$i+1))"),")}")+"}else{".concat(Ko(e,"$$c"),"}"),null,!0)}(t,r,o);else if("input"===i&&"radio"===a)!function(t,e,n){var r=n&&n.number,o=Ho(t,"value")||"null";o=r?"_n(".concat(o,")"):o,Do(t,"checked","_q(".concat(e,",").concat(o,")")),Fo(t,"change",Ko(e,o),null,!0)}(t,r,o);else if("input"===i||"textarea"===i)!function(t,e,n){var r=t.attrsMap.type,o=n||{},i=o.lazy,a=o.number,s=o.trim,c=!i&&"range"!==r,u=i?"change":"range"===r?Yo:"input",l="$event.target.value";s&&(l="$event.target.value.trim()");a&&(l="_n(".concat(l,")"));var f=Ko(e,l);c&&(f="if($event.target.composing)return;".concat(f));Do(t,"value","(".concat(e,")")),Fo(t,u,f,null,!0),(s||a)&&Fo(t,"blur","$forceUpdate()")}(t,r,o);else if(!H.isReservedTag(i))return Vo(t,r,o),!1;return!0},text:function(t,e){e.value&&Do(t,"textContent","_s(".concat(e.value,")"),e)},html:function(t,e){e.value&&Do(t,"innerHTML","_s(".concat(e.value,")"),e)}},As={expectHTML:!0,modules:ks,directives:Ts,isPreTag:function(t){return"pre"===t},isUnaryTag:Ca,mustUseProp:Mr,canBeLeftOpenTag:ka,isReservedTag:Gr,getTagNamespace:Xr,staticKeys:function(t){return t.reduce((function(t,e){return t.concat(e.staticKeys||[])}),[]).join(",")}(ks)},js=b((function(t){return v("type,tag,attrsList,attrsMap,plain,parent,children,attrs,start,end,rawAttrsMap"+(t?","+t:""))}));function Es(t,e){t&&(Ss=js(e.staticKeys||""),Os=e.isReservedTag||E,Ns(t),Ps(t,!1))}function Ns(t){if(t.static=function(t){if(2===t.type)return!1;if(3===t.type)return!0;return!(!t.pre&&(t.hasBindings||t.if||t.for||h(t.tag)||!Os(t.tag)||function(t){for(;t.parent;){if("template"!==(t=t.parent).tag)return!1;if(t.for)return!0}return!1}(t)||!Object.keys(t).every(Ss)))}(t),1===t.type){if(!Os(t.tag)&&"slot"!==t.tag&&null==t.attrsMap["inline-template"])return;for(var e=0,n=t.children.length;e<n;e++){var r=t.children[e];Ns(r),r.static||(t.static=!1)}if(t.ifConditions)for(e=1,n=t.ifConditions.length;e<n;e++){var o=t.ifConditions[e].block;Ns(o),o.static||(t.static=!1)}}}function Ps(t,e){if(1===t.type){if((t.static||t.once)&&(t.staticInFor=e),t.static&&t.children.length&&(1!==t.children.length||3!==t.children[0].type))return void(t.staticRoot=!0);if(t.staticRoot=!1,t.children)for(var n=0,r=t.children.length;n<r;n++)Ps(t.children[n],e||!!t.for);if(t.ifConditions)for(n=1,r=t.ifConditions.length;n<r;n++)Ps(t.ifConditions[n].block,e)}}var Ds=/^([\w$_]+|\([^)]*?\))\s*=>|^function(?:\s+[\w$]+)?\s*\(/,Ms=/\([^)]*?\);*$/,Is=/^[A-Za-z_$][\w$]*(?:\.[A-Za-z_$][\w$]*|\['[^']*?']|\["[^"]*?"]|\[\d+]|\[[A-Za-z_$][\w$]*])*$/,Ls={esc:27,tab:9,enter:13,space:32,up:38,left:37,right:39,down:40,delete:[8,46]},Rs={esc:["Esc","Escape"],tab:"Tab",enter:"Enter",space:[" ","Spacebar"],up:["Up","ArrowUp"],left:["Left","ArrowLeft"],right:["Right","ArrowRight"],down:["Down","ArrowDown"],delete:["Backspace","Delete","Del"]},Fs=function(t){return"if(".concat(t,")return null;")},Hs={stop:"$event.stopPropagation();",prevent:"$event.preventDefault();",self:Fs("$event.target !== $event.currentTarget"),ctrl:Fs("!$event.ctrlKey"),shift:Fs("!$event.shiftKey"),alt:Fs("!$event.altKey"),meta:Fs("!$event.metaKey"),left:Fs("'button' in $event && $event.button !== 0"),middle:Fs("'button' in $event && $event.button !== 1"),right:Fs("'button' in $event && $event.button !== 2")};function Bs(t,e){var n=e?"nativeOn:":"on:",r="",o="";for(var i in t){var a=Us(t[i]);t[i]&&t[i].dynamic?o+="".concat(i,",").concat(a,","):r+='"'.concat(i,'":').concat(a,",")}return r="{".concat(r.slice(0,-1),"}"),o?n+"_d(".concat(r,",[").concat(o.slice(0,-1),"])"):n+r}function Us(t){if(!t)return"function(){}";if(Array.isArray(t))return"[".concat(t.map((function(t){return Us(t)})).join(","),"]");var e=Is.test(t.value),n=Ds.test(t.value),r=Is.test(t.value.replace(Ms,""));if(t.modifiers){var o="",i="",a=[],s=function(e){if(Hs[e])i+=Hs[e],Ls[e]&&a.push(e);else if("exact"===e){var n=t.modifiers;i+=Fs(["ctrl","shift","alt","meta"].filter((function(t){return!n[t]})).map((function(t){return"$event.".concat(t,"Key")})).join("||"))}else a.push(e)};for(var c in t.modifiers)s(c);a.length&&(o+=function(t){return"if(!$event.type.indexOf('key')&&"+"".concat(t.map(zs).join("&&"),")return null;")}(a)),i&&(o+=i);var u=e?"return ".concat(t.value,".apply(null, arguments)"):n?"return (".concat(t.value,").apply(null, arguments)"):r?"return ".concat(t.value):t.value;return"function($event){".concat(o).concat(u,"}")}return e||n?t.value:"function($event){".concat(r?"return ".concat(t.value):t.value,"}")}function zs(t){var e=parseInt(t,10);if(e)return"$event.keyCode!==".concat(e);var n=Ls[t],r=Rs[t];return"_k($event.keyCode,"+"".concat(JSON.stringify(t),",")+"".concat(JSON.stringify(n),",")+"$event.key,"+"".concat(JSON.stringify(r))+")"}var Vs={on:function(t,e){t.wrapListeners=function(t){return"_g(".concat(t,",").concat(e.value,")")}},bind:function(t,e){t.wrapData=function(n){return"_b(".concat(n,",'").concat(t.tag,"',").concat(e.value,",").concat(e.modifiers&&e.modifiers.prop?"true":"false").concat(e.modifiers&&e.modifiers.sync?",true":"",")")}},cloak:j},Ks=function(t){this.options=t,this.warn=t.warn||No,this.transforms=Po(t.modules,"transformCode"),this.dataGenFns=Po(t.modules,"genData"),this.directives=T(T({},Vs),t.directives);var e=t.isReservedTag||E;this.maybeComponent=function(t){return!!t.component||!e(t.tag)},this.onceId=0,this.staticRenderFns=[],this.pre=!1};function Js(t,e){var n=new Ks(e),r=t?"script"===t.tag?"null":qs(t,n):'_c("div")';return{render:"with(this){return ".concat(r,"}"),staticRenderFns:n.staticRenderFns}}function qs(t,e){if(t.parent&&(t.pre=t.pre||t.parent.pre),t.staticRoot&&!t.staticProcessed)return Ws(t,e);if(t.once&&!t.onceProcessed)return Zs(t,e);if(t.for&&!t.forProcessed)return Ys(t,e);if(t.if&&!t.ifProcessed)return Gs(t,e);if("template"!==t.tag||t.slotTarget||e.pre){if("slot"===t.tag)return function(t,e){var n=t.slotName||'"default"',r=nc(t,e),o="_t(".concat(n).concat(r?",function(){return ".concat(r,"}"):""),i=t.attrs||t.dynamicAttrs?ic((t.attrs||[]).concat(t.dynamicAttrs||[]).map((function(t){return{name:w(t.name),value:t.value,dynamic:t.dynamic}}))):null,a=t.attrsMap["v-bind"];!i&&!a||r||(o+=",null");i&&(o+=",".concat(i));a&&(o+="".concat(i?"":",null",",").concat(a));return o+")"}(t,e);var n=void 0;if(t.component)n=function(t,e,n){var r=e.inlineTemplate?null:nc(e,n,!0);return"_c(".concat(t,",").concat(Qs(e,n)).concat(r?",".concat(r):"",")")}(t.component,t,e);else{var r=void 0,o=e.maybeComponent(t);(!t.plain||t.pre&&o)&&(r=Qs(t,e));var i=void 0,a=e.options.bindings;o&&a&&!1!==a.__isScriptSetup&&(i=function(t,e){var n=w(e),r=x(n),o=function(o){return t[e]===o?e:t[n]===o?n:t[r]===o?r:void 0},i=o("setup-const")||o("setup-reactive-const");if(i)return i;var a=o("setup-let")||o("setup-ref")||o("setup-maybe-ref");if(a)return a}(a,t.tag)),i||(i="'".concat(t.tag,"'"));var s=t.inlineTemplate?null:nc(t,e,!0);n="_c(".concat(i).concat(r?",".concat(r):"").concat(s?",".concat(s):"",")")}for(var c=0;c<e.transforms.length;c++)n=e.transforms[c](t,n);return n}return nc(t,e)||"void 0"}function Ws(t,e){t.staticProcessed=!0;var n=e.pre;return t.pre&&(e.pre=t.pre),e.staticRenderFns.push("with(this){return ".concat(qs(t,e),"}")),e.pre=n,"_m(".concat(e.staticRenderFns.length-1).concat(t.staticInFor?",true":"",")")}function Zs(t,e){if(t.onceProcessed=!0,t.if&&!t.ifProcessed)return Gs(t,e);if(t.staticInFor){for(var n="",r=t.parent;r;){if(r.for){n=r.key;break}r=r.parent}return n?"_o(".concat(qs(t,e),",").concat(e.onceId++,",").concat(n,")"):qs(t,e)}return Ws(t,e)}function Gs(t,e,n,r){return t.ifProcessed=!0,Xs(t.ifConditions.slice(),e,n,r)}function Xs(t,e,n,r){if(!t.length)return r||"_e()";var o=t.shift();return o.exp?"(".concat(o.exp,")?").concat(i(o.block),":").concat(Xs(t,e,n,r)):"".concat(i(o.block));function i(t){return n?n(t,e):t.once?Zs(t,e):qs(t,e)}}function Ys(t,e,n,r){var o=t.for,i=t.alias,a=t.iterator1?",".concat(t.iterator1):"",s=t.iterator2?",".concat(t.iterator2):"";return t.forProcessed=!0,"".concat(r||"_l","((").concat(o,"),")+"function(".concat(i).concat(a).concat(s,"){")+"return ".concat((n||qs)(t,e))+"})"}function Qs(t,e){var n="{",r=function(t,e){var n=t.directives;if(!n)return;var r,o,i,a,s="directives:[",c=!1;for(r=0,o=n.length;r<o;r++){i=n[r],a=!0;var u=e.directives[i.name];u&&(a=!!u(t,i,e.warn)),a&&(c=!0,s+='{name:"'.concat(i.name,'",rawName:"').concat(i.rawName,'"').concat(i.value?",value:(".concat(i.value,"),expression:").concat(JSON.stringify(i.value)):"").concat(i.arg?",arg:".concat(i.isDynamicArg?i.arg:'"'.concat(i.arg,'"')):"").concat(i.modifiers?",modifiers:".concat(JSON.stringify(i.modifiers)):"","},"))}if(c)return s.slice(0,-1)+"]"}(t,e);r&&(n+=r+","),t.key&&(n+="key:".concat(t.key,",")),t.ref&&(n+="ref:".concat(t.ref,",")),t.refInFor&&(n+="refInFor:true,"),t.pre&&(n+="pre:true,"),t.component&&(n+='tag:"'.concat(t.tag,'",'));for(var o=0;o<e.dataGenFns.length;o++)n+=e.dataGenFns[o](t);if(t.attrs&&(n+="attrs:".concat(ic(t.attrs),",")),t.props&&(n+="domProps:".concat(ic(t.props),",")),t.events&&(n+="".concat(Bs(t.events,!1),",")),t.nativeEvents&&(n+="".concat(Bs(t.nativeEvents,!0),",")),t.slotTarget&&!t.slotScope&&(n+="slot:".concat(t.slotTarget,",")),t.scopedSlots&&(n+="".concat(function(t,e,n){var r=t.for||Object.keys(e).some((function(t){var n=e[t];return n.slotTargetDynamic||n.if||n.for||tc(n)})),o=!!t.if;if(!r)for(var i=t.parent;i;){if(i.slotScope&&i.slotScope!==ps||i.for){r=!0;break}i.if&&(o=!0),i=i.parent}var a=Object.keys(e).map((function(t){return ec(e[t],n)})).join(",");return"scopedSlots:_u([".concat(a,"]").concat(r?",null,true":"").concat(!r&&o?",null,false,".concat(function(t){var e=5381,n=t.length;for(;n;)e=33*e^t.charCodeAt(--n);return e>>>0}(a)):"",")")}(t,t.scopedSlots,e),",")),t.model&&(n+="model:{value:".concat(t.model.value,",callback:").concat(t.model.callback,",expression:").concat(t.model.expression,"},")),t.inlineTemplate){var i=function(t,e){var n=t.children[0];if(n&&1===n.type){var r=Js(n,e.options);return"inlineTemplate:{render:function(){".concat(r.render,"},staticRenderFns:[").concat(r.staticRenderFns.map((function(t){return"function(){".concat(t,"}")})).join(","),"]}")}}(t,e);i&&(n+="".concat(i,","))}return n=n.replace(/,$/,"")+"}",t.dynamicAttrs&&(n="_b(".concat(n,',"').concat(t.tag,'",').concat(ic(t.dynamicAttrs),")")),t.wrapData&&(n=t.wrapData(n)),t.wrapListeners&&(n=t.wrapListeners(n)),n}function tc(t){return 1===t.type&&("slot"===t.tag||t.children.some(tc))}function ec(t,e){var n=t.attrsMap["slot-scope"];if(t.if&&!t.ifProcessed&&!n)return Gs(t,e,ec,"null");if(t.for&&!t.forProcessed)return Ys(t,e,ec);var r=t.slotScope===ps?"":String(t.slotScope),o="function(".concat(r,"){")+"return ".concat("template"===t.tag?t.if&&n?"(".concat(t.if,")?").concat(nc(t,e)||"undefined",":undefined"):nc(t,e)||"undefined":qs(t,e),"}"),i=r?"":",proxy:true";return"{key:".concat(t.slotTarget||'"default"',",fn:").concat(o).concat(i,"}")}function nc(t,e,n,r,o){var i=t.children;if(i.length){var a=i[0];if(1===i.length&&a.for&&"template"!==a.tag&&"slot"!==a.tag){var s=n?e.maybeComponent(a)?",1":",0":"";return"".concat((r||qs)(a,e)).concat(s)}var c=n?function(t,e){for(var n=0,r=0;r<t.length;r++){var o=t[r];if(1===o.type){if(rc(o)||o.ifConditions&&o.ifConditions.some((function(t){return rc(t.block)}))){n=2;break}(e(o)||o.ifConditions&&o.ifConditions.some((function(t){return e(t.block)})))&&(n=1)}}return n}(i,e.maybeComponent):0,u=o||oc;return"[".concat(i.map((function(t){return u(t,e)})).join(","),"]").concat(c?",".concat(c):"")}}function rc(t){return void 0!==t.for||"template"===t.tag||"slot"===t.tag}function oc(t,e){return 1===t.type?qs(t,e):3===t.type&&t.isComment?function(t){return"_e(".concat(JSON.stringify(t.text),")")}(t):function(t){return"_v(".concat(2===t.type?t.expression:ac(JSON.stringify(t.text)),")")}(t)}function ic(t){for(var e="",n="",r=0;r<t.length;r++){var o=t[r],i=ac(o.value);o.dynamic?n+="".concat(o.name,",").concat(i,","):e+='"'.concat(o.name,'":').concat(i,",")}return e="{".concat(e.slice(0,-1),"}"),n?"_d(".concat(e,",[").concat(n.slice(0,-1),"])"):e}function ac(t){return t.replace(/\u2028/g,"\\u2028").replace(/\u2029/g,"\\u2029")}function sc(t,e){try{return new Function(t)}catch(n){return e.push({err:n,code:t}),j}}function cc(t){var e=Object.create(null);return function(n,r,o){(r=T({},r)).warn,delete r.warn;var i=r.delimiters?String(r.delimiters)+n:n;if(e[i])return e[i];var a=t(n,r),s={},c=[];return s.render=sc(a.render,c),s.staticRenderFns=a.staticRenderFns.map((function(t){return sc(t,c)})),e[i]=s}}new RegExp("\\b"+"do,if,for,let,new,try,var,case,else,with,await,break,catch,class,const,super,throw,while,yield,delete,export,import,return,switch,default,extends,finally,continue,debugger,function,arguments".split(",").join("\\b|\\b")+"\\b"),new RegExp("\\b"+"delete,typeof,void".split(",").join("\\s*\\([^\\)]*\\)|\\b")+"\\s*\\([^\\)]*\\)");var uc,lc,fc=(uc=function(t,e){var n=hs(t.trim(),e);!1!==e.optimize&&Es(n,e);var r=Js(n,e);return{ast:n,render:r.render,staticRenderFns:r.staticRenderFns}},function(t){function e(e,n){var r=Object.create(t),o=[],i=[];if(n)for(var a in n.modules&&(r.modules=(t.modules||[]).concat(n.modules)),n.directives&&(r.directives=T(Object.create(t.directives||null),n.directives)),n)"modules"!==a&&"directives"!==a&&(r[a]=n[a]);r.warn=function(t,e,n){(n?i:o).push(t)};var s=uc(e.trim(),r);return s.errors=o,s.tips=i,s}return{compile:e,compileToFunctions:cc(e)}}),dc=fc(As).compileToFunctions;function pc(t){return(lc=lc||document.createElement("div")).innerHTML=t?'<a href="\n"/>':'<div a="\n"/>',lc.innerHTML.indexOf("&#10;")>0}var vc=!!J&&pc(!1),hc=!!J&&pc(!0),mc=b((function(t){var e=to(t);return e&&e.innerHTML})),gc=Cr.prototype.$mount;return Cr.prototype.$mount=function(t,e){if((t=t&&to(t))===document.body||t===document.documentElement)return this;var n=this.$options;if(!n.render){var r=n.template;if(r)if("string"==typeof r)"#"===r.charAt(0)&&(r=mc(r));else{if(!r.nodeType)return this;r=r.innerHTML}else t&&(r=function(t){if(t.outerHTML)return t.outerHTML;var e=document.createElement("div");return e.appendChild(t.cloneNode(!0)),e.innerHTML}(t));if(r){var o=dc(r,{outputSourceRange:!1,shouldDecodeNewlines:vc,shouldDecodeNewlinesForHref:hc,delimiters:n.delimiters,comments:n.comments},this),i=o.render,a=o.staticRenderFns;n.render=i,n.staticRenderFns=a}}return gc.call(this,t,e)},Cr.compile=dc,T(Cr,Fn),Cr.effect=function(t,e){var n=new Vn(ct,t,j,{sync:!0});e&&(n.update=function(){e((function(){return n.run()}))})},Cr}));
  </script>
  <title>__TITLE__</title>
  <style>
    .row {
      display: flex;
      flex-wrap: wrap;
    }

    .column {
      flex: 1;
      padding: 10px;
    }

    .table-header {
      font-weight: bold;
      border-bottom: 1px solid black;
    }

    /*  */
    .table-row {
      border-bottom: 1px solid lightgray;
    }

    .table-cell {
      padding: 5px;
    }
  </style>
</head>
<!--  -->

<body style="padding: 0 200px;background-color: #f5f5f5;">
  <div id="app">
    <h1 style="padding-left: 20px;font-size: 40px;"></h1>

    <ul>
      <li v-if="num == 1 || num == 0" v-for="(i,index) in contentList" :key="index"><a :href="`#${i.primary_col.header}`">{{ i.primary_col.header ? i.primary_col.header : '' }}</a></li>
      <li v-if="num == 2 " v-for="(i,index) in contentList" :key="index"><a :href="`#${i.secondary_rol.header}`">{{i.secondary_rol.header ?i.secondary_rol.header : '' }}</a></li </ul>
      <!--  -->
      <button style="cursor: pointer;cursor: pointer;height: 30px;" @click="showStatus">{{ text }}</button>
      <!-- border-radius box-shadow-->
      <div class="row table-row" v-for="(i,index) in contentList" :key="index" style="border-radius: 10px;background-color: rgb(255, 255, 255);margin: 40px 0px;padding: 20px 40px;position: relative;box-shadow: 0px 0px 15px -8px;">
        <div class="column table-cell" v-if="num == 1 || num == 0">
          <div class="markdown-body">
            <h1 :id="i.primary_col.header">{{ i.primary_col.header }}</h1>
            <div v-html="i.primary_col.msg"></div>
          </div>
        </div>
        <div class="column table-cell" v-if="num == 2 || num == 0">
          <div class="markdown-body">
            <h1 :id="i.secondary_rol.header">{{ i.secondary_rol.header }}</h1>
            <div v-html="i.secondary_rol.msg"></div>
          </div>
        </div>
      </div>
  </div>
</body>

<script>
  new Vue({
    el: '#app',
    data() {
      return {
        // 
        contentList: [
          
            {
                primary_col: {
                    header: String.raw``,
                    msg: String.raw`<div class="markdown-body"></div>`,
                },
                secondary_rol: {
                    header: String.raw``,
                    msg: String.raw`<div class="markdown-body"></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw``,
                    msg: String.raw`<div class="markdown-body"><p></p></div>`,
                },
                secondary_rol: {
                    header: String.raw``,
                    msg: String.raw`<div class="markdown-body"></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`Abstract`,
                    msg: String.raw`<div class="markdown-body"><p>Abstract</p></div>`,
                },
                secondary_rol: {
                    header: String.raw``,
                    msg: String.raw`<div class="markdown-body"><p>Jarrod Ragsdale; Rajendra V Boppana; L Spitzner; I Mokube; M Adams; M F Razali; M N Razali; F Z Mans</p>
<p>LLMsGPTCowrie77%LLMsLLMs</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw``,
                    msg: String.raw`<div class="markdown-body"><p></p></div>`,
                },
                secondary_rol: {
                    header: String.raw``,
                    msg: String.raw`<div class="markdown-body"></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# I. INTRODUCTION`,
                    msg: String.raw`<div class="markdown-body"><p>Engaging threat actors using cyber deception assets is one of the most effective ways of understanding the threat in its entirety, as it enables defenders to monitor live attacks and to collect data for post-mortem analyses. Honeypots, used for cyber deception, are an excellent security resource whose primary utility is to be probed, attacked, and otherwise
The associate editor coordinating the review of this manuscript and approving it for publication was Anandakumar Haldorai . compromised so that the attacker's modus operandi can be dissected [1].
Such honeypots can be deployed with an emphasis on attack discovery in a research context or an emphasis on protection and mitigation in a production context [2]. Both versions rely on deception and their ability to realistically respond to an attacker's interaction. The aim is to maximize attacker interaction while minimizing detection for research honeypots and to distract attackers effectively for production honeypots. For the purpose of this paper, the design of research honeypots is investigated.
The sophistication of a deployed honeypot can be described by the level of interaction it affords to the attacker. For example, full device emulation provides a higher level of interaction, allowing for greater deception while opening the possibility of aiding and propagating attacks. Conversely, service simulation offers a lower level of interaction but is cheaper to deploy [3]. Most honeypots are designed to offer a fixed level of interaction within this range [4].</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# I. INTRODUCTION`,
                    msg: String.raw`<div class="markdown-body"><p>I. </p>
<p> [1]</p>
<p> [2]</p>
<p> [3] [4]</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# A. DETECTION AND BREAKOUT RISKS`,
                    msg: String.raw`<div class="markdown-body"><p>The main threats to honeypots lie in two areas: an attacker's ability to detect they are in a deceiving environment and an attacker's ability to break out of the defined scope of actions and use the honeypot for their intended purpose. These phenomena will be referred to as the detection of deception and risk respectively. It is the goal of the honeypot to maintain deception while minimizing detection and risk.
These threats are proven founded when observing the current state of widely deployed honeypots across the internet. Of lower interaction honeypots, 7000 were fingerprinted due to their interactions in a study by Vetterl et al. of which over a quarter hadn't been updated in over two years [5]. This highlights a lack of consistent maintenance for static implementations. Other studies also find a potential risk of breakout in higher interaction honeypots due to their complicated or volatile implementation in giving an attacker free rein on a system [4]. These issues present a potential avenue for using an alternative method to maintain modern interaction without presenting extra risk.
Generative models can fill this gap where natural language is a core part of the deception. GPTs (Generative Pre-trained Transformers) are transformer models that are pre-trained by an organization to fulfill a wide range of generative use cases. GPT models such as ChatGPT have seen an explosion in popularity of late as a summarizer, conversationalist, and recommendation system, giving responses (also called answers) based on the context and questions provided by the user [6], [7]. These transformer models operate by predicting the next token in a sequence based on the observed history [8]. A token in this case is a character sequence of around 0.75 words [9].
Large Language Models (LLMs) are an implementation of GPTs that focus on natural language use cases. These models have billions of parameters and are trained using a large dataset, limiting their deployment [10]. Alternatively, GPTs trained for a more specific use case with fewer parameters can be used instead of a general LLM. In this paper, the model is treated as a black box, and the two terms are used interchangeably.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# A. DETECTION AND BREAKOUT RISKS`,
                    msg: String.raw`<div class="markdown-body"><p></p>
<p>Vetterl7000[5][4]</p>
<p>GPTChatGPTGPT[6][7][8]0.75[9]</p>
<p>LLMsGPT[10]GPT</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# B. PROBLEM STATEMENT`,
                    msg: String.raw`<div class="markdown-body"><p>This paper investigates the design of research honeypots that are designed to be easy to update and maintain via publicly available generative models without sacrificing interactivity or risking unintended breakouts by attackers. As proof of concept, the use of GPT is explored where a large language model (GPT3.5-Turbo-0301 [11]) is instructed to behave as a honeypot that accepts terminal commands as its input. Since these models operate on prediction based on probabilities to formulate their answers and have limits on the size (measured in tokens or words) of questions and answers, merely passing an attacker's commands as questions to the model simulating a honeypot and its answers to the attacker presents limitations that could reveal the deception.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# B. PROBLEM STATEMENT`,
                    msg: String.raw`<div class="markdown-body"><p>GPTGPT3.5-Turbo-0301 [11]token</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# C. CONTRIBUTIONS`,
                    msg: String.raw`<div class="markdown-body"><p>This paper addresses the challenges of simulating and maintaining honeypots by presenting an architecture to manage the inputs and outputs of a generative model simulating a honeypot to reduce non-deterministic outputs and token usage while making the deception more realistic. An example honeypot following this methodology is evaluated against a traditional low-risk honeypot, Cowrie, where greater similarity to an actual machine is achieved for single commands. Effectiveness in different multi-step scenarios is also measured to evaluate deception and token usage through five simulated attacks based on MITRE ATT&amp;CK techniques, tactics, and procedures (TTPs) [12]: system reconnaissance, data obfuscation and ransomware, scanning and lateral propagation, persistence, and data reconnaissance and exfiltration. This paper makes the following contributions.
1) The paper presents a methodology to minimize randomness and token use while preserving the illusion of an attackable system when processing commands passed to an example LLM.
2) The presented methodology is used as the basis for an architecture presented through a block diagram and a series of algorithms for manipulating the input/output (I/O) of a generative pre-trained model. This example architecture is used as a proof of concept and can be modified to simulate any system using any sufficiently trained transformer or language model. 3) The interactivity and effectiveness of the proposed honeypot implementation is evaluated against a commonly used medium-interaction honeypot, Cowrie. For single commands, the proposed honeypot achieved greater similarity to an actual machine than Cowrie. Furthermore, in several multi-step attack scenarios, the proposed architecture maintained sessions for longer than Cowrie and reduced the token usage by up to 77% when compared to a baseline scenario that did not manage the inputs to and outputs from the LLM. 4) The paper discusses the limitations of using generative models for cyber deception and potential directions for future research.
The remainder of the paper is organized as follows: Section II provides background and context for intelligent and contextual interaction in honeypots and generative model usage. Section III detail how to alter the model's input to facilitate a base honeypot deployment. Section IV compares the proposed honeypot to a traditional honeypot to determine if the  proposed implementation deceives attackers better. Section V details the limitations of using generative models such as GPT as a honeypot backend and their mitigations. Section VI concludes the paper with a pointer to further work.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# C. CONTRIBUTIONS`,
                    msg: String.raw`<div class="markdown-body"><p>CowrieMITRE ATT&amp;CKTTPs
1) LLM
2) /I/O
3) CowrieCowrieCowrieLLM77%
4) 
IIIIIIVVGPTVI</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# II. BACKGROUND AND RELATED WORK`,
                    msg: String.raw`<div class="markdown-body"><p>A preliminary investigation into the background and related work is necessitated when exploring the realm of LLMs and their potential application for cyber deception. To this end, Honeypots, LLMs, and their combination are explored. For honeypots, their classifications, distinguishing features, and behaviors are explored. For LLMs, their underlying functionality and their potential use for a cyber deception scenario are examined. Both domains along with pertinent research within them are examined in the following subsections.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# II. BACKGROUND AND RELATED WORK`,
                    msg: String.raw`<div class="markdown-body"><p>LLMsLLMsLLMs</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# A. BACKGROUND`,
                    msg: String.raw`<div class="markdown-body"><p>Honeypots and large language models, the two key technologies relevant to the proposed honeypot design, are outlined.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# A. BACKGROUND`,
                    msg: String.raw`<div class="markdown-body"><p></p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 1) HONEYPOTS AND INTERACTIVITY`,
                    msg: String.raw`<div class="markdown-body"><p>Honeypots at their core aim to engage attackers without their knowledge [1]. Honeypots have been used for threat engagement since 1997 through the use of Fred Cohen's Deception Toolkit (DTK) to emulate seemingly real devices and systems [13]. From that initial idea, honeypots of varying levels of interaction have proliferated and been incorporated into research and production environments.
Research honeypots serve to gather information and collect artifacts from attackers. The value they add is derived from their ability to discover new trends in attack vectors and patterns [2]. Production honeypots protect a network by serving as a sandbox to discover potential entry vectors for patching in the form of active defense [2], [14]. Production honeypots can also operate alongside their real counterpart, serving as a ''jail'' by reactively switching the connection to the honeypot. Zarca et al. [15] demonstrated this idea by using software-defined networking (SDN) to redirect flows to virtual versions of IoT devices by a security orchestrator after the initial compromise.
Most honeypots exist on a spectrum between low and high interaction determined by the scope of available actions in the environment that is afforded to the attacker [2]. A more barebones environment is provided by Low-interaction honeypots (LIHs), meant to be easy to deploy with low setup costs. Services are emulated by an LIH in a limited capacity in a way that they cannot be fully exploited. One such way is through hard-coded outputs, which prevent complete access from being gained by attackers, as there is no operating system for them to interact with.
A much more realistic emulation of a target is provided by high-interaction honeypots (HIHs), granting attackers more freedom in their actions. Virtual machines or quarantined physical devices can be used as HIHs. Dynamically existing on the spectrum of interaction in various ways can also be achieved by honeypots, with one way being the adjustment of their level of interactivity depending on the provided context.
Greater deception is available in a more robust environment via static HIHs. However, with this higher level of interaction and interoperability, a greater risk of out-of-scope compromises and an increased cost to set up, maintain, and scale are also introduced. Conversely, LIHs are operated at a reduced cost and risk of exploitation, allowing for greater scalability while sacrificing detectability and capability [3]. Another option is to take a different or ''intelligent'' approach to generating output. This is further expounded in the related work section. The pros and cons of each level are shown in Table 1. The goal of the proposed LLM-based honeypot is to reach the perceived level of capability of HIHs while maintaining the safety and cost of LIHs in addition to avoiding being fingerprinted.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 1) HONEYPOTS AND INTERACTIVITY`,
                    msg: String.raw`<div class="markdown-body"><p>Honeypots at their core aim to engage attackers without their knowledge [1]. Honeypots have been used for threat engagement since 1997 through the use of Fred Cohen's Deception Toolkit (DTK) to emulate seemingly real devices and systems [13]. From that initial idea, honeypots of varying levels of interaction have proliferated and been incorporated into research and production environments.</p>
<p>Research honeypots serve to gather information and collect artifacts from attackers. The value they add is derived from their ability to discover new trends in attack vectors and patterns [2]. Production honeypots protect a network by serving as a sandbox to discover potential entry vectors for patching in the form of active defense [2], [14]. Production honeypots can also operate alongside their real counterpart, serving as a ''jail'' by reactively switching the connection to the honeypot. Zarca et al. [15] demonstrated this idea by using software-defined networking (SDN) to redirect flows to virtual versions of IoT devices by a security orchestrator after the initial compromise.</p>
<p>Most honeypots exist on a spectrum between low and high interaction determined by the scope of available actions in the environment that is afforded to the attacker [2]. A more barebones environment is provided by Low-interaction honeypots (LIHs), meant to be easy to deploy with low setup costs. Services are emulated by an LIH in a limited capacity in a way that they cannot be fully exploited. One such way is through hard-coded outputs, which prevent complete access from being gained by attackers, as there is no operating system for them to interact with.</p>
<p>A much more realistic emulation of a target is provided by high-interaction honeypots (HIHs), granting attackers more freedom in their actions. Virtual machines or quarantined physical devices can be used as HIHs. Dynamically existing on the spectrum of interaction in various ways can also be achieved by honeypots, with one way being the adjustment of their level of interactivity depending on the provided context.</p>
<p>Greater deception is available in a more robust environment via static HIHs. However, with this higher level of interaction and interoperability, a greater risk of out-of-scope compromises and an increased cost to set up, maintain, and scale are also introduced. Conversely, LIHs are operated at a reduced cost and risk of exploitation, allowing for greater scalability while sacrificing detectability and capability [3]. Another option is to take a different or ''intelligent'' approach to generating output. This is further expounded in the related work section. The pros and cons of each level are shown in Table 1. The goal of the proposed LLM-based honeypot is to reach the perceived level of capability of HIHs while maintaining the safety and cost of LIHs in addition to avoiding being fingerprinted.</p>
<p></p>
<p>[1]1997Fred CohenDTK[13]</p>
<p>[2][2][14]Zarca[15]SDN</p>
<p>[2]LIHsLIHs</p>
<p>HIHsHIHs</p>
<p>[3]LIHs1LLMLIHsHIHs</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 2) GENERATIVE AND LARGE LANGUAGE MODELS (LLMs)`,
                    msg: String.raw`<div class="markdown-body"><p>LLMs are large-scale transformer models that use the self-attention mechanism [27] to generate text. Self-attention allows the model to dynamically weigh the importance and relevance of different parts of an input sequence to the whole, enabling it to adapt and capture long-range dependencies effectively. The attention mechanism is crucial in allowing generative models to predict what word or token is likely to follow in a sequence [28]. This allows a model to understand the importance of those tokens and their sequence, allowing for greater understanding than previous NLP models [27].
A key strength of these models is the ability to handle a wide range of tasks, input types, and sizes, which is made possible by allowing them to learn from vast and diverse data sources and generalize to new examples, further enhancing their adaptability. Additionally, their adaptability can be further enhanced by fine-tuning or extending them to accommodate specific requirements, making them versatile in their implementation.
ChatGPT and its underlying models, gpt-3.5-turbo and gpt-4 [6], [29] will be more closely examined due to OpenAI's GPT model's explosion in popularity. gpt-3.5-turbo is made up of 175 billion features trained on Internet data [30]. gpt-4 is an improvement in knowledge and implementation of gpt-3.5-turbo, where is was able to score in the 90th percentile on the Uniform BAR Exam while gpt-3.5-turbo scored in the 10th [29].
ChatGPT's models can have varying levels of randomness set by its temperature parameter. A higher temperature permits the model to be allowed to take risks and select a token sequence that may not have been the most probable. Compounding randomness can be introduced in the output as future probabilities will be altered by earlier differences in choices. In the context of cyber deception, it is preferred that the token generation be made as deterministic as possible to avoid detection, so a lower temperature is applied.
OpenAI's chat completion models also provide the ability to provide a persona-defining system prompt to further fine-tune the use case easily [31]. This is explored further in Section III.
The models follow a QA (question-answer) completion paradigm that can be made to behave in different ways by using the current question or prompt and past QA pairs to craft its response [7]. Based on this input, the model generates the most likely sequence of tokens. gpt-3.5-turbo,gpt-4, and gpt-4-32k each have memory limits brought from attention and the positional encoding of each token with each token being approximately 0.75 words [9], [32]. This limits the number of tokens that the model can operate within both its input and output completion. These token limits are 4k, 8k, and 32k respectively.
OpenAI's models charge a small amount per 1k tokens sent to their model via their API [33]. The rate varies depending on the model used. For gpt-3.5-turbo and gpt-3.5-turbo-0301, it is <font color="#00FF00">$</font><font color="#FF00FF">0.002 per 1k tokens for both prompt and completion. The vastly improved gpt-4 has two versions: a base and an extended context model referred to as gpt-4 and gpt-4-32k respectively. With the improved knowledge base and size of GPT4 comes an increased cost with gpt-4 costing </font><font color="#00FF00">$</font>0.03 per 1k prompt tokens and <font color="#00FF00">$</font><font color="#FF00FF">0.06 per 1k completion tokens. gpt-4-32k is double this at </font><font color="#00FF00">$</font>0.06 per 1k prompt tokens and $0.12 per 1k completion tokens. Compared to GPT3.5, GPT4 models cost 15x-30x for prompts and 30x-60x as much for completion.
These operating constraints place certain limitations on how much context can be given to the model for it to perform its completion as efficiently and prudently as possible. However, even with limited context windows, the prompt can be crafted to provide desirable output personalities such as writing code based on human description or interacting with the user as a command terminal [7], [34].</p><hr /><p>LLMs are large-scale transformer models that use the self-attention mechanism [27] to generate text. Self-attention allows the model to dynamically weigh the importance and relevance of different parts of an input sequence to the whole, enabling it to adapt and capture long-range dependencies effectively. The attention mechanism is crucial in allowing generative models to predict what word or token is likely to follow in a sequence [28]. This allows a model to understand the importance of those tokens and their sequence, allowing for greater understanding than previous NLP models [27].
A key strength of these models is the ability to handle a wide range of tasks, input types, and sizes, which is made possible by allowing them to learn from vast and diverse data sources and generalize to new examples, further enhancing their adaptability. Additionally, their adaptability can be further enhanced by fine-tuning or extending them to accommodate specific requirements, making them versatile in their implementation.
ChatGPT and its underlying models, gpt-3.5-turbo and gpt-4 [6], [29] will be more closely examined due to OpenAI's GPT model's explosion in popularity. gpt-3.5-turbo is made up of 175 billion features trained on Internet data [30]. gpt-4 is an improvement in knowledge and implementation of gpt-3.5-turbo, where is was able to score in the 90th percentile on the Uniform BAR Exam while gpt-3.5-turbo scored in the 10th [29].
ChatGPT's models can have varying levels of randomness set by its temperature parameter. A higher temperature permits the model to be allowed to take risks and select a token sequence that may not have been the most probable. Compounding randomness can be introduced in the output as future probabilities will be altered by earlier differences in choices. In the context of cyber deception, it is preferred that the token generation be made as deterministic as possible to avoid detection, so a lower temperature is applied.
OpenAI's chat completion models also provide the ability to provide a persona-defining system prompt to further fine-tune the use case easily [31]. This is explored further in Section III.
The models follow a QA (question-answer) completion paradigm that can be made to behave in different ways by using the current question or prompt and past QA pairs to craft its response [7]. Based on this input, the model generates the most likely sequence of tokens. gpt-3.5-turbo,gpt-4, and gpt-4-32k each have memory limits brought from attention and the positional encoding of each token with each token being approximately 0.75 words [9], [32]. This limits the number of tokens that the model can operate within both its input and output completion. These token limits are 4k, 8k, and 32k respectively.
OpenAI's models charge a small amount per 1k tokens sent to their model via their API [33]. The rate varies depending on the model used. For gpt-3.5-turbo and gpt-3.5-turbo-0301, it is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>0.002</mn><mi>p</mi><mi>e</mi><mi>r</mi><mn>1</mn><mi>k</mi><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>b</mi><mi>o</mi><mi>t</mi><mi>h</mi><mi>p</mi><mi>r</mi><mi>o</mi><mi>m</mi><mi>p</mi><mi>t</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>c</mi><mi>o</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>&#x0002E;</mo><mi>T</mi><mi>h</mi><mi>e</mi><mi>v</mi><mi>a</mi><mi>s</mi><mi>t</mi><mi>l</mi><mi>y</mi><mi>i</mi><mi>m</mi><mi>p</mi><mi>r</mi><mi>o</mi><mi>v</mi><mi>e</mi><mi>d</mi><mi>g</mi><mi>p</mi><mi>t</mi><mo>&#x02212;</mo><mn>4</mn><mi>h</mi><mi>a</mi><mi>s</mi><mi>t</mi><mi>w</mi><mi>o</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>s</mi><mi>:</mi><mi>a</mi><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>a</mi><mi>n</mi><mi>e</mi><mi>x</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>d</mi><mi>e</mi><mi>d</mi><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi><mi>r</mi><mi>e</mi><mi>f</mi><mi>e</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>t</mi><mi>o</mi><mi>a</mi><mi>s</mi><mi>g</mi><mi>p</mi><mi>t</mi><mo>&#x02212;</mo><mn>4</mn><mi>a</mi><mi>n</mi><mi>d</mi><mi>g</mi><mi>p</mi><mi>t</mi><mo>&#x02212;</mo><mn>4</mn><mo>&#x02212;</mo><mn>32</mn><mi>k</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>p</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>l</mi><mi>y</mi><mo>&#x0002E;</mo><mi>W</mi><mi>i</mi><mi>t</mi><mi>h</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>i</mi><mi>m</mi><mi>p</mi><mi>r</mi><mi>o</mi><mi>v</mi><mi>e</mi><mi>d</mi><mi>k</mi><mi>n</mi><mi>o</mi><mi>w</mi><mi>l</mi><mi>e</mi><mi>d</mi><mi>g</mi><mi>e</mi><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi><mi>o</mi><mi>f</mi><mi>G</mi><mi>P</mi><mi>T</mi><mn>4</mn><mi>c</mi><mi>o</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>a</mi><mi>n</mi><mi>i</mi><mi>n</mi><mi>c</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi>d</mi><mi>c</mi><mi>o</mi><mi>s</mi><mi>t</mi><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mi>g</mi><mi>p</mi><mi>t</mi><mo>&#x02212;</mo><mn>4</mn><mi>c</mi><mi>o</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow></math>0.03 per 1k prompt tokens and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>0.06</mn><mi>p</mi><mi>e</mi><mi>r</mi><mn>1</mn><mi>k</mi><mi>c</mi><mi>o</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><mi>n</mi><mi>s</mi><mo>&#x0002E;</mo><mi>g</mi><mi>p</mi><mi>t</mi><mo>&#x02212;</mo><mn>4</mn><mo>&#x02212;</mo><mn>32</mn><mi>k</mi><mi>i</mi><mi>s</mi><mi>d</mi><mi>o</mi><mi>u</mi><mi>b</mi><mi>l</mi><mi>e</mi><mi>t</mi><mi>h</mi><mi>i</mi><mi>s</mi><mi>a</mi><mi>t</mi></mrow></math>0.06 per 1k prompt tokens and $0.12 per 1k completion tokens. Compared to GPT3.5, GPT4 models cost 15x-30x for prompts and 30x-60x as much for completion.
These operating constraints place certain limitations on how much context can be given to the model for it to perform its completion as efficiently and prudently as possible. However, even with limited context windows, the prompt can be crafted to provide desirable output personalities such as writing code based on human description or interacting with the user as a command terminal [7], [34].</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 2) GENERATIVE AND LARGE LANGUAGE MODELS (LLMs)`,
                    msg: String.raw`<div class="markdown-body"><p>LLMs[27]Transformer[28][27]</p>
<p></p>
<p>OpenAIGPTChatGPTgpt-3.5-turbogpt-4 [6][29]gpt-3.5-turbo1750[30]gpt-4gpt-3.5-turbo90%gpt-3.5-turbo10%[29]</p>
<p>ChatGPT</p>
<p>OpenAI[31]</p>
<p>QAQA[7]gpt-3.5-turbogpt-4gpt-4-32k0.75[9][32]4k8k32k</p>
<p>OpenAIAPI1k[33]gpt-3.5-turbogpt-3.5-turbo-03011k0.002gpt-4gpt-4gpt-4-32kgpt-41k0.031k0.06gpt-4-32k1k0.060.12GPT3.5GPT415303060</p>
<p>[7][34]</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# B. RELATED WORK`,
                    msg: String.raw`<div class="markdown-body"><p>This section provides a summary of relevant prior work in the areas of honeypots, adaptive interaction, and LLMs used for cyber deception. Table 2 summarizes the mentioned honeypots.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# B. RELATED WORK`,
                    msg: String.raw`<div class="markdown-body"><p>LLM2</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 1) STATIC HONEYPOTS`,
                    msg: String.raw`<div class="markdown-body"><p>Extensive work has been done in the design and implementation of LIHs and HIHs over the years. Notable work includes the development of various LIH frameworks such as Honeyd [16], which provide lightweight and scalable solutions for emulating vulnerable services through static response mechanisms. However, any traffic outside of the defined behavior may cause a failure in deception that could be used to fingerprint the honeypot [35].
Affording slightly more freedom to the attacker are medium interaction honeypots such as Cowrie and Thingpot [17], [18]. An entire system is simulated by these honeypots, providing more freedom than a single service. However, these honeypots are still detectable if they receive an input that they are unable to handle.
The most advanced and interactive of honeypots are HIHs, of which much research has been done. These honeypots are either entire virtualizations of systems or are physical devices. For instance, Siphon [19] is a network of HIHs where each device is physically present and connected to attackers via SSH forwarding. However, this setup is costly to maintain, requiring a dedicated space for the devices as well as a separate monitoring agent to watch the health of each device. Honware [20] is a virtualized high-interaction honeypot in which unique firmware and filesystems of embedded devices are served via a custom kernel.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 1) STATIC HONEYPOTS`,
                    msg: String.raw`<div class="markdown-body"><p>LIHsHIHsLIHHoneyd [16] [35]</p>
<p>CowrieThingpot [17], [18]</p>
<p>HIHsSiphon [19]HIHsSSHHonware [20]</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 2) INTELLIGENT AND DYNAMIC HONEYPOTS`,
                    msg: String.raw`<div class="markdown-body"><p>Knowing the pitfalls of static interaction honeypots, some work has been done on an alternate path to define deceivable interaction as a dynamic and adaptive process rather than a static one. Luo et al. [22] coined the idea of ''intelligent interaction'' in IoTCandyJar, where the honeypot works toward achieving a ''correct'' conversation with attackers and becoming more interactive over time as the expected responses are discovered starting from zero knowledge. Using intelligent interaction, Yamamoto et al. present Firmpot as a framework that uses firmware images and machine learning to learn the behaviors of those devices for intelligent interaction.
Mfogo et al. presented AIIPOT, a novel transformer-based honeypot using chatbots to capture vulnerabilities for Internet of Things (IoT) devices [24]. They follow intelligent interaction principles to learn and interact with each attacker. Their approach using reinforcement learning and transformer models is novel and requires comparison to using LLMs, of which is related work.
Similarly, ''dynamic'' honeypots exist that adapt themselves based on environmental stimuli [36]. This dynamic honeypot design pattern has been used by Pauna et al. via Q-learning to deploy a self-adaptive SSH honeypot that modifies its state based on the observed environment and attacker input [21]. Intelligence and adaptiveness in honeypots aid in the discovery of new threats while avoiding fingerprinting campaigns [5], [37]. However, these honeypots can have varying costs and dependencies depending on the technology used such as IoTCandyJar requiring a preliminary internet-wide scan [22].</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 2) INTELLIGENT AND DYNAMIC HONEYPOTS`,
                    msg: String.raw`<div class="markdown-body"><p>Luo[22]IoTCandyJar""YamamotoFirmpot</p>
<p>MfogoAIIPOT[24]LLM</p>
<p>[36]PaunaQ-learningSSH[21][5][37]IoTCandyJar[22]</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 3) LANGUAGE MODELS FOR CYBER DECEPTION`,
                    msg: String.raw`<div class="markdown-body"><p>For a honeypot to be effective, it must convince the attacker it's both a vulnerable target and a real system. Both charades must be convincing so that a motivated attacker is unable to make the distinction from that of a real system. This idea is reminiscent of the Turing Test for artificial intelligence where an artificial intelligence (AI) can pass if its behavior is indistinguishable from that of a human [38]. Our ''honeypot test'' is if an the honeypot using model-generated output is indistinguishable from that of a real system and is able to avoid honeypot detection through a breakdown in communication.
Mckee and Noever use ChatGPT to model different honeypot tasks an attacker might execute [25]. They mention a token limit of 8,000 for ChatGPT but believe the token limit is a non-issue. However, with the cost of use and long outputs found in testing, this can still remain an issue. When the token limit is reached, old context is thrown out, which can lead to older but still relevant context-changing commands being lost, leading to detection or breakdown in the attack. Further limitations are detailed in Section V.
Sladic et al. investigate the deceptive potential of using generative models as a honeypot [26]. In their work, they survey users and security experts to see if they are able to differentiate output from a real system and that from an LLM. While their work supports the use of LLMs for cyber deception, the focus of this paper is on deception to a dedicated attacker and how that deception might fail.
Cambiaso and Caviglione take a different approach to language model-assisted cyber deception [39]. In their work, they use ChatGPT's ability to craft realistic human interactions to engage email scammers in an effort to waste their time, providing a proactive defense.
Using ChatGPT's context-aware QA functionality and its ability to create the illusion of an attackable interface, adaptive and intelligent interaction can be employed to design an interactive honeypot. This type of honeypot can simulate a number of internal services through its ability to understand and respond to natural language queries. Using generative models to behave as a honeypot is as safe as LIHs since no commands are actually executed. Additionally, as more context is gathered, the honeypot can adjust its behavior and responses to better mimic a real system, thereby increasing the chances of capturing and analyzing new tactics and techniques. This allows for high-interaction targets to be created at a much lower upfront cost. Depending on the use case, these honeypots can be deployed in a research or production environment, as long as they have the proper context.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 3) LANGUAGE MODELS FOR CYBER DECEPTION`,
                    msg: String.raw`<div class="markdown-body"><p> [38]</p>
<p>Mckee  Noever  ChatGPT  [25] ChatGPT  8,000</p>
<p>Sladic  [26] LLM  LLM </p>
<p>Cambiaso  Caviglione  [39] ChatGPT </p>
<p> ChatGPT </p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# III. METHODOLOGY`,
                    msg: String.raw`<div class="markdown-body"><p>Large Language Models (LLMs) can serve a variety of use cases that rely on processing language input. However, for cyber deception through asset simulation, how and what one passes to the model must be examined, and more specifically, how that input and certain directives can be used in preprocessing to direct the output deterministically. Due to the availability and ease of implementation, gpt-3.5-turbo is the model chosen to generate output for the methodology and evaluation of this work.
The methodology is built following the same general pattern of steps where a question is received, the input is modified to fit the model's use case, sent to the model, output is saved for the future, and the response is returned to the user. A flowchart of the general steps taken for the methodology is given in Fig. 1.
Once this methodology is established, pre and postprocessing can be modeled for a naive approach. Then, a more comprehensive approach can be taken to mitigate any limitations in the generating model's design. The last subsection fully outlines the proposed architecture to be evaluated in Section IV.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# III. METHODOLOGY`,
                    msg: String.raw`<div class="markdown-body"><p>LLMsgpt-3.5-turbo</p>
<p>1</p>
<p></p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# A. PROMPT REFINEMENT`,
                    msg: String.raw`<div class="markdown-body"><p>GPT (Generative Pre-trained Transformer), is a publicly available large language model that crafts its persona based on the user-provided prompt that directs the pre-trained model to generate an output answer for the given question [7], [40]. This means the implementation and expected output can  Additionally, the quality of the response increases with the level of detail and completeness of the prompt, as it provides additional context to the model. Moreover, model parameters such as the temperature can be modified to increase the randomness of the response, leading to greater creativity. However, in this use case, the temperature should be as low as possible. For these experiments and implementation, OpenAI's GPT chat completion models are used. These are used due to their sophistication, popularity, and extendability in context administration [11]. Among these, gpt-3.5-turbo is chosen over gpt-4 due to its reduced usage cost and public availability.
To begin with, in order to make the responses appear as normal conversations, the questions must be crafted carefully. This is illustrated in Tables 345where the question is refined through multiple iterations to make the response more natural. As can be seen, the answer moves from a generic answer indicating that it's an AI language model to a more conclusive and succinct response by providing the model with more context and predetermined biases. To create the illusion of an attackable cyber asset, prompts must be enhanced to ensure that the output to hackers does not include additional explanations that ChatGPT tends to add.
Using the prompt to guide the response can be extended to provide context for future questions. In tables 3-5, the model operates with only a single question as context. However, gpt-3.5-turbo can be used to provide a context history for that session in the form of previous questions and answers [32]. The prompt is split into two sections by GPT's API implementation of questions and answers: system and user-assistant. The system prompt provides the overarching context that the model operates in for all future conversations, such as ''You are a baseball expert'' or ''You are a security professional.'' This guides the knowledge base, even when no further context is given.
The user-assistant section provides example questions and answers as a ''QA pair'' to provide more context, guiding the conversation within the guidelines of the system prompt. For example, in Table 6, the second question is able to pick up from the first QA pair in its context history and use it to answer the next question. In the second conversation in Table 7, that context is not provided to the model, so the model has no idea what food is being asked about. For a honeypot use case, including prior context is useful for preserving changes made by an attacker, such as directory traversal or file changes.
Interestingly, during Conversation 1 in Table 6, the model provided an excessive amount of information regarding the calories, surpassing what would typically be expected in a to-the-point human conversation by repeating the subject. However, when the prompt is modified to minimize the information output, the model responded with just the calorie amount, aligning better with conversational norms. This observation inspired us to delve into manipulating the input and output of the model to create cyber deception assets. If these assets are ''to-the-point'' in their responses, they are able to minimize detection and risk of exploitation beyond the intended scope.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# A. PROMPT REFINEMENT`,
                    msg: String.raw`<div class="markdown-body"><p>GPTGenerative Pre-trained Transformer[7][40]OpenAIGPT[11]gpt-3.5-turbogpt-4</p>
<p>3-5AIChatGPT</p>
<p>3-5gpt-3.5-turbo[32]GPTAPI</p>
<p>QA6QA7</p>
<p>6</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# B. SIMPLE LLM CONTEXT HONEYPOT PRINCIPLES`,
                    msg: String.raw`<div class="markdown-body"><p>Establishing that the responses can be guided using the prompt and that responses can use the provided context to determine the output, it can be formulated how to use LLMs like GPT as a honeypot.
Mckee and Noever [25] illustrate this idea for Windows and Linux terminals using ChatGPT to test possible honeypot scenarios. Their approach outlined terminal behavior in the system prompt and appended all past commands and their outputs to be used by the model for completion with the output. The generation of answer a by the LLM presented with question q  is given as:
a = LLM(q  )(1)
LLM input q  is comprised of the combination of the system prompt S, context history of past questions and answers C, and question q defined as:
q  = S  C  q (2)
The context history C must then be updated with each new answer and the question that generated it in order for use with the next question in the session:
C = C  {q, a}(3)
These steps are combined to create the following algorithm for building a question in which a large-scale generative model is tuned and prepped to answer.
An architecture using these basic building blocks is illustrated in Fig. 2 with accompanying pseudocode in Algorithm 1. The answer a is derived from the completion of the system prompt S, context history C, and the most recent question q by Algorithm 1 Simple LLM Honeypot Input: q: Attacker-provided question C: Session Context History S: Persona-defining System Prompt Output: a: LLM-generated answer 1: a  LLM(S, C, q)
2: C  C  {[q, a]} 3: return a FIGURE 2. Simple LLM honeypot.
the model. However, the context history can indefinitely grow at a fast rate, becoming an issue for longer sessions where the context will be truncated due to token limits or future responses will take time to be calculated.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# B. SIMPLE LLM CONTEXT HONEYPOT PRINCIPLES`,
                    msg: String.raw`<div class="markdown-body"><p>GPTLLMMckeeNoever [25] ChatGPTWindowsLinuxLLMq'a
<font color="#00FF00">$$</font><font color="#FF00FF"> a = LLM(q') \quad (1) </font><font color="#00FF00">$$</font>
LLM<font color="#00FF00">$</font><font color="#FF00FF"> q' </font><font color="#00FF00">$</font>SCq
<font color="#00FF00">$$</font><font color="#FF00FF"> q' = S \cup C \cup q \quad (2) </font><font color="#00FF00">$$</font>
C
<font color="#00FF00">$$</font><font color="#FF00FF"> C = C \cup \{q, a\} \quad (3) </font><font color="#00FF00">$$</font>
</p>
<p>21</p>
<p>aSCq1LLMqCSaLLM
1: <font color="#00FF00">$</font><font color="#FF00FF"> a \leftarrow LLM(S, C, q) </font><font color="#00FF00">$</font>
2: <font color="#00FF00">$</font><font color="#FF00FF"> C \leftarrow C \cup \{[q, a]\} </font><font color="#00FF00">$</font>
3: a</p>
<p><strong>2.</strong> LLM</p>
<p></p><hr /><p>GPTLLMMckeeNoever [25] ChatGPTWindowsLinuxLLMq'a
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>a</mi><mo>&#x0003D;</mo><mi>L</mi><mi>L</mi><mi>M</mi><mo stretchy="false">&#x00028;</mo><msup><mi>q</mi><mi>&#x02032;</mi></msup><mo stretchy="false">&#x00029;</mo><mspace width="1em" /><mo stretchy="false">&#x00028;</mo><mn>1</mn><mo stretchy="false">&#x00029;</mo></mrow></math>
LLM<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msup><mi>q</mi><mi>&#x02032;</mi></msup></mrow></math>SCq
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msup><mi>q</mi><mi>&#x02032;</mi></msup><mo>&#x0003D;</mo><mi>S</mi><mo>&#x0222A;</mo><mi>C</mi><mo>&#x0222A;</mo><mi>q</mi><mspace width="1em" /><mo stretchy="false">&#x00028;</mo><mn>2</mn><mo stretchy="false">&#x00029;</mo></mrow></math>
C
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>C</mi><mo>&#x0003D;</mo><mi>C</mi><mo>&#x0222A;</mo><mo stretchy="false">&#x0007B;</mo><mi>q</mi><mo>&#x0002C;</mo><mi>a</mi><mo stretchy="false">&#x0007D;</mo><mspace width="1em" /><mo stretchy="false">&#x00028;</mo><mn>3</mn><mo stretchy="false">&#x00029;</mo></mrow></math>
</p>
<p>21</p>
<p>aSCq1LLMqCSaLLM
1: <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>a</mi><mo>&#x02190;</mo><mi>L</mi><mi>L</mi><mi>M</mi><mo stretchy="false">&#x00028;</mo><mi>S</mi><mo>&#x0002C;</mo><mi>C</mi><mo>&#x0002C;</mo><mi>q</mi><mo stretchy="false">&#x00029;</mo></mrow></math>
2: <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>C</mi><mo>&#x02190;</mo><mi>C</mi><mo>&#x0222A;</mo><mo stretchy="false">&#x0007B;</mo><mo stretchy="false">[</mo><mi>q</mi><mo>&#x0002C;</mo><mi>a</mi><mo stretchy="false">]</mo><mo stretchy="false">&#x0007D;</mo></mrow></math>
3: a</p>
<p><strong>2.</strong> LLM</p>
<p></p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# C. ADAPTIVE LLM CONTEXT HONEYPOTS PRINCIPLES`,
                    msg: String.raw`<div class="markdown-body"><p>To save on tokens, useful context is preloaded into the system prompt to streamline the attacker's use. For example, if an attacker attempts to install a non-standard package like Nmap, a line can be added to the system prompt such as All packages are installed to ensure the attempted execution behaves deterministically. This has the added benefit of potentially saving hundreds of tokens by avoiding a lengthy install output. This handling can also be used to filter interactive packages that will break deception, such as Vim. This is all defined before any session and remains unchanged.
To address the issue of losing context owing to the token limit, past QA pairs can be selectively appended to the context history for that session based on whether they alter the answer to subsequent questions. This context history is then passed to the model. This process extends the length of the session by prolonging the time it takes for the token limit to be reached. For example, ls, a command that shows all files in a directory, will have a different output depending on the current working directory. To ensure the correct output is supplied, any working directory changes are appended to the context history passed to the model. Since ls has no direct downstream effect on other commands, it is not necessary to save that QA pair for future questions. This saves tokens.
In the case of a honeypot session, any context-changing commands are saved to the context history, thereby saving space by discarding QA pairs that don't affect future inputs. This process also has the added benefit of reducing response time as there are fewer calculations for inter-token meaning and sentence structure when predicting the next sequence.
However, some UNIX commands, such as history, require a full input history to have the correct output. If only the context history as previously described is maintained, the full input history needed for that command would not be available. Alternatively, If all inputs and their outputs are preserved similar to Algorithm 1 rather than just the contextchanging ones, the token limit would be reached. Instead, two histories can be maintained: a context history of filtered QA pairs as previously described to replace the context history C in Algorithm 1 and a global history H containing all inputs for that session to be used when needed.
Both histories need to be maintained and updated with each new question and answer. This method of having two histories gives the option of using the question-only history if the question-answer context history becomes too large. Just using the inputs may be lossy, but is preferred to losing deterministicness by running out of context memory. This is preferable since the model fails when the knowledge does not exist but it can make some inferences with incomplete (lossy) knowledge. The global history must also be maintained to ensure it doesn't exceed memory constraints either.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# C. ADAPTIVE LLM CONTEXT HONEYPOTS PRINCIPLES`,
                    msg: String.raw`<div class="markdown-body"><p>tokenNmap"All packages are installed"tokenVim</p>
<p>tokentokenlslstoken</p>
<p>honeypottoken</p>
<p>UNIXhistory1tokenC1H</p>
<p></p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# D. DESIGN`,
                    msg: String.raw`<div class="markdown-body"><p>The above methodology is formulated to create a model for deterministic interaction when employing generative models such as LLMs as a black box. The proposed model aims to establish a reliable framework for effectively utilizing language models such as GPT as a honeypot backend, ensuring deceptive interaction throughout the process.
The described updating is modeled with the session context history C 1 being updated with the most recent question q and answer a if that question is a member of the set C 0 . C 0 is a set of questions that have the capability to immediately change the context of future questions.
The session global history H 1 is updated with each new question unless H 1 extends to be greater than the max token limit. If the max token limit is exceeded, earlier questions are removed to make room as represented by the Last function. These histories are updated last after answer generation. However, the process is defined here to introduce C 1 and H 1 , which are used for calculating C  :
C 1 = (q  C 0  C 1  {[q, a]})  ((q  C 0 )  C 1 ) (4) H 1 = (H 1  q &lt; MAX  H 1  q)  ((H 1  q &lt; MAX)  Last(H 1  q, MAX)) (5)
Whether C 1 or H 1 is used as context for q  would be dependent on if a question requires a global history or if the token limit is reached when using the more robust context history. This behavior is modeled as such with C  being the chosen history to be used by q  when passed to the LLM:
C  = (len(C 1 ) &gt; MAX  q  H 0  H 1 )  ((len(C 1 ) &gt; MAX  q  H 0 )  C 1 )(6)
q  = S  C   q (7)
Once the appropriate history is chosen to be passed to the model as context, the full question can be built using system prompt S, chosen context C  , and attacker question q. The fully formulated question is then passed to the model for answer generation. If the generated answer has some breakdown in deception i.e. responding as an ''AI language model'' like in Table 3, that answer needs to be sanitized before being returned. This is handled by the Sanitize function and is modeled as such:
a  = LLM(q  ) (8) a = Sanitize(a  )(9)
Eq. ( 4)-Eq. ( 9) collectively describe the operation of the proposed honeypot. This model is implemented as described below for future evaluation.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# D. DESIGN`,
                    msg: String.raw`<div class="markdown-body"><p>LLMsGPT</p>
<p>CqaCCCHHLastCHC
[ C = (q  C  C  {[q, a]})  ((q  C )  C) ]
[ H = (H  q &lt; MAX  H  q)  ((H  q &lt; MAX)  Last(H  q, MAX)) ]</p>
<p>CHqCLLMq
[ C = (len(C) &gt; MAX  q  H  H)  ((len(C) &gt; MAX  q  H)  C) ]
[ q = S  C  q ]</p>
<p>SCq3"AI"Sanitize
[ a = LLM(q) ]
[ a = Sanitize(a) ]</p>
<p>(4)(9)</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# E. PROPOSED FRAMEWORK`,
                    msg: String.raw`<div class="markdown-body"><p>In the proposed framework, the following six actions are executed for each new attacker input to maintain context and deception for extended interaction efficiently.
1) Select the context history or the more lossy global history for the current question. (Eq. ( 6)) 2) Generate an answer for the question in the chosen context. (Eq. ( 7), Eq. ( 8)) 3) Sanitize answers to maintain the deception. (Eq. ( 9)) 4) Maintain global session history of questions for cases where all questions are needed. (Eq. ( 5)) 5) Maintain context-changing questions and answers in the session context history for future interactions. (Eq. ( 4)) 6) Return the answer to the user. An algorithm implementing the proposed framework is designed. The algorithm examines the question and calls the context-choosing sub-algorithm to return the context required for that question. Once an answer is generated by the LLM, the algorithm calls another sub-algorithm that updates the context and input histories using the question and generated answer.
Algorithm 2 presents a pseudocode to implement the proposed framework. The first output creates the sanitized answer after selecting the context based on whether the provided question q requires a global history as defined by H 0 or if the request exceeds the defined q  Input()
9:
if q  KillCmds then break 10:</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# E. PROPOSED FRAMEWORK`,
                    msg: String.raw`<div class="markdown-body"><p>
1)  (6)
2)  (7), (8)
3)  (9)
4)  (5)
5)  (4)
6) LLM
2 q  H 0 </p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# end if Part-1`,
                    msg: String.raw`<div class="markdown-body"><p>11:
h   ChooseContext(S, q, C 1 , H 1 , H 0 ) 12:
a  LLM(S, h  , q)
13:
a  Sanitize(a)
14:
C 1 , H 1  UpdateContext(q, a, C 1 , H 1 , C 0 ) 15:</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# end if`,
                    msg: String.raw`<div class="markdown-body"><p>11:
h'  ChooseContext(S, q, C, H, H)
12:
a  LLM(S, h', q)
13:
a  Sanitize(a)
14:
C, H  UpdateContext(q, a, C, H, C)
15:</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# end if Part-2`,
                    msg: String.raw`<div class="markdown-body"><p>SendAnswer(a) 16: end while Algorithm 3 Choose Context Input: S: Persona-defining System Prompt q: Attacker-provided Question
C 1 : {[q  , a  ] | q  generates a  } (Session Context History) H 1 : {q  | q  0 , . . . , q  q-1 } (Session Global History) H 0 : {q | q requires H 1 } Output: Chosen History Set 1: if q  H 0 or len(S, C 1 , q)  MAX_TOKENS then 2:
return H 1 3: end if 4: return C 1 token limit. The global history is updated with each question. The context history is updated with the question and answer if the question is defined by the set of context-changing questions C 0 . This formulation is described explicitly:</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# end if`,
                    msg: String.raw`<div class="markdown-body"><p>SendAnswer(a) 16:  3 : S:  q: 
C 1 : {[q  , a  ] | q   a  } () H 1 : {q  | q  0 , . . . , q  q-1 } () H 0 : {q | q  H 1 } :  1:  q  H 0  len(S, C 1 , q)  MAX_TOKENS  2:
 H 1 3:  4:  C 1 token  C 0 :</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# end if Part-3`,
                    msg: String.raw`<div class="markdown-body"><p>1) If the input question q is a member of the set H 0 , the set of all questions that require a global history, or if the number of tokens used by the system prompt s, session context history C 1 , and input question q exceeds the MAX_TOKENS for LLM, then the session's global history of questions H 1 is used as context. Otherwise, the session's context history C 1 is used as the chosen context. (Algorithm 3 2) An answer a is generated by the LLM using the query formulated by s, context, q. (Algorithm 3) 3) If the answer a has portions that would potentially jeopardize deception, that answer is sanitized by the Sanitize function to remove said portions.(Algorithm 2 Line 13) 4) Append input question q to session's global history of questions H 1 if that appending does not exceed Algorithm 4 Update Context Input: q: Attacker-provided Question a: Sanitized LLM-generated answer C 1 : {[q  , a  ] | q  generates a  } (Session Context History) H 1 : {q  | q  0 , . . . , q  q-1 } (Session Global History) C 0 : {q | q affects a q+1 } Output: C 1 : {[q  , a  ] | q  generates a  } (Updated Session Context History) H 1 : {q  | q  0 , . . . , q  q-1 } (Updated Session Global History) 1: if len(H 1  q) &lt; MAX_TOKENS then 2:
H 1  H 1  {q} 3: else 4: H 1  Last(H 1  q, MAX_TOKENS) 5: end if 6: if q  C 0 then 7: C 1  C 1  {[q, a]} 8: end if 9: return C 1 , H 1</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# end if`,
                    msg: String.raw`<div class="markdown-body"><p>1)  q  H 0 s C 1  q  LLM  MAX_TOKENS H 1  C 1  3</p>
<p>2) LLM  s q  a 3</p>
<p>3)  a  Sanitize  2  13 </p>
<p>4)  q  H 1 MAX_TOKENS H 1  4 : q a LLM  C 1{[q, a] | q  a} H 1{q | q 0...q q-1} C 0{q | q  q+1} : C 1{[q, a] | q  a} H 1{q | q 0...q q-1} 1: if len(H 1  q) &lt; MAX_TOKENS then 2: H 1  H 1  {q} 3: else 4: H 1  Last(H 1  q, MAX_TOKENS) 5: end if 6: if q  C 0 then 7: C 1  C 1  {[q, a]} 8: end if 9: return C 1H 1</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# end if Part-4`,
                    msg: String.raw`<div class="markdown-body"><p>the MAX_TOKENS for LLM. Else, remove the earliest questions until under MAX_TOKENS via the Max function. (Algorithm 4 Lines 1-5) 5) If the set of context-changing questions C 0 contains input question q, then the QA pair {[q, a]} is appended to that session's set of context-changing QA pairs C 1 .
(Algorithm 4 Lines 6-8) 6) Answer a is returned to the user while C 1 and H 1 are maintained until the session is terminated. (Algorithm 2 Lines 14-15)
These actions can be formalized as independent actions and checks as formulated in Algorithms 2-4. The controlling portion of the algorithm in Algorithm 2 continuously accepts input from the attacker until a command that would end the sequence is received as defined by KilllCmds. With each input, the required context is chosen to generate each answer. The contexts are then updated as previously formulated based on the provided question and its answer. ChooseContext selects which history to use with the question based on set membership as defined by H 0 . UpdateContexts updates the global and context histories based on actions 4 and 5 to be used for future questions.
The actions taken in the above algorithms are illustrated in Fig. 3. Context handling is implemented in a front-end interface (FEI) made up of an input and output handler. This FEI handles question generation and input curation on behalf of the generating model.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# end if`,
                    msg: String.raw`<div class="markdown-body"><p></p>
<p>LLMMAX_TOKENSMaxMAX_TOKENS 41-5C0qQA{[q, a]}QAC1 46-8aC1H1 214-15</p>
<p>242KilllCmds45ChooseContextH0UpdateContexts</p>
<p>3FEIFEI</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 1) INPUT HANDLER`,
                    msg: String.raw`<div class="markdown-body"><p>The input handler accepts input from the attacker on behalf of the model and decides what context to use based on that input and the size of the context history. Once the proper context is chosen, the full question is built and sent to the model. This corresponds with actions 1 and 2 in Algorithm 2. </p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 1) INPUT HANDLER`,
                    msg: String.raw`<div class="markdown-body"><p>212</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 2) OUTPUT HANDLER`,
                    msg: String.raw`<div class="markdown-body"><p>The output handler sanitizes the model's output by removing notes or comments that slip through the prompt. The output handler then updates that session's context and global history for future questions before returning the answer to the attacker. This corresponds with actions 3-6 in Algorithm 2.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 2) OUTPUT HANDLER`,
                    msg: String.raw`<div class="markdown-body"><p>23-6</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 3) FEI EXAMPLE`,
                    msg: String.raw`<div class="markdown-body"><p>A sample session is provided in Table 8. When updating the context history for this example, Directory and file changes such as those in QA 4 and QA 8 are included in the context history. The global history use is shown in QA 10 where only past questions are passed to the model. By only passing context-changing QA pairs to the model and all questions for certain edge cases, 1054 unique tokens are eliminated over the course of short conversation while still maintaining believable output.
The FEI conservatively manages the context and questions to the model on the fly, allowing us to maintain consistency with what the attacker expects while using as few tokens as possible. minimizing token use is paramount due to a memory limit present in GPT [32]. Additionally, for a more robust use of the proposed FEI, the operating environment such as the operating system, hostname, or other data provided to the attacker can be changed by updating the prompt. While this new architecture is a significant improvement over the architecture in Figure 2, it has limitations, such as third-party interaction and cost. These limitations are further summarized in Section V.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 3) FEI EXAMPLE`,
                    msg: String.raw`<div class="markdown-body"><p>8QA 4QA 8QA 101054</p>
<p>FEIGPT[32]</p>
<p>FEI2</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# IV. RESULTS AND DISCUSSION`,
                    msg: String.raw`<div class="markdown-body"><p>In this section, the performance of an adaptive LLM context honeypot as opposed to a traditional static interaction honeypot is evaluated. For this comparison, Cowrie is chosen to represent static honeypots of the same risk level due to its widespread use on the internet and its backend terminal emulation [41].
The subsections explore the setup for the following evaluation, single command similarity comparisons, and extended session preservation.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# IV. RESULTS AND DISCUSSION`,
                    msg: String.raw`<div class="markdown-body"><p>LLMCowrie [41]</p>
<p></p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# A. EVALUATION SETUP`,
                    msg: String.raw`<div class="markdown-body"><p>Cowrie is a medium to high-interaction honeypot based on Kippo that captures SSH and Telnet sessions [42]. For the medium interaction version, the backend is implemented via hard-coded responses for 34 commands as of 2018 [5]. The authors state that these commands are chosen due to most attackers only using those commands and that implementing all commands would take too much effort [42]. For the highinteraction version, Cowrie acts as a proxy to another system where it reroutes traffic from the login handler to a secondary high-interaction cyber asset. For the purpose of this evaluation, the LLM honeypot supported by the FEI described in Fig. 3 will be compared to the medium interaction Cowrie configuration. This comparison is chosen due to how both the medium-interaction Cowrie and the LLM honeypot return output without any execution, limiting the risk of misuse.
For the LLM honeypot, OpenAI's gpt-3.5-turbo is used due to its availability, consistency, and sophistication.  The temperature is set to 0 for all inputs. For the extended session, the architecture outlined in Fig. 3 </p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# A. EVALUATION SETUP`,
                    msg: String.raw`<div class="markdown-body"><p>CowrieKippoSSHTelnet[42]201834[5][42]Cowrie</p>
<p>3FEILLMCowrieCowrieLLM</p>
<p>LLMOpenAIgpt-3.5-turbo03</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# is used.`,
                    msg: String.raw`<div class="markdown-body"><p>A Debian 7 Wheezy virtual machine acts as a control since Cowrie in its base configuration mimics that operating system. Although the LLM can mimic any version or terminal via the system prompt, the prompt is set to guide the model to behave as similarly to Cowrie and the control as possible. The feasibility of using an LLM to generate honeypot output is first evaluated by assessing the similarity of outputs for single inputs before comparing deception, interactivity, and token usage in extended sessions in the next subsection.
The algorithms used to calculate these similarities are given where the first calculates a similarity ratio based on the distance and length calculated by the second:</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# is used.`,
                    msg: String.raw`<div class="markdown-body"><p>Debian 7 WheezyCowrieLLMCowrieLLM
</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# B. SINGLE COMMAND COMPARISON Part-1`,
                    msg: String.raw`<div class="markdown-body"><p>For this comparison, the system, filesystem, and perceived external connectivity of each honeypot are evaluated. To test the feasibility of using an LLM to mimic an attackable interface, the outputs for a single command are emulated by Cowrie and the LLM. These outputs are compared to those of a control virtual machine running Debian 7 Wheezy since Cowrie in its base configuration mimics that operating system. However, the LLM can mimic any version or terminal via the prompt. For fairness, the model's prompt is crafted to make the operating system as similar to Cowrie's as possible.
To see which of the two outputs is more similar, the average ratio (denoted as L-ratio) of similarity of the output's Levenshtein distance of ten outputs for each command from both honeypots and the control virtual machine is calculated [43]. The Levenshtein distance quantifies the number of edits required to make two strings identical to one another by deleting, inserting, or replacing characters. The L-ratio measures how similar the two texts are with 1.0 being identical. The  L-ratio and distance are calculated as shown in Algorithms 5 and 6 where the L-ratio of two strings is the size of the larger of the two strings subtracted by the number of edits necessary to make the two strings identical divided by that maximum length. These algorithms are not novel are provided for completeness.
d[i, j]  min{d[i -1, j] + 1, d[i, j -1] + 1, d[i - 1, j -1] + cost}
The L-ratio for both honeypots to the control virtual machine is calculated. If the L-ratio is higher for one of the two honeypots, it means their output is more similar to that of the control virtual machine. If the command is not implemented or if the LLM reports it is a fake system, it is automatically given a score of 0.0 for the input. The average L-ratio of all inputs is calculated as well as the average inputs accepted by both honeypots as seen in Table 9 and Table 10. These L-ratios are used as coordinates in Fig. 4 where the x value is the L-ratio for the LLM output and the Y value is the L-ratio for the Cowrie output. If the LLM's output is more similar to the control, it will fall below the diagonal (x=y) line and above if Cowrie is more similar. Each command is separated into one of three categories: system, filesystem, and connectivity categories. These are represented by orange dots, purple dots, and green dots respectively. The specific distances and L-ratios for each command can be found in the provided data repository [44].
The system category is made up of commands that are used to gather system information and make system-modifying changes. These include system file reading, package installs, and kernel modifications. The filesystem category is made up of commands that handle filesystem traversal and modifications. These commands include listing, creation, deletion, and access control for files, directories, and links. For connectivity, since both backends are simulating the attacker's commands, no external communication is taking place with the backend with the exception of curl and wget for Cowrie.
Both backends then must simulate this behavior using believable addresses, response time, firewalls, and routing. To test each, A variety of net utilities are used targeting both domain names and IP addresses.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# B. SINGLE COMMAND COMPARISON`,
                    msg: String.raw`<div class="markdown-body"><p>LLMCowrieLLMDebian 7 WheezyCowrieLLMCowrie</p>
<p>LevenshteinL-ratioLevenshteinL-ratio1.0L-ratio56L-ratio</p>
<p>L-ratioL-ratioLLM0.0L-ratio910L-ratio4xLLML-ratioyCowrieL-ratioLLMx=yCowrieL-ratioCowriecurlwgetIPnet</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# B. SINGLE COMMAND COMPARISON Part-2`,
                    msg: String.raw`<div class="markdown-body"><p>As can be seen in Fig. 4, Table 9, and Table 10, the LLM honeypot had a higher L-ratio 70.5% of the time over Cowrie when compared to the control's output for all inputs, indicating a higher level of similarity to a real machine for single commands. The average L-ratio for all inputs was 0.444 for Cowrie and 0.617 for the LLM. It's worth noting that the LLM honeypot can emulate outputs for any valid command input while medium-interaction Cowrie can only emulate 34 commands in its default configuration. This allows the LLM honeypot to provide a greater level of interaction than Cowrie. For fairness, the L-ratios for inputs implemented by both honeypots are compared separately. The LLM had a higher L-ratio 64.9% of the time with an average L-ratio of 0.601 and Cowrie having an average of 0.552.
System command outputs from the LLM had a higher Lratio 80.6% of the time with Cowrie having an average L-ratio of 0.329 and the LLM having an average L-ratio of 0.6465. This was expected since Cowrie has limited system command support. For commands implemented by both, Cowrie had an average L-ratio for system commands of 0.511 and the LLM of 0.568.
Filesystem commands had a more incremental improvement with 59% of LLM commands having a higher L-ratio than Cowrie with the average L-ratios being 0.551 and for Cowrie and LLM respectively. This incremental improvement is due to most of Cowrie emulates being filesystem commands. For commands implemented by both, Cowrie had an average L-ratio for filesystem commands of 0.566 and the LLM of 0.606.
Connectivity commands were more surprising with 79.2% having a higher LLM L-ratio with the average L-ratios being 0.419 and 0.594 for Cowrie and LLM respectively. Actually downloading with Curl and Wget gave Cowrie a higher ratio since it actually executes the request. However, other commands such as ip were not implemented but should have been for a base system. IPTables was unable to handle and display changes. Netstat displayed the same output with different options. For commands implemented by both, Cowrie had an average L-ratio for connectivity commands of 0.572 and the LLM of 0.633.
Several factors contribute to the outliers observed. Firstly, Cowrie actually executes Curl and Wget commands, while the LLM only emulates them, resulting in L-ratios being skewed towards Cowrie in that specific circumstance. This aspect makes Cowrie more advantageous for malware capturing, as it can provide real-time logging of captured artifacts. However, it is worth noting that the capturing capabilities of the LLM honeypot can be extended in future work to include downloading by the FEI when these specific commands come up, reducing the disparity between the two honeypots.
Secondly, the LLM exhibited undesirable behavior with certain commands due to the use of Debian 7 in the prompt.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# B. SINGLE COMMAND COMPARISON`,
                    msg: String.raw`<div class="markdown-body"><p>4910LLM70.5%L-ratioCowrieL-ratioCowrie0.444LLM0.617LLMCowrie34LLMCowrieL-ratioLLM64.9%L-ratioL-ratio0.601Cowrie0.552</p>
<p>LLM80.6%L-ratioCowrieL-ratio0.329LLML-ratio0.6465CowrieCowrieL-ratio0.511LLM0.568</p>
<p>59%LLMCowrieL-ratioCowrieLLML-ratio0.5510.606CowrieCowrieL-ratio0.566LLM0.606</p>
<p>79.2%LLM L-ratioCowrieLLML-ratio0.4190.594CurlWgetCowrieipIPTablesNetstatCowrieL-ratio0.572LLM0.633</p>
<p>CowrieCurlWgetLLML-ratioCowrieCowrieLLMFEI</p>
<p>Debian 7LLM</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# B. SINGLE COMMAND COMPARISON Part-3`,
                    msg: String.raw`<div class="markdown-body"><p>For instance, ifconfig, being deprecated, was not emulated, and the enable command was unrecognized. These issues can be alleviated through prompt refinement. Thirdly, the LLM occasionally generated excessive output or went offtopic, particularly when dealing with commands that had long outputs like dmesg and ps, resulting in the token limit being reached. This ''hallucinatory rambling'' is further discussed in Section V.
To conclude the single-command evaluation, the Levenshtein distance of outputs was used to calculate a similarity ratio of honeypot output to real system output as an initial indicator of deception. Despite the limitations and outliers mentioned, the evaluation results demonstrate the proposed honeypot's advantages over Cowrie in terms of emulating realistic machine behavior for single commands across all categories. The potential for greater attacker interaction and favorable L-ratios observed in the majority of cases further support this conclusion. Improvements such as prompt refinement, sanity checks, caching outputs, and exploring an extended architecture can be used to mitigate limitations and outliers for a honeypot where a language model to generate its output is used. While the L-ratio is not a perfect metric and different outputs may not necessarily indicate incorrect output, it serves as a valuable starting point for evaluating single command outputs and finding similarities in output formats.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# B. SINGLE COMMAND COMPARISON`,
                    msg: String.raw`<div class="markdown-body"><p> ifconfig  enable LLM  dmesg  ps </p>
<p> Cowrie  L- L-</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# C. EXTENDED SESSION INTEGRITY COMPARISON`,
                    msg: String.raw`<div class="markdown-body"><p>To determine if language models can be used for extended attacker deception, the number of interactions it takes for a honeypot session to break down is compared. This comparison is conducted between static low-risk honeypots and two LLM-based honeypots. Cowrie is chosen to represent traditional low-risk honeypots due to its widespread use and availability as well as its use in the previous evaluation [41]. The first LLM honeypot is implemented using gpt-3.5-turbo with the previously used system prompt that saves all questions and answers as context as a base honeypot (Fig. 2). This LLM honeypot is compared to one augmented with an FEI to represent the proposed implementation as detailed in Fig. 3. The number of tokens used in both LLM setups throughout each session is measured to measure how many tokens the FEI saved and if that affects overall deception.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# C. EXTENDED SESSION INTEGRITY COMPARISON`,
                    msg: String.raw`<div class="markdown-body"><p>LLMCowrie[41]LLMgpt-3.5-turbo2LLMFEI3LLMFEI</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 1) SESSION PROGRESS`,
                    msg: String.raw`<div class="markdown-body"><p>To evaluate interaction and deception preservation for attack sessions, five attacker scenarios for the three honeypot setups are followed to see how long it takes for each honeypot's deception to break down. Each scenario contains tactics and techniques listed in the MITRE ATT&amp;CK matrix framework [12], which are outlined in our data repository [44].
The scenarios used for evaluation are system reconnaissance (SR), data obfuscation (DO), lateral propagation (LP), persistence (PE), and exfiltration (EX). These scenarios are chosen due to their relevance to real-world cybersecurity  It should be noted that the command sequences were not chosen with the intention of causing a breakdown for any of the honeypots. If a command fails and there is a suitable alternative available, that alternative is used for that step in the scenario instead of the intended input. This is done so long as the data needed for the following commands are given with the substitute i.e. ip addr and ifconfig. If no suitable substitution can be done, the scenario ends early. Therefore, this test measures the most favorable outcome for each scenario for each setup.
Each attack scenario has nine steps. In each step, one or more Linux Bash shell commands, scripts, or a combination of them are run. The intended commands used to accomplish each tactic in the scenario are further detailed in our data repository [44].
The SR scenario includes steps attackers might take to scout a recently compromised system, such as identifying the operating system, processor, services, disk usage, paths, open ports, log files, the passwd file, and active users. The DO scenario encompasses both the inputs that an attacker might employ to disrupt the use of files on a system and the methods they employ to conceal their actions. The LP scenario includes network reconnaissance, shellcode execution, and basic user authentication on a secondary system as well as commands to verify their actions. The PE scenario is made up of commands to ensure repeat access to the compromised system via backdoors and new user accounts. The DR scenario includes commands to search for and exfiltrate interesting files using different methods.
The number of steps each honeypot is able to execute in the sequence is measured, stopping when all possible inputs are rejected, an output is so unbelievable a rational attacker would not continue, or the scenario concludes. Each bar graph in Fig. 5 quantifies the number of steps achieved in each scenario with orange being Cowrie, green being the selectivecontext FEI-assisted LLM honeypot outlined at the end of Section III, and purple being an LLM honeypot that saves all context. For the honeypots using LLMs, the same system prompt is used as in the single command evaluation.
As can be seen in Fig. 5, the LLM and FEI-augmented LLM outlasted Cowrie in every scenario, completing three of the five while maintaining context. Cowrie was able to fully complete the SR scenario but fell short for the other four.
For data obfuscation and ransomware (DO), Cowrie failed when attempting to create and encrypt the archive whereas both LLMs made it to the end. However, later in that scenario, upon executing the history command, both LLMs failed to give proper output after clearing with history -c.
For scanning and lateral propagation (LP), Cowrie was unable to execute a for loop to ping all addresses on the subnet in the provided environment, cutting the interaction short. Both LLM honeypots were able to simulate this interaction. Later in the LP scenario, both the LLM and FEI-augmented LLM failed to execute an internet-fetched shell script and identified themselves as an AI agent.
For the persistence (PE) scenario, both LLM honeypots were able to complete the session while Cowrie failed to set up a Netcat backdoor that would start a shell upon connection.
Lastly, for DR, Cowrie failed when attempting to compress a file for exfiltration whereas the LLM was able to do so but failed when trying to view the password shadow file, citing invalid permissions. While the LLM honeypots had some mistakes in later outputs, they were able to emulate an attackable environment for longer than or for the same length as Cowrie.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 1) SESSION PROGRESS`,
                    msg: String.raw`<div class="markdown-body"><p>MITRE ATT&amp;CK[12][44]</p>
<p>SRDOLPPEEXIPifconfig</p>
<p>Linux Bash shell[44]</p>
<p>SRpasswdDOLPshellcodePEDR</p>
<p>5CowrieFEILLMLLMLLM</p>
<p>5LLMFEILLMCowrieCowrieSR</p>
<p>DOCowrieLLMhistoryLLMhistory -c</p>
<p>LPCowriepingforLLMLPLLMFEILLMshellAI</p>
<p>PELLMCowrieNetcatshell</p>
<p>DRCowrieLLMLLMCowrie</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 2) SESSION TOKEN USE`,
                    msg: String.raw`<div class="markdown-body"><p>To measure the effectiveness and retention of only passing context-changing interactions to the LLM as chosen by an FEI, the number of tokens used for each step in each scenario is measured. Both implementations started by using 70 tokens for the system prompt. If an answer is determined to change the operating context for the session, that command input and output are used as a QA pair and included when passed to the model for future questions. If the session breaks deception before the scenario is concluded, the measurement of tokens will end on that step. This will be one step further than seen in Fig. 5 as tokens will be used to generate the deceptionbreaking answer. The FEI-augmented LLM honeypot was able to maintain interaction with the user for the same amount of steps as the LLM honeypot that saved all context while using up to 77.26% fewer tokens by the end of a session as seen by the system reconnaissance scenario in graph (a) in Fig. 6. On average across the five scenarios, the FEI-augmented LLM honeypot used 62.17% fewer tokens.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 2) SESSION TOKEN USE`,
                    msg: String.raw`<div class="markdown-body"><p>FEILLM7056(a)FEILLMLLM77.26%FEILLM62.17%</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# D. DISCUSSION`,
                    msg: String.raw`<div class="markdown-body"><p>Results from the evaluations show that a honeypot using an LLM as its backend is able to better emulate outputs than traditional honeypots of a similar level of risk (attacker using the honeypot outside the accepted scope). the LLM honeypot was also able to maintain attack sessions that used standard tools for the operating system it emulated at a high rate. The FEI proposed in Section III was able to reduce the number of tokens used over the course of a session while still maintaining deception. However, several limitations that may limit the use of LLMs for cyber deception were encountered.
In this work, a study on the effectiveness of LLMs for cyber deception is provided. Past work in using LLMs for attacker deception has proposed the idea of using LLMs to emulate terminal behavior [7], [25] or to generate artifacts to prolong attacker engagement [39]. A simple and elegant architecture is designed and evaluated. This architecture can be easily implemented while lowering the detection probability even compared to commonly used and well-designed medium-interaction honeypots such as Cowrie. The proposed architecture reduced token use by anywhere from 11-77% for attack scenarios that were simulated to completion when compared to a base architecture [25].</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# D. DISCUSSION`,
                    msg: String.raw`<div class="markdown-body"><p>LLMLLMFEILLM</p>
<p>LLMLLMLLM[7][25][39]Cowrie[25]11%77%</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# V. LIMITATIONS`,
                    msg: String.raw`<div class="markdown-body"><p>Generative models used for cyber deception can change the operating environment and threat surface provided to the attacker by providing relevant context in the prompt. This capability makes them well-suited for threat engagement which can evolve as new threats emerge. However, such a use case requires addressing several limitations inherent to the design, technology, and implementation derived from the specific model used.
These limitations can be categorized by whether they're inherent to the design of the underlying transformer architecture or due to the model used. Both are explored more in-depth in the following subsections. With each limitation, a discussion of possible solutions to mitigate the severity when deploying honeypots using LLMs are provided.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# V. LIMITATIONS`,
                    msg: String.raw`<div class="markdown-body"><p>
LLM</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# A. DESIGN LIMITATIONS`,
                    msg: String.raw`<div class="markdown-body"><p>These limitations are inherent to the use of models that generate output using the self-attention mechanism and will be present no matter which model is used. Each limitation presented is summarized and accompanied with mitigations to limit or eliminate their impact.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# A. DESIGN LIMITATIONS`,
                    msg: String.raw`<div class="markdown-body"><p></p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 1) DETERMINISTIC OUTPUT`,
                    msg: String.raw`<div class="markdown-body"><p>Using LLMs for something deterministic where for an input, the output should be the same can lead to some issues. This is due to how LLMs calculate the next token in a sequence using few-shot learning [30], [45]. If token probabilities are statistically close, variations in output may occur. This difference can cause further changes down the line as future inputs will use that discrepancy when calculating subsequent sequences in that output [46].
To mitigate this shortcoming, executing the same command multiple times and taking the most common output will find the most common occurrence in the parts of the output that have discrepancies. However, this method comes with increased cost and reduced performance and responsiveness. Alternatively, finding where discrepancies are common i.e. in version numbers, and masking those based on the previous context will increase the deterministicness of the honeypot. Li et al. take this approach by guiding the model to learn the deterministic relationship between masked content and the rest of the content to capture factual knowledge [47].</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 1) DETERMINISTIC OUTPUT`,
                    msg: String.raw`<div class="markdown-body"><p>LLMsLLMs [30], [45] [46]</p>
<p>Li [47]</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 2) HALLUCINATION, RAMBLING, AND MISBEHAVING`,
                    msg: String.raw`<div class="markdown-body"><p>Another limitation can be occasionally found in the text generation for longer outputs which can lead to detection. These come up in the form of ''hallucinatory rambling,'' which occurs when the model makes something up and becomes stuck in an output loop until the token limit is reached. In addition to detecting wrong output, it takes much longer for the LLM to output a rambling answer. To mitigate this, the long response time can be used to detect when rambling occurs and reset with altered context.
Additionally, in testing with GPT3.5, the model would occasionally misbehave with certain questions where it added additional notes or comments at the end of answers when told not to in the prompt. If this occurs and that answer is used as context for future questions, future answers are likely to contain those notes or comments as well. A Sanitization step like the one used in Fig. 3 to remove notes or comments is crucial for maintaining the deception.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 2) HALLUCINATION, RAMBLING, AND MISBEHAVING`,
                    msg: String.raw`<div class="markdown-body"><p></p>
<p>GPT3.53</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 3) EXTERNAL COMMUNICATION`,
                    msg: String.raw`<div class="markdown-body"><p>Another possibility of detection comes with the interaction between the honeypot and some observable infrastructure under the attacker's control. Such scenarios include starting a session with a C&amp;C server, downloading malware, or exfiltrating data. Since no traffic is generated, the attacker can determine that their commands are not being executed and that they are in a honeypot. These detection scenarios can't be addressed without help from other technologies such as communication handlers and sandboxes. To mitigate this, a communication handler can be used to spoof the communication on behalf of the generating model.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 3) EXTERNAL COMMUNICATION`,
                    msg: String.raw`<div class="markdown-body"><p>C&amp;C</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# B. IMPLEMENTATION LIMITATIONS`,
                    msg: String.raw`<div class="markdown-body"><p>These limitations were encountered due to the specific model used and may or may not be present when using different models. GPT3.5 is implemented as an example use case in Section III. However, there are some constraints to using OpenAI's chat completion models that can have implications that bring to light further limitations.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# B. IMPLEMENTATION LIMITATIONS`,
                    msg: String.raw`<div class="markdown-body"><p>GPT3.5IIIOpenAI</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 1) COST`,
                    msg: String.raw`<div class="markdown-body"><p>Compared to GPT3.5, GPT4 costs 15x-30x for prompts and 30x-60x as much for completion. For a research deployment where thousands of attacks can be observed daily, costs can quickly add up when each command can possibly return thousands of tokens. With this in mind, it's imperative to appear as a real system because if an attacker determines the honeypot is implemented using a pay-as-you-go model, they can maximize context by executing commands that have large outputs such as file reading or recursive directory listing.
When effectively utilized, the proposed honeypot exhibits a considerable reduction in deployment expenses, as evidenced by the lower token usage observed in the attack scenarios in Section IV. However, if the goal is to attract a large number of hosts, the cost per token is still a limiting factor. To overcome this, a locally trained and hosted language model could be used.
In total, around 240K tokens were used to develop the FEI using gpt-3.5-turbo. Evaluation consumed significantly more tokens due to each instruction being executed 10 times to compute the average Levenshtein distances. On the other hand, testing the five different MITRE ATT&amp;CK scenarios only consumed 50,840 tokens. Of these, the FEI-assisted model used 47.34% fewer tokens than the base LLM honeypot.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 1) COST`,
                    msg: String.raw`<div class="markdown-body"><p>GPT-3.5GPT-415303060</p>
<p></p>
<p>gpt-3.5-turboFEI240,00010LevenshteinMITRE ATT&amp;CK50,840FEILLM47.34%</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 2) MEMORY LIMIT`,
                    msg: String.raw`<div class="markdown-body"><p>OpenAI's chat completion models have a limit on how many tokens can be supplied per query. This is due to memory limitations inherent to transformer-based models [48]. This limit applies to both prompt and response so some space is required to be left to not cut off the response. Models with larger memory are generally more expensive to train, meaning it's more expensive for the user as seen with GPT-4 being more expensive than GPT-3.5 [33], [49]. Using a reduced memory model, if only relevant context is supplied per each command as detailed in Section III, this should cut down on costs and extend the session by a large margin.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 2) MEMORY LIMIT`,
                    msg: String.raw`<div class="markdown-body"><p>OpenAITransformer [48]GPT-4GPT-3.5 [33], [49]III</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 3) RESPONSINVENESS`,
                    msg: String.raw`<div class="markdown-body"><p>To ensure efficient live usage and maintaining of deception, it is crucial to minimize the time required to generate output for certain inputs. Currently, generating output for specific inputs can take seconds or even minutes, which is impractical for real-time interactions. Therefore, it is essential to significantly reduce the response time to less than a second.
To accomplish this, a caching mechanism within the Front-End Interface (FEI) could be used. By caching large outputs in the FEI, the system can swiftly retrieve and send the pre-generated responses instead of waiting for the LLM to generate them on the spot. Another solution is to use a local model with characteristics put in place in the training stage to limit long outputs.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 3) RESPONSINVENESS`,
                    msg: String.raw`<div class="markdown-body"><p></p>
<p>FEIFEILLM</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 4) TRAINING BIAS`,
                    msg: String.raw`<div class="markdown-body"><p>GPT3.5 has only been trained with data up to 2021 [50]. The impact of this cutoff is that any bias present in the training data will be inherited by the model such as outdated log contents or lack of modern interactions [10]. This bias can lead to a breakdown in deception if an attacker uses a modern package or tool with critical usage implemented after that cutoff. So long as these biases are internally consistent, this can be managed. However, it may limit what devices and technologies the honeypot can masquerade as. Because of this limit, evolution with the attacker is also hampered. To mitigate this, a more up-to-date model can be used.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 4) TRAINING BIAS`,
                    msg: String.raw`<div class="markdown-body"><p>GPT3.52021[50] [10]</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 5) ETHICAL CONSTRAINTS`,
                    msg: String.raw`<div class="markdown-body"><p>OpenAI has implemented safeguards to prevent its models from returning malicious or otherwise harmful responses. Though the use case of a honeypot is not intentionally malicious, questions from the attacker may be categorized as such. When attempting to view log files that contain sensitive information, the deception may also fail as seen in Table 11 TABLE 11. gpt-3.5-turbo Terminal breakdown -sensitive output [11] [The warning is unable to be suppressed even with an appropriately worded system prompt.].
due to moderation policies put in place. To mitigate this, sanitizing self-reporting output and returning error codes can be done to present the illusion of a faulty system rather than a honeypot.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 5) ETHICAL CONSTRAINTS`,
                    msg: String.raw`<div class="markdown-body"><p>OpenAI1111gpt-3.5-turbo [11] [] </p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# VI. CONCLUSION`,
                    msg: String.raw`<div class="markdown-body"><p>Generative models such as LLMs have exploded in popularity in non-research sectors since the release of ChatGPT, a conversational large language model. With this explosion comes an exploration by users of alternate use cases that generative models can fulfill by changing their personalities based on the provided prompt. One of the use cases is as an interactive cyber deception asset to learn the tactics, techniques, and procedures (TTPs) of attackers.
An implementation of a honeypot using input curation for generative models to generate terminal output is proposed. The proposed honeypot takes inspiration from other dynamic interaction honeypots in its design, incorporating a context-aware approach to engage with potential threats without increasing risk. This implementation filters past inputs to limit token usage while maintaining interactivity and deception.
The effectiveness of this implementation is evaluated for both singular commands in its output similarity and extended sessions in its ability to maintain deception. This evaluation compared the proposed language model honeypot to a static medium-interaction honeypot of a similar risk level, Cowrie. The proposed honeypot's output was more similar than Cowrie's 70% of the time, with an average similarity score 16% higher. The proposed honeypot compares favorably to a non-curating implementation where the proposed architecture reduced token use by up to 77% by saving only the relevant context.
Section V discusses the limitations encountered and how they may be overcome. While language models offer interesting possibilities as honeypots, they do have limitations. The limitations include responsiveness, nondeterministic output, and non-verifiable output. Further research is needed before honeypots using this technology can be deployed effectively in the wild.
Future work is to be done in the extension of the input curation mechanism. More robust methods of context selection and handling of input edge cases are left for future work. Alternatively, research in the model used for output generation could be explored to find which of a series of different models with different feature sets are the most effective for cyber deception. The development and deployment or extensive tuning of a generative model for the use of cyber deception is also considered to replace the general-purpose model used.
Threat engagement is a constantly evolving issue due to the evolutionary nature of cyberattacks. This requires a technology that can evolve with it and be able to adapt to new issues as they come up. Generative models fulfill this need as one of a still-expanding set of use cases. To this end, the efficient handling of input for these models is paramount and has demonstrated improvements to a base deployment in token use reduction without compromising deception.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# VI. CONCLUSION`,
                    msg: String.raw`<div class="markdown-body"><p>VI. </p>
<p>ChatGPTLLMsTTPs</p>
<p></p>
<p>CowrieCowrie70%16%77%</p>
<p></p>
<p></p>
<p></p></div>`,
                }
            },
        
        ],
        // 
        num: 0
      }
    },
    computed: {
      text() {
        // 
        switch (this.num) {
          case 0:
            return '' //0 ->  
            break;
          case 1:
            return '' // 1-> 
            break;
          default:
            return '' //  -> ''
            break;
        }
      }
    },
    methods: {
      // 
      showStatus() {
        if (this.num >= 2) { //>=2  
          this.num = 0
        } else {
          this.num++
        }
      }
    },
  })
</script>
