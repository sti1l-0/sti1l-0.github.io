<!DOCTYPE html>

<head>
  <meta charset="utf-8">
  <script>
  !function(t,e){"object"==typeof exports&&"undefined"!=typeof module?module.exports=e():"function"==typeof define&&define.amd?define(e):(t="undefined"!=typeof globalThis?globalThis:t||self).Vue=e()}(this,(function(){"use strict";var t=Object.freeze({}),e=Array.isArray;function n(t){return null==t}function r(t){return null!=t}function o(t){return!0===t}function i(t){return"string"==typeof t||"number"==typeof t||"symbol"==typeof t||"boolean"==typeof t}function a(t){return"function"==typeof t}function s(t){return null!==t&&"object"==typeof t}var c=Object.prototype.toString;function u(t){return"[object Object]"===c.call(t)}function l(t){var e=parseFloat(String(t));return e>=0&&Math.floor(e)===e&&isFinite(t)}function f(t){return r(t)&&"function"==typeof t.then&&"function"==typeof t.catch}function d(t){return null==t?"":Array.isArray(t)||u(t)&&t.toString===c?JSON.stringify(t,null,2):String(t)}function p(t){var e=parseFloat(t);return isNaN(e)?t:e}function v(t,e){for(var n=Object.create(null),r=t.split(","),o=0;o<r.length;o++)n[r[o]]=!0;return e?function(t){return n[t.toLowerCase()]}:function(t){return n[t]}}var h=v("slot,component",!0),m=v("key,ref,slot,slot-scope,is");function g(t,e){var n=t.length;if(n){if(e===t[n-1])return void(t.length=n-1);var r=t.indexOf(e);if(r>-1)return t.splice(r,1)}}var y=Object.prototype.hasOwnProperty;function _(t,e){return y.call(t,e)}function b(t){var e=Object.create(null);return function(n){return e[n]||(e[n]=t(n))}}var $=/-(\w)/g,w=b((function(t){return t.replace($,(function(t,e){return e?e.toUpperCase():""}))})),x=b((function(t){return t.charAt(0).toUpperCase()+t.slice(1)})),C=/\B([A-Z])/g,k=b((function(t){return t.replace(C,"-$1").toLowerCase()}));var S=Function.prototype.bind?function(t,e){return t.bind(e)}:function(t,e){function n(n){var r=arguments.length;return r?r>1?t.apply(e,arguments):t.call(e,n):t.call(e)}return n._length=t.length,n};function O(t,e){e=e||0;for(var n=t.length-e,r=new Array(n);n--;)r[n]=t[n+e];return r}function T(t,e){for(var n in e)t[n]=e[n];return t}function A(t){for(var e={},n=0;n<t.length;n++)t[n]&&T(e,t[n]);return e}function j(t,e,n){}var E=function(t,e,n){return!1},N=function(t){return t};function P(t,e){if(t===e)return!0;var n=s(t),r=s(e);if(!n||!r)return!n&&!r&&String(t)===String(e);try{var o=Array.isArray(t),i=Array.isArray(e);if(o&&i)return t.length===e.length&&t.every((function(t,n){return P(t,e[n])}));if(t instanceof Date&&e instanceof Date)return t.getTime()===e.getTime();if(o||i)return!1;var a=Object.keys(t),c=Object.keys(e);return a.length===c.length&&a.every((function(n){return P(t[n],e[n])}))}catch(t){return!1}}function D(t,e){for(var n=0;n<t.length;n++)if(P(t[n],e))return n;return-1}function M(t){var e=!1;return function(){e||(e=!0,t.apply(this,arguments))}}function I(t,e){return t===e?0===t&&1/t!=1/e:t==t||e==e}var L="data-server-rendered",R=["component","directive","filter"],F=["beforeCreate","created","beforeMount","mounted","beforeUpdate","updated","beforeDestroy","destroyed","activated","deactivated","errorCaptured","serverPrefetch","renderTracked","renderTriggered"],H={optionMergeStrategies:Object.create(null),silent:!1,productionTip:!1,devtools:!1,performance:!1,errorHandler:null,warnHandler:null,ignoredElements:[],keyCodes:Object.create(null),isReservedTag:E,isReservedAttr:E,isUnknownElement:E,getTagNamespace:j,parsePlatformTagName:N,mustUseProp:E,async:!0,_lifecycleHooks:F},B=/a-zA-Z\u00B7\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u037D\u037F-\u1FFF\u200C-\u200D\u203F-\u2040\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD/;function U(t){var e=(t+"").charCodeAt(0);return 36===e||95===e}function z(t,e,n,r){Object.defineProperty(t,e,{value:n,enumerable:!!r,writable:!0,configurable:!0})}var V=new RegExp("[^".concat(B.source,".$_\\d]"));var K="__proto__"in{},J="undefined"!=typeof window,q=J&&window.navigator.userAgent.toLowerCase(),W=q&&/msie|trident/.test(q),Z=q&&q.indexOf("msie 9.0")>0,G=q&&q.indexOf("edge/")>0;q&&q.indexOf("android");var X=q&&/iphone|ipad|ipod|ios/.test(q);q&&/chrome\/\d+/.test(q),q&&/phantomjs/.test(q);var Y,Q=q&&q.match(/firefox\/(\d+)/),tt={}.watch,et=!1;if(J)try{var nt={};Object.defineProperty(nt,"passive",{get:function(){et=!0}}),window.addEventListener("test-passive",null,nt)}catch(t){}var rt=function(){return void 0===Y&&(Y=!J&&"undefined"!=typeof global&&(global.process&&"server"===global.process.env.VUE_ENV)),Y},ot=J&&window.__VUE_DEVTOOLS_GLOBAL_HOOK__;function it(t){return"function"==typeof t&&/native code/.test(t.toString())}var at,st="undefined"!=typeof Symbol&&it(Symbol)&&"undefined"!=typeof Reflect&&it(Reflect.ownKeys);at="undefined"!=typeof Set&&it(Set)?Set:function(){function t(){this.set=Object.create(null)}return t.prototype.has=function(t){return!0===this.set[t]},t.prototype.add=function(t){this.set[t]=!0},t.prototype.clear=function(){this.set=Object.create(null)},t}();var ct=null;function ut(t){void 0===t&&(t=null),t||ct&&ct._scope.off(),ct=t,t&&t._scope.on()}var lt=function(){function t(t,e,n,r,o,i,a,s){this.tag=t,this.data=e,this.children=n,this.text=r,this.elm=o,this.ns=void 0,this.context=i,this.fnContext=void 0,this.fnOptions=void 0,this.fnScopeId=void 0,this.key=e&&e.key,this.componentOptions=a,this.componentInstance=void 0,this.parent=void 0,this.raw=!1,this.isStatic=!1,this.isRootInsert=!0,this.isComment=!1,this.isCloned=!1,this.isOnce=!1,this.asyncFactory=s,this.asyncMeta=void 0,this.isAsyncPlaceholder=!1}return Object.defineProperty(t.prototype,"child",{get:function(){return this.componentInstance},enumerable:!1,configurable:!0}),t}(),ft=function(t){void 0===t&&(t="");var e=new lt;return e.text=t,e.isComment=!0,e};function dt(t){return new lt(void 0,void 0,void 0,String(t))}function pt(t){var e=new lt(t.tag,t.data,t.children&&t.children.slice(),t.text,t.elm,t.context,t.componentOptions,t.asyncFactory);return e.ns=t.ns,e.isStatic=t.isStatic,e.key=t.key,e.isComment=t.isComment,e.fnContext=t.fnContext,e.fnOptions=t.fnOptions,e.fnScopeId=t.fnScopeId,e.asyncMeta=t.asyncMeta,e.isCloned=!0,e}var vt=0,ht=[],mt=function(){function t(){this._pending=!1,this.id=vt++,this.subs=[]}return t.prototype.addSub=function(t){this.subs.push(t)},t.prototype.removeSub=function(t){this.subs[this.subs.indexOf(t)]=null,this._pending||(this._pending=!0,ht.push(this))},t.prototype.depend=function(e){t.target&&t.target.addDep(this)},t.prototype.notify=function(t){for(var e=this.subs.filter((function(t){return t})),n=0,r=e.length;n<r;n++){e[n].update()}},t}();mt.target=null;var gt=[];function yt(t){gt.push(t),mt.target=t}function _t(){gt.pop(),mt.target=gt[gt.length-1]}var bt=Array.prototype,$t=Object.create(bt);["push","pop","shift","unshift","splice","sort","reverse"].forEach((function(t){var e=bt[t];z($t,t,(function(){for(var n=[],r=0;r<arguments.length;r++)n[r]=arguments[r];var o,i=e.apply(this,n),a=this.__ob__;switch(t){case"push":case"unshift":o=n;break;case"splice":o=n.slice(2)}return o&&a.observeArray(o),a.dep.notify(),i}))}));var wt=Object.getOwnPropertyNames($t),xt={},Ct=!0;function kt(t){Ct=t}var St={notify:j,depend:j,addSub:j,removeSub:j},Ot=function(){function t(t,n,r){if(void 0===n&&(n=!1),void 0===r&&(r=!1),this.value=t,this.shallow=n,this.mock=r,this.dep=r?St:new mt,this.vmCount=0,z(t,"__ob__",this),e(t)){if(!r)if(K)t.__proto__=$t;else for(var o=0,i=wt.length;o<i;o++){z(t,s=wt[o],$t[s])}n||this.observeArray(t)}else{var a=Object.keys(t);for(o=0;o<a.length;o++){var s;At(t,s=a[o],xt,void 0,n,r)}}}return t.prototype.observeArray=function(t){for(var e=0,n=t.length;e<n;e++)Tt(t[e],!1,this.mock)},t}();function Tt(t,n,r){return t&&_(t,"__ob__")&&t.__ob__ instanceof Ot?t.__ob__:!Ct||!r&&rt()||!e(t)&&!u(t)||!Object.isExtensible(t)||t.__v_skip||Ft(t)||t instanceof lt?void 0:new Ot(t,n,r)}function At(t,n,r,o,i,a){var s=new mt,c=Object.getOwnPropertyDescriptor(t,n);if(!c||!1!==c.configurable){var u=c&&c.get,l=c&&c.set;u&&!l||r!==xt&&2!==arguments.length||(r=t[n]);var f=!i&&Tt(r,!1,a);return Object.defineProperty(t,n,{enumerable:!0,configurable:!0,get:function(){var n=u?u.call(t):r;return mt.target&&(s.depend(),f&&(f.dep.depend(),e(n)&&Nt(n))),Ft(n)&&!i?n.value:n},set:function(e){var n=u?u.call(t):r;if(I(n,e)){if(l)l.call(t,e);else{if(u)return;if(!i&&Ft(n)&&!Ft(e))return void(n.value=e);r=e}f=!i&&Tt(e,!1,a),s.notify()}}}),s}}function jt(t,n,r){if(!Lt(t)){var o=t.__ob__;return e(t)&&l(n)?(t.length=Math.max(t.length,n),t.splice(n,1,r),o&&!o.shallow&&o.mock&&Tt(r,!1,!0),r):n in t&&!(n in Object.prototype)?(t[n]=r,r):t._isVue||o&&o.vmCount?r:o?(At(o.value,n,r,void 0,o.shallow,o.mock),o.dep.notify(),r):(t[n]=r,r)}}function Et(t,n){if(e(t)&&l(n))t.splice(n,1);else{var r=t.__ob__;t._isVue||r&&r.vmCount||Lt(t)||_(t,n)&&(delete t[n],r&&r.dep.notify())}}function Nt(t){for(var n=void 0,r=0,o=t.length;r<o;r++)(n=t[r])&&n.__ob__&&n.__ob__.dep.depend(),e(n)&&Nt(n)}function Pt(t){return Dt(t,!0),z(t,"__v_isShallow",!0),t}function Dt(t,e){Lt(t)||Tt(t,e,rt())}function Mt(t){return Lt(t)?Mt(t.__v_raw):!(!t||!t.__ob__)}function It(t){return!(!t||!t.__v_isShallow)}function Lt(t){return!(!t||!t.__v_isReadonly)}var Rt="__v_isRef";function Ft(t){return!(!t||!0!==t.__v_isRef)}function Ht(t,e){if(Ft(t))return t;var n={};return z(n,Rt,!0),z(n,"__v_isShallow",e),z(n,"dep",At(n,"value",t,null,e,rt())),n}function Bt(t,e,n){Object.defineProperty(t,n,{enumerable:!0,configurable:!0,get:function(){var t=e[n];if(Ft(t))return t.value;var r=t&&t.__ob__;return r&&r.dep.depend(),t},set:function(t){var r=e[n];Ft(r)&&!Ft(t)?r.value=t:e[n]=t}})}function Ut(t,e,n){var r=t[e];if(Ft(r))return r;var o={get value(){var r=t[e];return void 0===r?n:r},set value(n){t[e]=n}};return z(o,Rt,!0),o}function zt(t){return Vt(t,!1)}function Vt(t,e){if(!u(t))return t;if(Lt(t))return t;var n=e?"__v_rawToShallowReadonly":"__v_rawToReadonly",r=t[n];if(r)return r;var o=Object.create(Object.getPrototypeOf(t));z(t,n,o),z(o,"__v_isReadonly",!0),z(o,"__v_raw",t),Ft(t)&&z(o,Rt,!0),(e||It(t))&&z(o,"__v_isShallow",!0);for(var i=Object.keys(t),a=0;a<i.length;a++)Kt(o,t,i[a],e);return o}function Kt(t,e,n,r){Object.defineProperty(t,n,{enumerable:!0,configurable:!0,get:function(){var t=e[n];return r||!u(t)?t:zt(t)},set:function(){}})}var Jt=b((function(t){var e="&"===t.charAt(0),n="~"===(t=e?t.slice(1):t).charAt(0),r="!"===(t=n?t.slice(1):t).charAt(0);return{name:t=r?t.slice(1):t,once:n,capture:r,passive:e}}));function qt(t,n){function r(){var t=r.fns;if(!e(t))return dn(t,null,arguments,n,"v-on handler");for(var o=t.slice(),i=0;i<o.length;i++)dn(o[i],null,arguments,n,"v-on handler")}return r.fns=t,r}function Wt(t,e,r,i,a,s){var c,u,l,f;for(c in t)u=t[c],l=e[c],f=Jt(c),n(u)||(n(l)?(n(u.fns)&&(u=t[c]=qt(u,s)),o(f.once)&&(u=t[c]=a(f.name,u,f.capture)),r(f.name,u,f.capture,f.passive,f.params)):u!==l&&(l.fns=u,t[c]=l));for(c in e)n(t[c])&&i((f=Jt(c)).name,e[c],f.capture)}function Zt(t,e,i){var a;t instanceof lt&&(t=t.data.hook||(t.data.hook={}));var s=t[e];function c(){i.apply(this,arguments),g(a.fns,c)}n(s)?a=qt([c]):r(s.fns)&&o(s.merged)?(a=s).fns.push(c):a=qt([s,c]),a.merged=!0,t[e]=a}function Gt(t,e,n,o,i){if(r(e)){if(_(e,n))return t[n]=e[n],i||delete e[n],!0;if(_(e,o))return t[n]=e[o],i||delete e[o],!0}return!1}function Xt(t){return i(t)?[dt(t)]:e(t)?Qt(t):void 0}function Yt(t){return r(t)&&r(t.text)&&!1===t.isComment}function Qt(t,a){var s,c,u,l,f=[];for(s=0;s<t.length;s++)n(c=t[s])||"boolean"==typeof c||(l=f[u=f.length-1],e(c)?c.length>0&&(Yt((c=Qt(c,"".concat(a||"","_").concat(s)))[0])&&Yt(l)&&(f[u]=dt(l.text+c[0].text),c.shift()),f.push.apply(f,c)):i(c)?Yt(l)?f[u]=dt(l.text+c):""!==c&&f.push(dt(c)):Yt(c)&&Yt(l)?f[u]=dt(l.text+c.text):(o(t._isVList)&&r(c.tag)&&n(c.key)&&r(a)&&(c.key="__vlist".concat(a,"_").concat(s,"__")),f.push(c)));return f}function te(t,n,c,u,l,f){return(e(c)||i(c))&&(l=u,u=c,c=void 0),o(f)&&(l=2),function(t,n,o,i,c){if(r(o)&&r(o.__ob__))return ft();r(o)&&r(o.is)&&(n=o.is);if(!n)return ft();e(i)&&a(i[0])&&((o=o||{}).scopedSlots={default:i[0]},i.length=0);2===c?i=Xt(i):1===c&&(i=function(t){for(var n=0;n<t.length;n++)if(e(t[n]))return Array.prototype.concat.apply([],t);return t}(i));var u,l;if("string"==typeof n){var f=void 0;l=t.$vnode&&t.$vnode.ns||H.getTagNamespace(n),u=H.isReservedTag(n)?new lt(H.parsePlatformTagName(n),o,i,void 0,void 0,t):o&&o.pre||!r(f=yr(t.$options,"components",n))?new lt(n,o,i,void 0,void 0,t):cr(f,o,t,i,n)}else u=cr(n,o,t,i);return e(u)?u:r(u)?(r(l)&&ee(u,l),r(o)&&function(t){s(t.style)&&Bn(t.style);s(t.class)&&Bn(t.class)}(o),u):ft()}(t,n,c,u,l)}function ee(t,e,i){if(t.ns=e,"foreignObject"===t.tag&&(e=void 0,i=!0),r(t.children))for(var a=0,s=t.children.length;a<s;a++){var c=t.children[a];r(c.tag)&&(n(c.ns)||o(i)&&"svg"!==c.tag)&&ee(c,e,i)}}function ne(t,n){var o,i,a,c,u=null;if(e(t)||"string"==typeof t)for(u=new Array(t.length),o=0,i=t.length;o<i;o++)u[o]=n(t[o],o);else if("number"==typeof t)for(u=new Array(t),o=0;o<t;o++)u[o]=n(o+1,o);else if(s(t))if(st&&t[Symbol.iterator]){u=[];for(var l=t[Symbol.iterator](),f=l.next();!f.done;)u.push(n(f.value,u.length)),f=l.next()}else for(a=Object.keys(t),u=new Array(a.length),o=0,i=a.length;o<i;o++)c=a[o],u[o]=n(t[c],c,o);return r(u)||(u=[]),u._isVList=!0,u}function re(t,e,n,r){var o,i=this.$scopedSlots[t];i?(n=n||{},r&&(n=T(T({},r),n)),o=i(n)||(a(e)?e():e)):o=this.$slots[t]||(a(e)?e():e);var s=n&&n.slot;return s?this.$createElement("template",{slot:s},o):o}function oe(t){return yr(this.$options,"filters",t)||N}function ie(t,n){return e(t)?-1===t.indexOf(n):t!==n}function ae(t,e,n,r,o){var i=H.keyCodes[e]||n;return o&&r&&!H.keyCodes[e]?ie(o,r):i?ie(i,t):r?k(r)!==e:void 0===t}function se(t,n,r,o,i){if(r)if(s(r)){e(r)&&(r=A(r));var a=void 0,c=function(e){if("class"===e||"style"===e||m(e))a=t;else{var s=t.attrs&&t.attrs.type;a=o||H.mustUseProp(n,s,e)?t.domProps||(t.domProps={}):t.attrs||(t.attrs={})}var c=w(e),u=k(e);c in a||u in a||(a[e]=r[e],i&&((t.on||(t.on={}))["update:".concat(e)]=function(t){r[e]=t}))};for(var u in r)c(u)}else;return t}function ce(t,e){var n=this._staticTrees||(this._staticTrees=[]),r=n[t];return r&&!e||le(r=n[t]=this.$options.staticRenderFns[t].call(this._renderProxy,this._c,this),"__static__".concat(t),!1),r}function ue(t,e,n){return le(t,"__once__".concat(e).concat(n?"_".concat(n):""),!0),t}function le(t,n,r){if(e(t))for(var o=0;o<t.length;o++)t[o]&&"string"!=typeof t[o]&&fe(t[o],"".concat(n,"_").concat(o),r);else fe(t,n,r)}function fe(t,e,n){t.isStatic=!0,t.key=e,t.isOnce=n}function de(t,e){if(e)if(u(e)){var n=t.on=t.on?T({},t.on):{};for(var r in e){var o=n[r],i=e[r];n[r]=o?[].concat(o,i):i}}else;return t}function pe(t,n,r,o){n=n||{$stable:!r};for(var i=0;i<t.length;i++){var a=t[i];e(a)?pe(a,n,r):a&&(a.proxy&&(a.fn.proxy=!0),n[a.key]=a.fn)}return o&&(n.$key=o),n}function ve(t,e){for(var n=0;n<e.length;n+=2){var r=e[n];"string"==typeof r&&r&&(t[e[n]]=e[n+1])}return t}function he(t,e){return"string"==typeof t?e+t:t}function me(t){t._o=ue,t._n=p,t._s=d,t._l=ne,t._t=re,t._q=P,t._i=D,t._m=ce,t._f=oe,t._k=ae,t._b=se,t._v=dt,t._e=ft,t._u=pe,t._g=de,t._d=ve,t._p=he}function ge(t,e){if(!t||!t.length)return{};for(var n={},r=0,o=t.length;r<o;r++){var i=t[r],a=i.data;if(a&&a.attrs&&a.attrs.slot&&delete a.attrs.slot,i.context!==e&&i.fnContext!==e||!a||null==a.slot)(n.default||(n.default=[])).push(i);else{var s=a.slot,c=n[s]||(n[s]=[]);"template"===i.tag?c.push.apply(c,i.children||[]):c.push(i)}}for(var u in n)n[u].every(ye)&&delete n[u];return n}function ye(t){return t.isComment&&!t.asyncFactory||" "===t.text}function _e(t){return t.isComment&&t.asyncFactory}function be(e,n,r,o){var i,a=Object.keys(r).length>0,s=n?!!n.$stable:!a,c=n&&n.$key;if(n){if(n._normalized)return n._normalized;if(s&&o&&o!==t&&c===o.$key&&!a&&!o.$hasNormal)return o;for(var u in i={},n)n[u]&&"$"!==u[0]&&(i[u]=$e(e,r,u,n[u]))}else i={};for(var l in r)l in i||(i[l]=we(r,l));return n&&Object.isExtensible(n)&&(n._normalized=i),z(i,"$stable",s),z(i,"$key",c),z(i,"$hasNormal",a),i}function $e(t,n,r,o){var i=function(){var n=ct;ut(t);var r=arguments.length?o.apply(null,arguments):o({}),i=(r=r&&"object"==typeof r&&!e(r)?[r]:Xt(r))&&r[0];return ut(n),r&&(!i||1===r.length&&i.isComment&&!_e(i))?void 0:r};return o.proxy&&Object.defineProperty(n,r,{get:i,enumerable:!0,configurable:!0}),i}function we(t,e){return function(){return t[e]}}function xe(e){return{get attrs(){if(!e._attrsProxy){var n=e._attrsProxy={};z(n,"_v_attr_proxy",!0),Ce(n,e.$attrs,t,e,"$attrs")}return e._attrsProxy},get listeners(){e._listenersProxy||Ce(e._listenersProxy={},e.$listeners,t,e,"$listeners");return e._listenersProxy},get slots(){return function(t){t._slotsProxy||Se(t._slotsProxy={},t.$scopedSlots);return t._slotsProxy}(e)},emit:S(e.$emit,e),expose:function(t){t&&Object.keys(t).forEach((function(n){return Bt(e,t,n)}))}}}function Ce(t,e,n,r,o){var i=!1;for(var a in e)a in t?e[a]!==n[a]&&(i=!0):(i=!0,ke(t,a,r,o));for(var a in t)a in e||(i=!0,delete t[a]);return i}function ke(t,e,n,r){Object.defineProperty(t,e,{enumerable:!0,configurable:!0,get:function(){return n[r][e]}})}function Se(t,e){for(var n in e)t[n]=e[n];for(var n in t)n in e||delete t[n]}function Oe(){var t=ct;return t._setupContext||(t._setupContext=xe(t))}var Te,Ae=null;function je(t,e){return(t.__esModule||st&&"Module"===t[Symbol.toStringTag])&&(t=t.default),s(t)?e.extend(t):t}function Ee(t){if(e(t))for(var n=0;n<t.length;n++){var o=t[n];if(r(o)&&(r(o.componentOptions)||_e(o)))return o}}function Ne(t,e){Te.$on(t,e)}function Pe(t,e){Te.$off(t,e)}function De(t,e){var n=Te;return function r(){var o=e.apply(null,arguments);null!==o&&n.$off(t,r)}}function Me(t,e,n){Te=t,Wt(e,n||{},Ne,Pe,De,t),Te=void 0}var Ie=null;function Le(t){var e=Ie;return Ie=t,function(){Ie=e}}function Re(t){for(;t&&(t=t.$parent);)if(t._inactive)return!0;return!1}function Fe(t,e){if(e){if(t._directInactive=!1,Re(t))return}else if(t._directInactive)return;if(t._inactive||null===t._inactive){t._inactive=!1;for(var n=0;n<t.$children.length;n++)Fe(t.$children[n]);Be(t,"activated")}}function He(t,e){if(!(e&&(t._directInactive=!0,Re(t))||t._inactive)){t._inactive=!0;for(var n=0;n<t.$children.length;n++)He(t.$children[n]);Be(t,"deactivated")}}function Be(t,e,n,r){void 0===r&&(r=!0),yt();var o=ct;r&&ut(t);var i=t.$options[e],a="".concat(e," hook");if(i)for(var s=0,c=i.length;s<c;s++)dn(i[s],t,n||null,t,a);t._hasHookEvent&&t.$emit("hook:"+e),r&&ut(o),_t()}var Ue=[],ze=[],Ve={},Ke=!1,Je=!1,qe=0;var We=0,Ze=Date.now;if(J&&!W){var Ge=window.performance;Ge&&"function"==typeof Ge.now&&Ze()>document.createEvent("Event").timeStamp&&(Ze=function(){return Ge.now()})}var Xe=function(t,e){if(t.post){if(!e.post)return 1}else if(e.post)return-1;return t.id-e.id};function Ye(){var t,e;for(We=Ze(),Je=!0,Ue.sort(Xe),qe=0;qe<Ue.length;qe++)(t=Ue[qe]).before&&t.before(),e=t.id,Ve[e]=null,t.run();var n=ze.slice(),r=Ue.slice();qe=Ue.length=ze.length=0,Ve={},Ke=Je=!1,function(t){for(var e=0;e<t.length;e++)t[e]._inactive=!0,Fe(t[e],!0)}(n),function(t){var e=t.length;for(;e--;){var n=t[e],r=n.vm;r&&r._watcher===n&&r._isMounted&&!r._isDestroyed&&Be(r,"updated")}}(r),function(){for(var t=0;t<ht.length;t++){var e=ht[t];e.subs=e.subs.filter((function(t){return t})),e._pending=!1}ht.length=0}(),ot&&H.devtools&&ot.emit("flush")}function Qe(t){var e=t.id;if(null==Ve[e]&&(t!==mt.target||!t.noRecurse)){if(Ve[e]=!0,Je){for(var n=Ue.length-1;n>qe&&Ue[n].id>t.id;)n--;Ue.splice(n+1,0,t)}else Ue.push(t);Ke||(Ke=!0,Cn(Ye))}}var tn="watcher",en="".concat(tn," callback"),nn="".concat(tn," getter"),rn="".concat(tn," cleanup");function on(t,e){return cn(t,null,{flush:"post"})}var an,sn={};function cn(n,r,o){var i=void 0===o?t:o,s=i.immediate,c=i.deep,u=i.flush,l=void 0===u?"pre":u;i.onTrack,i.onTrigger;var f,d,p=ct,v=function(t,e,n){return void 0===n&&(n=null),dn(t,null,n,p,e)},h=!1,m=!1;if(Ft(n)?(f=function(){return n.value},h=It(n)):Mt(n)?(f=function(){return n.__ob__.dep.depend(),n},c=!0):e(n)?(m=!0,h=n.some((function(t){return Mt(t)||It(t)})),f=function(){return n.map((function(t){return Ft(t)?t.value:Mt(t)?Bn(t):a(t)?v(t,nn):void 0}))}):f=a(n)?r?function(){return v(n,nn)}:function(){if(!p||!p._isDestroyed)return d&&d(),v(n,tn,[y])}:j,r&&c){var g=f;f=function(){return Bn(g())}}var y=function(t){d=_.onStop=function(){v(t,rn)}};if(rt())return y=j,r?s&&v(r,en,[f(),m?[]:void 0,y]):f(),j;var _=new Vn(ct,f,j,{lazy:!0});_.noRecurse=!r;var b=m?[]:sn;return _.run=function(){if(_.active)if(r){var t=_.get();(c||h||(m?t.some((function(t,e){return I(t,b[e])})):I(t,b)))&&(d&&d(),v(r,en,[t,b===sn?void 0:b,y]),b=t)}else _.get()},"sync"===l?_.update=_.run:"post"===l?(_.post=!0,_.update=function(){return Qe(_)}):_.update=function(){if(p&&p===ct&&!p._isMounted){var t=p._preWatchers||(p._preWatchers=[]);t.indexOf(_)<0&&t.push(_)}else Qe(_)},r?s?_.run():b=_.get():"post"===l&&p?p.$once("hook:mounted",(function(){return _.get()})):_.get(),function(){_.teardown()}}var un=function(){function t(t){void 0===t&&(t=!1),this.detached=t,this.active=!0,this.effects=[],this.cleanups=[],this.parent=an,!t&&an&&(this.index=(an.scopes||(an.scopes=[])).push(this)-1)}return t.prototype.run=function(t){if(this.active){var e=an;try{return an=this,t()}finally{an=e}}},t.prototype.on=function(){an=this},t.prototype.off=function(){an=this.parent},t.prototype.stop=function(t){if(this.active){var e=void 0,n=void 0;for(e=0,n=this.effects.length;e<n;e++)this.effects[e].teardown();for(e=0,n=this.cleanups.length;e<n;e++)this.cleanups[e]();if(this.scopes)for(e=0,n=this.scopes.length;e<n;e++)this.scopes[e].stop(!0);if(!this.detached&&this.parent&&!t){var r=this.parent.scopes.pop();r&&r!==this&&(this.parent.scopes[this.index]=r,r.index=this.index)}this.parent=void 0,this.active=!1}},t}();function ln(t){var e=t._provided,n=t.$parent&&t.$parent._provided;return n===e?t._provided=Object.create(n):e}function fn(t,e,n){yt();try{if(e)for(var r=e;r=r.$parent;){var o=r.$options.errorCaptured;if(o)for(var i=0;i<o.length;i++)try{if(!1===o[i].call(r,t,e,n))return}catch(t){pn(t,r,"errorCaptured hook")}}pn(t,e,n)}finally{_t()}}function dn(t,e,n,r,o){var i;try{(i=n?t.apply(e,n):t.call(e))&&!i._isVue&&f(i)&&!i._handled&&(i.catch((function(t){return fn(t,r,o+" (Promise/async)")})),i._handled=!0)}catch(t){fn(t,r,o)}return i}function pn(t,e,n){if(H.errorHandler)try{return H.errorHandler.call(null,t,e,n)}catch(e){e!==t&&vn(e)}vn(t)}function vn(t,e,n){if(!J||"undefined"==typeof console)throw t;console.error(t)}var hn,mn=!1,gn=[],yn=!1;function _n(){yn=!1;var t=gn.slice(0);gn.length=0;for(var e=0;e<t.length;e++)t[e]()}if("undefined"!=typeof Promise&&it(Promise)){var bn=Promise.resolve();hn=function(){bn.then(_n),X&&setTimeout(j)},mn=!0}else if(W||"undefined"==typeof MutationObserver||!it(MutationObserver)&&"[object MutationObserverConstructor]"!==MutationObserver.toString())hn="undefined"!=typeof setImmediate&&it(setImmediate)?function(){setImmediate(_n)}:function(){setTimeout(_n,0)};else{var $n=1,wn=new MutationObserver(_n),xn=document.createTextNode(String($n));wn.observe(xn,{characterData:!0}),hn=function(){$n=($n+1)%2,xn.data=String($n)},mn=!0}function Cn(t,e){var n;if(gn.push((function(){if(t)try{t.call(e)}catch(t){fn(t,e,"nextTick")}else n&&n(e)})),yn||(yn=!0,hn()),!t&&"undefined"!=typeof Promise)return new Promise((function(t){n=t}))}function kn(t){return function(e,n){if(void 0===n&&(n=ct),n)return function(t,e,n){var r=t.$options;r[e]=vr(r[e],n)}(n,t,e)}}var Sn=kn("beforeMount"),On=kn("mounted"),Tn=kn("beforeUpdate"),An=kn("updated"),jn=kn("beforeDestroy"),En=kn("destroyed"),Nn=kn("activated"),Pn=kn("deactivated"),Dn=kn("serverPrefetch"),Mn=kn("renderTracked"),In=kn("renderTriggered"),Ln=kn("errorCaptured");var Rn="2.7.14";var Fn=Object.freeze({__proto__:null,version:Rn,defineComponent:function(t){return t},ref:function(t){return Ht(t,!1)},shallowRef:function(t){return Ht(t,!0)},isRef:Ft,toRef:Ut,toRefs:function(t){var n=e(t)?new Array(t.length):{};for(var r in t)n[r]=Ut(t,r);return n},unref:function(t){return Ft(t)?t.value:t},proxyRefs:function(t){if(Mt(t))return t;for(var e={},n=Object.keys(t),r=0;r<n.length;r++)Bt(e,t,n[r]);return e},customRef:function(t){var e=new mt,n=t((function(){e.depend()}),(function(){e.notify()})),r=n.get,o=n.set,i={get value(){return r()},set value(t){o(t)}};return z(i,Rt,!0),i},triggerRef:function(t){t.dep&&t.dep.notify()},reactive:function(t){return Dt(t,!1),t},isReactive:Mt,isReadonly:Lt,isShallow:It,isProxy:function(t){return Mt(t)||Lt(t)},shallowReactive:Pt,markRaw:function(t){return Object.isExtensible(t)&&z(t,"__v_skip",!0),t},toRaw:function t(e){var n=e&&e.__v_raw;return n?t(n):e},readonly:zt,shallowReadonly:function(t){return Vt(t,!0)},computed:function(t,e){var n,r,o=a(t);o?(n=t,r=j):(n=t.get,r=t.set);var i=rt()?null:new Vn(ct,n,j,{lazy:!0}),s={effect:i,get value(){return i?(i.dirty&&i.evaluate(),mt.target&&i.depend(),i.value):n()},set value(t){r(t)}};return z(s,Rt,!0),z(s,"__v_isReadonly",o),s},watch:function(t,e,n){return cn(t,e,n)},watchEffect:function(t,e){return cn(t,null,e)},watchPostEffect:on,watchSyncEffect:function(t,e){return cn(t,null,{flush:"sync"})},EffectScope:un,effectScope:function(t){return new un(t)},onScopeDispose:function(t){an&&an.cleanups.push(t)},getCurrentScope:function(){return an},provide:function(t,e){ct&&(ln(ct)[t]=e)},inject:function(t,e,n){void 0===n&&(n=!1);var r=ct;if(r){var o=r.$parent&&r.$parent._provided;if(o&&t in o)return o[t];if(arguments.length>1)return n&&a(e)?e.call(r):e}},h:function(t,e,n){return te(ct,t,e,n,2,!0)},getCurrentInstance:function(){return ct&&{proxy:ct}},useSlots:function(){return Oe().slots},useAttrs:function(){return Oe().attrs},useListeners:function(){return Oe().listeners},mergeDefaults:function(t,n){var r=e(t)?t.reduce((function(t,e){return t[e]={},t}),{}):t;for(var o in n){var i=r[o];i?e(i)||a(i)?r[o]={type:i,default:n[o]}:i.default=n[o]:null===i&&(r[o]={default:n[o]})}return r},nextTick:Cn,set:jt,del:Et,useCssModule:function(e){return t},useCssVars:function(t){if(J){var e=ct;e&&on((function(){var n=e.$el,r=t(e,e._setupProxy);if(n&&1===n.nodeType){var o=n.style;for(var i in r)o.setProperty("--".concat(i),r[i])}}))}},defineAsyncComponent:function(t){a(t)&&(t={loader:t});var e=t.loader,n=t.loadingComponent,r=t.errorComponent,o=t.delay,i=void 0===o?200:o,s=t.timeout;t.suspensible;var c=t.onError,u=null,l=0,f=function(){var t;return u||(t=u=e().catch((function(t){if(t=t instanceof Error?t:new Error(String(t)),c)return new Promise((function(e,n){c(t,(function(){return e((l++,u=null,f()))}),(function(){return n(t)}),l+1)}));throw t})).then((function(e){return t!==u&&u?u:(e&&(e.__esModule||"Module"===e[Symbol.toStringTag])&&(e=e.default),e)})))};return function(){return{component:f(),delay:i,timeout:s,error:r,loading:n}}},onBeforeMount:Sn,onMounted:On,onBeforeUpdate:Tn,onUpdated:An,onBeforeUnmount:jn,onUnmounted:En,onActivated:Nn,onDeactivated:Pn,onServerPrefetch:Dn,onRenderTracked:Mn,onRenderTriggered:In,onErrorCaptured:function(t,e){void 0===e&&(e=ct),Ln(t,e)}}),Hn=new at;function Bn(t){return Un(t,Hn),Hn.clear(),t}function Un(t,n){var r,o,i=e(t);if(!(!i&&!s(t)||t.__v_skip||Object.isFrozen(t)||t instanceof lt)){if(t.__ob__){var a=t.__ob__.dep.id;if(n.has(a))return;n.add(a)}if(i)for(r=t.length;r--;)Un(t[r],n);else if(Ft(t))Un(t.value,n);else for(r=(o=Object.keys(t)).length;r--;)Un(t[o[r]],n)}}var zn=0,Vn=function(){function t(t,e,n,r,o){!function(t,e){void 0===e&&(e=an),e&&e.active&&e.effects.push(t)}(this,an&&!an._vm?an:t?t._scope:void 0),(this.vm=t)&&o&&(t._watcher=this),r?(this.deep=!!r.deep,this.user=!!r.user,this.lazy=!!r.lazy,this.sync=!!r.sync,this.before=r.before):this.deep=this.user=this.lazy=this.sync=!1,this.cb=n,this.id=++zn,this.active=!0,this.post=!1,this.dirty=this.lazy,this.deps=[],this.newDeps=[],this.depIds=new at,this.newDepIds=new at,this.expression="",a(e)?this.getter=e:(this.getter=function(t){if(!V.test(t)){var e=t.split(".");return function(t){for(var n=0;n<e.length;n++){if(!t)return;t=t[e[n]]}return t}}}(e),this.getter||(this.getter=j)),this.value=this.lazy?void 0:this.get()}return t.prototype.get=function(){var t;yt(this);var e=this.vm;try{t=this.getter.call(e,e)}catch(t){if(!this.user)throw t;fn(t,e,'getter for watcher "'.concat(this.expression,'"'))}finally{this.deep&&Bn(t),_t(),this.cleanupDeps()}return t},t.prototype.addDep=function(t){var e=t.id;this.newDepIds.has(e)||(this.newDepIds.add(e),this.newDeps.push(t),this.depIds.has(e)||t.addSub(this))},t.prototype.cleanupDeps=function(){for(var t=this.deps.length;t--;){var e=this.deps[t];this.newDepIds.has(e.id)||e.removeSub(this)}var n=this.depIds;this.depIds=this.newDepIds,this.newDepIds=n,this.newDepIds.clear(),n=this.deps,this.deps=this.newDeps,this.newDeps=n,this.newDeps.length=0},t.prototype.update=function(){this.lazy?this.dirty=!0:this.sync?this.run():Qe(this)},t.prototype.run=function(){if(this.active){var t=this.get();if(t!==this.value||s(t)||this.deep){var e=this.value;if(this.value=t,this.user){var n='callback for watcher "'.concat(this.expression,'"');dn(this.cb,this.vm,[t,e],this.vm,n)}else this.cb.call(this.vm,t,e)}}},t.prototype.evaluate=function(){this.value=this.get(),this.dirty=!1},t.prototype.depend=function(){for(var t=this.deps.length;t--;)this.deps[t].depend()},t.prototype.teardown=function(){if(this.vm&&!this.vm._isBeingDestroyed&&g(this.vm._scope.effects,this),this.active){for(var t=this.deps.length;t--;)this.deps[t].removeSub(this);this.active=!1,this.onStop&&this.onStop()}},t}(),Kn={enumerable:!0,configurable:!0,get:j,set:j};function Jn(t,e,n){Kn.get=function(){return this[e][n]},Kn.set=function(t){this[e][n]=t},Object.defineProperty(t,n,Kn)}function qn(t){var n=t.$options;if(n.props&&function(t,e){var n=t.$options.propsData||{},r=t._props=Pt({}),o=t.$options._propKeys=[];t.$parent&&kt(!1);var i=function(i){o.push(i);var a=_r(i,e,n,t);At(r,i,a),i in t||Jn(t,"_props",i)};for(var a in e)i(a);kt(!0)}(t,n.props),function(t){var e=t.$options,n=e.setup;if(n){var r=t._setupContext=xe(t);ut(t),yt();var o=dn(n,null,[t._props||Pt({}),r],t,"setup");if(_t(),ut(),a(o))e.render=o;else if(s(o))if(t._setupState=o,o.__sfc){var i=t._setupProxy={};for(var c in o)"__sfc"!==c&&Bt(i,o,c)}else for(var c in o)U(c)||Bt(t,o,c)}}(t),n.methods&&function(t,e){for(var n in t.$options.props,e)t[n]="function"!=typeof e[n]?j:S(e[n],t)}(t,n.methods),n.data)!function(t){var e=t.$options.data;u(e=t._data=a(e)?function(t,e){yt();try{return t.call(e,e)}catch(t){return fn(t,e,"data()"),{}}finally{_t()}}(e,t):e||{})||(e={});var n=Object.keys(e),r=t.$options.props;t.$options.methods;var o=n.length;for(;o--;){var i=n[o];r&&_(r,i)||U(i)||Jn(t,"_data",i)}var s=Tt(e);s&&s.vmCount++}(t);else{var r=Tt(t._data={});r&&r.vmCount++}n.computed&&function(t,e){var n=t._computedWatchers=Object.create(null),r=rt();for(var o in e){var i=e[o],s=a(i)?i:i.get;r||(n[o]=new Vn(t,s||j,j,Wn)),o in t||Zn(t,o,i)}}(t,n.computed),n.watch&&n.watch!==tt&&function(t,n){for(var r in n){var o=n[r];if(e(o))for(var i=0;i<o.length;i++)Yn(t,r,o[i]);else Yn(t,r,o)}}(t,n.watch)}var Wn={lazy:!0};function Zn(t,e,n){var r=!rt();a(n)?(Kn.get=r?Gn(e):Xn(n),Kn.set=j):(Kn.get=n.get?r&&!1!==n.cache?Gn(e):Xn(n.get):j,Kn.set=n.set||j),Object.defineProperty(t,e,Kn)}function Gn(t){return function(){var e=this._computedWatchers&&this._computedWatchers[t];if(e)return e.dirty&&e.evaluate(),mt.target&&e.depend(),e.value}}function Xn(t){return function(){return t.call(this,this)}}function Yn(t,e,n,r){return u(n)&&(r=n,n=n.handler),"string"==typeof n&&(n=t[n]),t.$watch(e,n,r)}function Qn(t,e){if(t){for(var n=Object.create(null),r=st?Reflect.ownKeys(t):Object.keys(t),o=0;o<r.length;o++){var i=r[o];if("__ob__"!==i){var s=t[i].from;if(s in e._provided)n[i]=e._provided[s];else if("default"in t[i]){var c=t[i].default;n[i]=a(c)?c.call(e):c}}}return n}}var tr=0;function er(t){var e=t.options;if(t.super){var n=er(t.super);if(n!==t.superOptions){t.superOptions=n;var r=function(t){var e,n=t.options,r=t.sealedOptions;for(var o in n)n[o]!==r[o]&&(e||(e={}),e[o]=n[o]);return e}(t);r&&T(t.extendOptions,r),(e=t.options=gr(n,t.extendOptions)).name&&(e.components[e.name]=t)}}return e}function nr(n,r,i,a,s){var c,u=this,l=s.options;_(a,"_uid")?(c=Object.create(a))._original=a:(c=a,a=a._original);var f=o(l._compiled),d=!f;this.data=n,this.props=r,this.children=i,this.parent=a,this.listeners=n.on||t,this.injections=Qn(l.inject,a),this.slots=function(){return u.$slots||be(a,n.scopedSlots,u.$slots=ge(i,a)),u.$slots},Object.defineProperty(this,"scopedSlots",{enumerable:!0,get:function(){return be(a,n.scopedSlots,this.slots())}}),f&&(this.$options=l,this.$slots=this.slots(),this.$scopedSlots=be(a,n.scopedSlots,this.$slots)),l._scopeId?this._c=function(t,n,r,o){var i=te(c,t,n,r,o,d);return i&&!e(i)&&(i.fnScopeId=l._scopeId,i.fnContext=a),i}:this._c=function(t,e,n,r){return te(c,t,e,n,r,d)}}function rr(t,e,n,r,o){var i=pt(t);return i.fnContext=n,i.fnOptions=r,e.slot&&((i.data||(i.data={})).slot=e.slot),i}function or(t,e){for(var n in e)t[w(n)]=e[n]}function ir(t){return t.name||t.__name||t._componentTag}me(nr.prototype);var ar={init:function(t,e){if(t.componentInstance&&!t.componentInstance._isDestroyed&&t.data.keepAlive){var n=t;ar.prepatch(n,n)}else{(t.componentInstance=function(t,e){var n={_isComponent:!0,_parentVnode:t,parent:e},o=t.data.inlineTemplate;r(o)&&(n.render=o.render,n.staticRenderFns=o.staticRenderFns);return new t.componentOptions.Ctor(n)}(t,Ie)).$mount(e?t.elm:void 0,e)}},prepatch:function(e,n){var r=n.componentOptions;!function(e,n,r,o,i){var a=o.data.scopedSlots,s=e.$scopedSlots,c=!!(a&&!a.$stable||s!==t&&!s.$stable||a&&e.$scopedSlots.$key!==a.$key||!a&&e.$scopedSlots.$key),u=!!(i||e.$options._renderChildren||c),l=e.$vnode;e.$options._parentVnode=o,e.$vnode=o,e._vnode&&(e._vnode.parent=o),e.$options._renderChildren=i;var f=o.data.attrs||t;e._attrsProxy&&Ce(e._attrsProxy,f,l.data&&l.data.attrs||t,e,"$attrs")&&(u=!0),e.$attrs=f,r=r||t;var d=e.$options._parentListeners;if(e._listenersProxy&&Ce(e._listenersProxy,r,d||t,e,"$listeners"),e.$listeners=e.$options._parentListeners=r,Me(e,r,d),n&&e.$options.props){kt(!1);for(var p=e._props,v=e.$options._propKeys||[],h=0;h<v.length;h++){var m=v[h],g=e.$options.props;p[m]=_r(m,g,n,e)}kt(!0),e.$options.propsData=n}u&&(e.$slots=ge(i,o.context),e.$forceUpdate())}(n.componentInstance=e.componentInstance,r.propsData,r.listeners,n,r.children)},insert:function(t){var e,n=t.context,r=t.componentInstance;r._isMounted||(r._isMounted=!0,Be(r,"mounted")),t.data.keepAlive&&(n._isMounted?((e=r)._inactive=!1,ze.push(e)):Fe(r,!0))},destroy:function(t){var e=t.componentInstance;e._isDestroyed||(t.data.keepAlive?He(e,!0):e.$destroy())}},sr=Object.keys(ar);function cr(i,a,c,u,l){if(!n(i)){var d=c.$options._base;if(s(i)&&(i=d.extend(i)),"function"==typeof i){var p;if(n(i.cid)&&(i=function(t,e){if(o(t.error)&&r(t.errorComp))return t.errorComp;if(r(t.resolved))return t.resolved;var i=Ae;if(i&&r(t.owners)&&-1===t.owners.indexOf(i)&&t.owners.push(i),o(t.loading)&&r(t.loadingComp))return t.loadingComp;if(i&&!r(t.owners)){var a=t.owners=[i],c=!0,u=null,l=null;i.$on("hook:destroyed",(function(){return g(a,i)}));var d=function(t){for(var e=0,n=a.length;e<n;e++)a[e].$forceUpdate();t&&(a.length=0,null!==u&&(clearTimeout(u),u=null),null!==l&&(clearTimeout(l),l=null))},p=M((function(n){t.resolved=je(n,e),c?a.length=0:d(!0)})),v=M((function(e){r(t.errorComp)&&(t.error=!0,d(!0))})),h=t(p,v);return s(h)&&(f(h)?n(t.resolved)&&h.then(p,v):f(h.component)&&(h.component.then(p,v),r(h.error)&&(t.errorComp=je(h.error,e)),r(h.loading)&&(t.loadingComp=je(h.loading,e),0===h.delay?t.loading=!0:u=setTimeout((function(){u=null,n(t.resolved)&&n(t.error)&&(t.loading=!0,d(!1))}),h.delay||200)),r(h.timeout)&&(l=setTimeout((function(){l=null,n(t.resolved)&&v(null)}),h.timeout)))),c=!1,t.loading?t.loadingComp:t.resolved}}(p=i,d),void 0===i))return function(t,e,n,r,o){var i=ft();return i.asyncFactory=t,i.asyncMeta={data:e,context:n,children:r,tag:o},i}(p,a,c,u,l);a=a||{},er(i),r(a.model)&&function(t,n){var o=t.model&&t.model.prop||"value",i=t.model&&t.model.event||"input";(n.attrs||(n.attrs={}))[o]=n.model.value;var a=n.on||(n.on={}),s=a[i],c=n.model.callback;r(s)?(e(s)?-1===s.indexOf(c):s!==c)&&(a[i]=[c].concat(s)):a[i]=c}(i.options,a);var v=function(t,e,o){var i=e.options.props;if(!n(i)){var a={},s=t.attrs,c=t.props;if(r(s)||r(c))for(var u in i){var l=k(u);Gt(a,c,u,l,!0)||Gt(a,s,u,l,!1)}return a}}(a,i);if(o(i.options.functional))return function(n,o,i,a,s){var c=n.options,u={},l=c.props;if(r(l))for(var f in l)u[f]=_r(f,l,o||t);else r(i.attrs)&&or(u,i.attrs),r(i.props)&&or(u,i.props);var d=new nr(i,u,s,a,n),p=c.render.call(null,d._c,d);if(p instanceof lt)return rr(p,i,d.parent,c);if(e(p)){for(var v=Xt(p)||[],h=new Array(v.length),m=0;m<v.length;m++)h[m]=rr(v[m],i,d.parent,c);return h}}(i,v,a,c,u);var h=a.on;if(a.on=a.nativeOn,o(i.options.abstract)){var m=a.slot;a={},m&&(a.slot=m)}!function(t){for(var e=t.hook||(t.hook={}),n=0;n<sr.length;n++){var r=sr[n],o=e[r],i=ar[r];o===i||o&&o._merged||(e[r]=o?ur(i,o):i)}}(a);var y=ir(i.options)||l;return new lt("vue-component-".concat(i.cid).concat(y?"-".concat(y):""),a,void 0,void 0,void 0,c,{Ctor:i,propsData:v,listeners:h,tag:l,children:u},p)}}}function ur(t,e){var n=function(n,r){t(n,r),e(n,r)};return n._merged=!0,n}var lr=j,fr=H.optionMergeStrategies;function dr(t,e,n){if(void 0===n&&(n=!0),!e)return t;for(var r,o,i,a=st?Reflect.ownKeys(e):Object.keys(e),s=0;s<a.length;s++)"__ob__"!==(r=a[s])&&(o=t[r],i=e[r],n&&_(t,r)?o!==i&&u(o)&&u(i)&&dr(o,i):jt(t,r,i));return t}function pr(t,e,n){return n?function(){var r=a(e)?e.call(n,n):e,o=a(t)?t.call(n,n):t;return r?dr(r,o):o}:e?t?function(){return dr(a(e)?e.call(this,this):e,a(t)?t.call(this,this):t)}:e:t}function vr(t,n){var r=n?t?t.concat(n):e(n)?n:[n]:t;return r?function(t){for(var e=[],n=0;n<t.length;n++)-1===e.indexOf(t[n])&&e.push(t[n]);return e}(r):r}function hr(t,e,n,r){var o=Object.create(t||null);return e?T(o,e):o}fr.data=function(t,e,n){return n?pr(t,e,n):e&&"function"!=typeof e?t:pr(t,e)},F.forEach((function(t){fr[t]=vr})),R.forEach((function(t){fr[t+"s"]=hr})),fr.watch=function(t,n,r,o){if(t===tt&&(t=void 0),n===tt&&(n=void 0),!n)return Object.create(t||null);if(!t)return n;var i={};for(var a in T(i,t),n){var s=i[a],c=n[a];s&&!e(s)&&(s=[s]),i[a]=s?s.concat(c):e(c)?c:[c]}return i},fr.props=fr.methods=fr.inject=fr.computed=function(t,e,n,r){if(!t)return e;var o=Object.create(null);return T(o,t),e&&T(o,e),o},fr.provide=function(t,e){return t?function(){var n=Object.create(null);return dr(n,a(t)?t.call(this):t),e&&dr(n,a(e)?e.call(this):e,!1),n}:e};var mr=function(t,e){return void 0===e?t:e};function gr(t,n,r){if(a(n)&&(n=n.options),function(t,n){var r=t.props;if(r){var o,i,a={};if(e(r))for(o=r.length;o--;)"string"==typeof(i=r[o])&&(a[w(i)]={type:null});else if(u(r))for(var s in r)i=r[s],a[w(s)]=u(i)?i:{type:i};t.props=a}}(n),function(t,n){var r=t.inject;if(r){var o=t.inject={};if(e(r))for(var i=0;i<r.length;i++)o[r[i]]={from:r[i]};else if(u(r))for(var a in r){var s=r[a];o[a]=u(s)?T({from:a},s):{from:s}}}}(n),function(t){var e=t.directives;if(e)for(var n in e){var r=e[n];a(r)&&(e[n]={bind:r,update:r})}}(n),!n._base&&(n.extends&&(t=gr(t,n.extends,r)),n.mixins))for(var o=0,i=n.mixins.length;o<i;o++)t=gr(t,n.mixins[o],r);var s,c={};for(s in t)l(s);for(s in n)_(t,s)||l(s);function l(e){var o=fr[e]||mr;c[e]=o(t[e],n[e],r,e)}return c}function yr(t,e,n,r){if("string"==typeof n){var o=t[e];if(_(o,n))return o[n];var i=w(n);if(_(o,i))return o[i];var a=x(i);return _(o,a)?o[a]:o[n]||o[i]||o[a]}}function _r(t,e,n,r){var o=e[t],i=!_(n,t),s=n[t],c=xr(Boolean,o.type);if(c>-1)if(i&&!_(o,"default"))s=!1;else if(""===s||s===k(t)){var u=xr(String,o.type);(u<0||c<u)&&(s=!0)}if(void 0===s){s=function(t,e,n){if(!_(e,"default"))return;var r=e.default;if(t&&t.$options.propsData&&void 0===t.$options.propsData[n]&&void 0!==t._props[n])return t._props[n];return a(r)&&"Function"!==$r(e.type)?r.call(t):r}(r,o,t);var l=Ct;kt(!0),Tt(s),kt(l)}return s}var br=/^\s*function (\w+)/;function $r(t){var e=t&&t.toString().match(br);return e?e[1]:""}function wr(t,e){return $r(t)===$r(e)}function xr(t,n){if(!e(n))return wr(n,t)?0:-1;for(var r=0,o=n.length;r<o;r++)if(wr(n[r],t))return r;return-1}function Cr(t){this._init(t)}function kr(t){t.cid=0;var e=1;t.extend=function(t){t=t||{};var n=this,r=n.cid,o=t._Ctor||(t._Ctor={});if(o[r])return o[r];var i=ir(t)||ir(n.options),a=function(t){this._init(t)};return(a.prototype=Object.create(n.prototype)).constructor=a,a.cid=e++,a.options=gr(n.options,t),a.super=n,a.options.props&&function(t){var e=t.options.props;for(var n in e)Jn(t.prototype,"_props",n)}(a),a.options.computed&&function(t){var e=t.options.computed;for(var n in e)Zn(t.prototype,n,e[n])}(a),a.extend=n.extend,a.mixin=n.mixin,a.use=n.use,R.forEach((function(t){a[t]=n[t]})),i&&(a.options.components[i]=a),a.superOptions=n.options,a.extendOptions=t,a.sealedOptions=T({},a.options),o[r]=a,a}}function Sr(t){return t&&(ir(t.Ctor.options)||t.tag)}function Or(t,n){return e(t)?t.indexOf(n)>-1:"string"==typeof t?t.split(",").indexOf(n)>-1:(r=t,"[object RegExp]"===c.call(r)&&t.test(n));var r}function Tr(t,e){var n=t.cache,r=t.keys,o=t._vnode;for(var i in n){var a=n[i];if(a){var s=a.name;s&&!e(s)&&Ar(n,i,r,o)}}}function Ar(t,e,n,r){var o=t[e];!o||r&&o.tag===r.tag||o.componentInstance.$destroy(),t[e]=null,g(n,e)}!function(e){e.prototype._init=function(e){var n=this;n._uid=tr++,n._isVue=!0,n.__v_skip=!0,n._scope=new un(!0),n._scope._vm=!0,e&&e._isComponent?function(t,e){var n=t.$options=Object.create(t.constructor.options),r=e._parentVnode;n.parent=e.parent,n._parentVnode=r;var o=r.componentOptions;n.propsData=o.propsData,n._parentListeners=o.listeners,n._renderChildren=o.children,n._componentTag=o.tag,e.render&&(n.render=e.render,n.staticRenderFns=e.staticRenderFns)}(n,e):n.$options=gr(er(n.constructor),e||{},n),n._renderProxy=n,n._self=n,function(t){var e=t.$options,n=e.parent;if(n&&!e.abstract){for(;n.$options.abstract&&n.$parent;)n=n.$parent;n.$children.push(t)}t.$parent=n,t.$root=n?n.$root:t,t.$children=[],t.$refs={},t._provided=n?n._provided:Object.create(null),t._watcher=null,t._inactive=null,t._directInactive=!1,t._isMounted=!1,t._isDestroyed=!1,t._isBeingDestroyed=!1}(n),function(t){t._events=Object.create(null),t._hasHookEvent=!1;var e=t.$options._parentListeners;e&&Me(t,e)}(n),function(e){e._vnode=null,e._staticTrees=null;var n=e.$options,r=e.$vnode=n._parentVnode,o=r&&r.context;e.$slots=ge(n._renderChildren,o),e.$scopedSlots=r?be(e.$parent,r.data.scopedSlots,e.$slots):t,e._c=function(t,n,r,o){return te(e,t,n,r,o,!1)},e.$createElement=function(t,n,r,o){return te(e,t,n,r,o,!0)};var i=r&&r.data;At(e,"$attrs",i&&i.attrs||t,null,!0),At(e,"$listeners",n._parentListeners||t,null,!0)}(n),Be(n,"beforeCreate",void 0,!1),function(t){var e=Qn(t.$options.inject,t);e&&(kt(!1),Object.keys(e).forEach((function(n){At(t,n,e[n])})),kt(!0))}(n),qn(n),function(t){var e=t.$options.provide;if(e){var n=a(e)?e.call(t):e;if(!s(n))return;for(var r=ln(t),o=st?Reflect.ownKeys(n):Object.keys(n),i=0;i<o.length;i++){var c=o[i];Object.defineProperty(r,c,Object.getOwnPropertyDescriptor(n,c))}}}(n),Be(n,"created"),n.$options.el&&n.$mount(n.$options.el)}}(Cr),function(t){var e={get:function(){return this._data}},n={get:function(){return this._props}};Object.defineProperty(t.prototype,"$data",e),Object.defineProperty(t.prototype,"$props",n),t.prototype.$set=jt,t.prototype.$delete=Et,t.prototype.$watch=function(t,e,n){var r=this;if(u(e))return Yn(r,t,e,n);(n=n||{}).user=!0;var o=new Vn(r,t,e,n);if(n.immediate){var i='callback for immediate watcher "'.concat(o.expression,'"');yt(),dn(e,r,[o.value],r,i),_t()}return function(){o.teardown()}}}(Cr),function(t){var n=/^hook:/;t.prototype.$on=function(t,r){var o=this;if(e(t))for(var i=0,a=t.length;i<a;i++)o.$on(t[i],r);else(o._events[t]||(o._events[t]=[])).push(r),n.test(t)&&(o._hasHookEvent=!0);return o},t.prototype.$once=function(t,e){var n=this;function r(){n.$off(t,r),e.apply(n,arguments)}return r.fn=e,n.$on(t,r),n},t.prototype.$off=function(t,n){var r=this;if(!arguments.length)return r._events=Object.create(null),r;if(e(t)){for(var o=0,i=t.length;o<i;o++)r.$off(t[o],n);return r}var a,s=r._events[t];if(!s)return r;if(!n)return r._events[t]=null,r;for(var c=s.length;c--;)if((a=s[c])===n||a.fn===n){s.splice(c,1);break}return r},t.prototype.$emit=function(t){var e=this,n=e._events[t];if(n){n=n.length>1?O(n):n;for(var r=O(arguments,1),o='event handler for "'.concat(t,'"'),i=0,a=n.length;i<a;i++)dn(n[i],e,r,e,o)}return e}}(Cr),function(t){t.prototype._update=function(t,e){var n=this,r=n.$el,o=n._vnode,i=Le(n);n._vnode=t,n.$el=o?n.__patch__(o,t):n.__patch__(n.$el,t,e,!1),i(),r&&(r.__vue__=null),n.$el&&(n.$el.__vue__=n);for(var a=n;a&&a.$vnode&&a.$parent&&a.$vnode===a.$parent._vnode;)a.$parent.$el=a.$el,a=a.$parent},t.prototype.$forceUpdate=function(){this._watcher&&this._watcher.update()},t.prototype.$destroy=function(){var t=this;if(!t._isBeingDestroyed){Be(t,"beforeDestroy"),t._isBeingDestroyed=!0;var e=t.$parent;!e||e._isBeingDestroyed||t.$options.abstract||g(e.$children,t),t._scope.stop(),t._data.__ob__&&t._data.__ob__.vmCount--,t._isDestroyed=!0,t.__patch__(t._vnode,null),Be(t,"destroyed"),t.$off(),t.$el&&(t.$el.__vue__=null),t.$vnode&&(t.$vnode.parent=null)}}}(Cr),function(t){me(t.prototype),t.prototype.$nextTick=function(t){return Cn(t,this)},t.prototype._render=function(){var t,n=this,r=n.$options,o=r.render,i=r._parentVnode;i&&n._isMounted&&(n.$scopedSlots=be(n.$parent,i.data.scopedSlots,n.$slots,n.$scopedSlots),n._slotsProxy&&Se(n._slotsProxy,n.$scopedSlots)),n.$vnode=i;try{ut(n),Ae=n,t=o.call(n._renderProxy,n.$createElement)}catch(e){fn(e,n,"render"),t=n._vnode}finally{Ae=null,ut()}return e(t)&&1===t.length&&(t=t[0]),t instanceof lt||(t=ft()),t.parent=i,t}}(Cr);var jr=[String,RegExp,Array],Er={name:"keep-alive",abstract:!0,props:{include:jr,exclude:jr,max:[String,Number]},methods:{cacheVNode:function(){var t=this,e=t.cache,n=t.keys,r=t.vnodeToCache,o=t.keyToCache;if(r){var i=r.tag,a=r.componentInstance,s=r.componentOptions;e[o]={name:Sr(s),tag:i,componentInstance:a},n.push(o),this.max&&n.length>parseInt(this.max)&&Ar(e,n[0],n,this._vnode),this.vnodeToCache=null}}},created:function(){this.cache=Object.create(null),this.keys=[]},destroyed:function(){for(var t in this.cache)Ar(this.cache,t,this.keys)},mounted:function(){var t=this;this.cacheVNode(),this.$watch("include",(function(e){Tr(t,(function(t){return Or(e,t)}))})),this.$watch("exclude",(function(e){Tr(t,(function(t){return!Or(e,t)}))}))},updated:function(){this.cacheVNode()},render:function(){var t=this.$slots.default,e=Ee(t),n=e&&e.componentOptions;if(n){var r=Sr(n),o=this.include,i=this.exclude;if(o&&(!r||!Or(o,r))||i&&r&&Or(i,r))return e;var a=this.cache,s=this.keys,c=null==e.key?n.Ctor.cid+(n.tag?"::".concat(n.tag):""):e.key;a[c]?(e.componentInstance=a[c].componentInstance,g(s,c),s.push(c)):(this.vnodeToCache=e,this.keyToCache=c),e.data.keepAlive=!0}return e||t&&t[0]}},Nr={KeepAlive:Er};!function(t){var e={get:function(){return H}};Object.defineProperty(t,"config",e),t.util={warn:lr,extend:T,mergeOptions:gr,defineReactive:At},t.set=jt,t.delete=Et,t.nextTick=Cn,t.observable=function(t){return Tt(t),t},t.options=Object.create(null),R.forEach((function(e){t.options[e+"s"]=Object.create(null)})),t.options._base=t,T(t.options.components,Nr),function(t){t.use=function(t){var e=this._installedPlugins||(this._installedPlugins=[]);if(e.indexOf(t)>-1)return this;var n=O(arguments,1);return n.unshift(this),a(t.install)?t.install.apply(t,n):a(t)&&t.apply(null,n),e.push(t),this}}(t),function(t){t.mixin=function(t){return this.options=gr(this.options,t),this}}(t),kr(t),function(t){R.forEach((function(e){t[e]=function(t,n){return n?("component"===e&&u(n)&&(n.name=n.name||t,n=this.options._base.extend(n)),"directive"===e&&a(n)&&(n={bind:n,update:n}),this.options[e+"s"][t]=n,n):this.options[e+"s"][t]}}))}(t)}(Cr),Object.defineProperty(Cr.prototype,"$isServer",{get:rt}),Object.defineProperty(Cr.prototype,"$ssrContext",{get:function(){return this.$vnode&&this.$vnode.ssrContext}}),Object.defineProperty(Cr,"FunctionalRenderContext",{value:nr}),Cr.version=Rn;var Pr=v("style,class"),Dr=v("input,textarea,option,select,progress"),Mr=function(t,e,n){return"value"===n&&Dr(t)&&"button"!==e||"selected"===n&&"option"===t||"checked"===n&&"input"===t||"muted"===n&&"video"===t},Ir=v("contenteditable,draggable,spellcheck"),Lr=v("events,caret,typing,plaintext-only"),Rr=v("allowfullscreen,async,autofocus,autoplay,checked,compact,controls,declare,default,defaultchecked,defaultmuted,defaultselected,defer,disabled,enabled,formnovalidate,hidden,indeterminate,inert,ismap,itemscope,loop,multiple,muted,nohref,noresize,noshade,novalidate,nowrap,open,pauseonexit,readonly,required,reversed,scoped,seamless,selected,sortable,truespeed,typemustmatch,visible"),Fr="http://www.w3.org/1999/xlink",Hr=function(t){return":"===t.charAt(5)&&"xlink"===t.slice(0,5)},Br=function(t){return Hr(t)?t.slice(6,t.length):""},Ur=function(t){return null==t||!1===t};function zr(t){for(var e=t.data,n=t,o=t;r(o.componentInstance);)(o=o.componentInstance._vnode)&&o.data&&(e=Vr(o.data,e));for(;r(n=n.parent);)n&&n.data&&(e=Vr(e,n.data));return function(t,e){if(r(t)||r(e))return Kr(t,Jr(e));return""}(e.staticClass,e.class)}function Vr(t,e){return{staticClass:Kr(t.staticClass,e.staticClass),class:r(t.class)?[t.class,e.class]:e.class}}function Kr(t,e){return t?e?t+" "+e:t:e||""}function Jr(t){return Array.isArray(t)?function(t){for(var e,n="",o=0,i=t.length;o<i;o++)r(e=Jr(t[o]))&&""!==e&&(n&&(n+=" "),n+=e);return n}(t):s(t)?function(t){var e="";for(var n in t)t[n]&&(e&&(e+=" "),e+=n);return e}(t):"string"==typeof t?t:""}var qr={svg:"http://www.w3.org/2000/svg",math:"http://www.w3.org/1998/Math/MathML"},Wr=v("html,body,base,head,link,meta,style,title,address,article,aside,footer,header,h1,h2,h3,h4,h5,h6,hgroup,nav,section,div,dd,dl,dt,figcaption,figure,picture,hr,img,li,main,ol,p,pre,ul,a,b,abbr,bdi,bdo,br,cite,code,data,dfn,em,i,kbd,mark,q,rp,rt,rtc,ruby,s,samp,small,span,strong,sub,sup,time,u,var,wbr,area,audio,map,track,video,embed,object,param,source,canvas,script,noscript,del,ins,caption,col,colgroup,table,thead,tbody,td,th,tr,button,datalist,fieldset,form,input,label,legend,meter,optgroup,option,output,progress,select,textarea,details,dialog,menu,menuitem,summary,content,element,shadow,template,blockquote,iframe,tfoot"),Zr=v("svg,animate,circle,clippath,cursor,defs,desc,ellipse,filter,font-face,foreignobject,g,glyph,image,line,marker,mask,missing-glyph,path,pattern,polygon,polyline,rect,switch,symbol,text,textpath,tspan,use,view",!0),Gr=function(t){return Wr(t)||Zr(t)};function Xr(t){return Zr(t)?"svg":"math"===t?"math":void 0}var Yr=Object.create(null);var Qr=v("text,number,password,search,email,tel,url");function to(t){if("string"==typeof t){var e=document.querySelector(t);return e||document.createElement("div")}return t}var eo=Object.freeze({__proto__:null,createElement:function(t,e){var n=document.createElement(t);return"select"!==t||e.data&&e.data.attrs&&void 0!==e.data.attrs.multiple&&n.setAttribute("multiple","multiple"),n},createElementNS:function(t,e){return document.createElementNS(qr[t],e)},createTextNode:function(t){return document.createTextNode(t)},createComment:function(t){return document.createComment(t)},insertBefore:function(t,e,n){t.insertBefore(e,n)},removeChild:function(t,e){t.removeChild(e)},appendChild:function(t,e){t.appendChild(e)},parentNode:function(t){return t.parentNode},nextSibling:function(t){return t.nextSibling},tagName:function(t){return t.tagName},setTextContent:function(t,e){t.textContent=e},setStyleScope:function(t,e){t.setAttribute(e,"")}}),no={create:function(t,e){ro(e)},update:function(t,e){t.data.ref!==e.data.ref&&(ro(t,!0),ro(e))},destroy:function(t){ro(t,!0)}};function ro(t,n){var o=t.data.ref;if(r(o)){var i=t.context,s=t.componentInstance||t.elm,c=n?null:s,u=n?void 0:s;if(a(o))dn(o,i,[c],i,"template ref function");else{var l=t.data.refInFor,f="string"==typeof o||"number"==typeof o,d=Ft(o),p=i.$refs;if(f||d)if(l){var v=f?p[o]:o.value;n?e(v)&&g(v,s):e(v)?v.includes(s)||v.push(s):f?(p[o]=[s],oo(i,o,p[o])):o.value=[s]}else if(f){if(n&&p[o]!==s)return;p[o]=u,oo(i,o,c)}else if(d){if(n&&o.value!==s)return;o.value=c}}}}function oo(t,e,n){var r=t._setupState;r&&_(r,e)&&(Ft(r[e])?r[e].value=n:r[e]=n)}var io=new lt("",{},[]),ao=["create","activate","update","remove","destroy"];function so(t,e){return t.key===e.key&&t.asyncFactory===e.asyncFactory&&(t.tag===e.tag&&t.isComment===e.isComment&&r(t.data)===r(e.data)&&function(t,e){if("input"!==t.tag)return!0;var n,o=r(n=t.data)&&r(n=n.attrs)&&n.type,i=r(n=e.data)&&r(n=n.attrs)&&n.type;return o===i||Qr(o)&&Qr(i)}(t,e)||o(t.isAsyncPlaceholder)&&n(e.asyncFactory.error))}function co(t,e,n){var o,i,a={};for(o=e;o<=n;++o)r(i=t[o].key)&&(a[i]=o);return a}var uo={create:lo,update:lo,destroy:function(t){lo(t,io)}};function lo(t,e){(t.data.directives||e.data.directives)&&function(t,e){var n,r,o,i=t===io,a=e===io,s=po(t.data.directives,t.context),c=po(e.data.directives,e.context),u=[],l=[];for(n in c)r=s[n],o=c[n],r?(o.oldValue=r.value,o.oldArg=r.arg,ho(o,"update",e,t),o.def&&o.def.componentUpdated&&l.push(o)):(ho(o,"bind",e,t),o.def&&o.def.inserted&&u.push(o));if(u.length){var f=function(){for(var n=0;n<u.length;n++)ho(u[n],"inserted",e,t)};i?Zt(e,"insert",f):f()}l.length&&Zt(e,"postpatch",(function(){for(var n=0;n<l.length;n++)ho(l[n],"componentUpdated",e,t)}));if(!i)for(n in s)c[n]||ho(s[n],"unbind",t,t,a)}(t,e)}var fo=Object.create(null);function po(t,e){var n,r,o=Object.create(null);if(!t)return o;for(n=0;n<t.length;n++){if((r=t[n]).modifiers||(r.modifiers=fo),o[vo(r)]=r,e._setupState&&e._setupState.__sfc){var i=r.def||yr(e,"_setupState","v-"+r.name);r.def="function"==typeof i?{bind:i,update:i}:i}r.def=r.def||yr(e.$options,"directives",r.name)}return o}function vo(t){return t.rawName||"".concat(t.name,".").concat(Object.keys(t.modifiers||{}).join("."))}function ho(t,e,n,r,o){var i=t.def&&t.def[e];if(i)try{i(n.elm,t,n,r,o)}catch(r){fn(r,n.context,"directive ".concat(t.name," ").concat(e," hook"))}}var mo=[no,uo];function go(t,e){var i=e.componentOptions;if(!(r(i)&&!1===i.Ctor.options.inheritAttrs||n(t.data.attrs)&&n(e.data.attrs))){var a,s,c=e.elm,u=t.data.attrs||{},l=e.data.attrs||{};for(a in(r(l.__ob__)||o(l._v_attr_proxy))&&(l=e.data.attrs=T({},l)),l)s=l[a],u[a]!==s&&yo(c,a,s,e.data.pre);for(a in(W||G)&&l.value!==u.value&&yo(c,"value",l.value),u)n(l[a])&&(Hr(a)?c.removeAttributeNS(Fr,Br(a)):Ir(a)||c.removeAttribute(a))}}function yo(t,e,n,r){r||t.tagName.indexOf("-")>-1?_o(t,e,n):Rr(e)?Ur(n)?t.removeAttribute(e):(n="allowfullscreen"===e&&"EMBED"===t.tagName?"true":e,t.setAttribute(e,n)):Ir(e)?t.setAttribute(e,function(t,e){return Ur(e)||"false"===e?"false":"contenteditable"===t&&Lr(e)?e:"true"}(e,n)):Hr(e)?Ur(n)?t.removeAttributeNS(Fr,Br(e)):t.setAttributeNS(Fr,e,n):_o(t,e,n)}function _o(t,e,n){if(Ur(n))t.removeAttribute(e);else{if(W&&!Z&&"TEXTAREA"===t.tagName&&"placeholder"===e&&""!==n&&!t.__ieph){var r=function(e){e.stopImmediatePropagation(),t.removeEventListener("input",r)};t.addEventListener("input",r),t.__ieph=!0}t.setAttribute(e,n)}}var bo={create:go,update:go};function $o(t,e){var o=e.elm,i=e.data,a=t.data;if(!(n(i.staticClass)&&n(i.class)&&(n(a)||n(a.staticClass)&&n(a.class)))){var s=zr(e),c=o._transitionClasses;r(c)&&(s=Kr(s,Jr(c))),s!==o._prevClass&&(o.setAttribute("class",s),o._prevClass=s)}}var wo,xo,Co,ko,So,Oo,To={create:$o,update:$o},Ao=/[\w).+\-_$\]]/;function jo(t){var e,n,r,o,i,a=!1,s=!1,c=!1,u=!1,l=0,f=0,d=0,p=0;for(r=0;r<t.length;r++)if(n=e,e=t.charCodeAt(r),a)39===e&&92!==n&&(a=!1);else if(s)34===e&&92!==n&&(s=!1);else if(c)96===e&&92!==n&&(c=!1);else if(u)47===e&&92!==n&&(u=!1);else if(124!==e||124===t.charCodeAt(r+1)||124===t.charCodeAt(r-1)||l||f||d){switch(e){case 34:s=!0;break;case 39:a=!0;break;case 96:c=!0;break;case 40:d++;break;case 41:d--;break;case 91:f++;break;case 93:f--;break;case 123:l++;break;case 125:l--}if(47===e){for(var v=r-1,h=void 0;v>=0&&" "===(h=t.charAt(v));v--);h&&Ao.test(h)||(u=!0)}}else void 0===o?(p=r+1,o=t.slice(0,r).trim()):m();function m(){(i||(i=[])).push(t.slice(p,r).trim()),p=r+1}if(void 0===o?o=t.slice(0,r).trim():0!==p&&m(),i)for(r=0;r<i.length;r++)o=Eo(o,i[r]);return o}function Eo(t,e){var n=e.indexOf("(");if(n<0)return'_f("'.concat(e,'")(').concat(t,")");var r=e.slice(0,n),o=e.slice(n+1);return'_f("'.concat(r,'")(').concat(t).concat(")"!==o?","+o:o)}function No(t,e){console.error("[Vue compiler]: ".concat(t))}function Po(t,e){return t?t.map((function(t){return t[e]})).filter((function(t){return t})):[]}function Do(t,e,n,r,o){(t.props||(t.props=[])).push(zo({name:e,value:n,dynamic:o},r)),t.plain=!1}function Mo(t,e,n,r,o){(o?t.dynamicAttrs||(t.dynamicAttrs=[]):t.attrs||(t.attrs=[])).push(zo({name:e,value:n,dynamic:o},r)),t.plain=!1}function Io(t,e,n,r){t.attrsMap[e]=n,t.attrsList.push(zo({name:e,value:n},r))}function Lo(t,e,n,r,o,i,a,s){(t.directives||(t.directives=[])).push(zo({name:e,rawName:n,value:r,arg:o,isDynamicArg:i,modifiers:a},s)),t.plain=!1}function Ro(t,e,n){return n?"_p(".concat(e,',"').concat(t,'")'):t+e}function Fo(e,n,r,o,i,a,s,c){var u;(o=o||t).right?c?n="(".concat(n,")==='click'?'contextmenu':(").concat(n,")"):"click"===n&&(n="contextmenu",delete o.right):o.middle&&(c?n="(".concat(n,")==='click'?'mouseup':(").concat(n,")"):"click"===n&&(n="mouseup")),o.capture&&(delete o.capture,n=Ro("!",n,c)),o.once&&(delete o.once,n=Ro("~",n,c)),o.passive&&(delete o.passive,n=Ro("&",n,c)),o.native?(delete o.native,u=e.nativeEvents||(e.nativeEvents={})):u=e.events||(e.events={});var l=zo({value:r.trim(),dynamic:c},s);o!==t&&(l.modifiers=o);var f=u[n];Array.isArray(f)?i?f.unshift(l):f.push(l):u[n]=f?i?[l,f]:[f,l]:l,e.plain=!1}function Ho(t,e,n){var r=Bo(t,":"+e)||Bo(t,"v-bind:"+e);if(null!=r)return jo(r);if(!1!==n){var o=Bo(t,e);if(null!=o)return JSON.stringify(o)}}function Bo(t,e,n){var r;if(null!=(r=t.attrsMap[e]))for(var o=t.attrsList,i=0,a=o.length;i<a;i++)if(o[i].name===e){o.splice(i,1);break}return n&&delete t.attrsMap[e],r}function Uo(t,e){for(var n=t.attrsList,r=0,o=n.length;r<o;r++){var i=n[r];if(e.test(i.name))return n.splice(r,1),i}}function zo(t,e){return e&&(null!=e.start&&(t.start=e.start),null!=e.end&&(t.end=e.end)),t}function Vo(t,e,n){var r=n||{},o=r.number,i="$$v",a=i;r.trim&&(a="(typeof ".concat(i," === 'string'")+"? ".concat(i,".trim()")+": ".concat(i,")")),o&&(a="_n(".concat(a,")"));var s=Ko(e,a);t.model={value:"(".concat(e,")"),expression:JSON.stringify(e),callback:"function (".concat(i,") {").concat(s,"}")}}function Ko(t,e){var n=function(t){if(t=t.trim(),wo=t.length,t.indexOf("[")<0||t.lastIndexOf("]")<wo-1)return(ko=t.lastIndexOf("."))>-1?{exp:t.slice(0,ko),key:'"'+t.slice(ko+1)+'"'}:{exp:t,key:null};xo=t,ko=So=Oo=0;for(;!qo();)Wo(Co=Jo())?Go(Co):91===Co&&Zo(Co);return{exp:t.slice(0,So),key:t.slice(So+1,Oo)}}(t);return null===n.key?"".concat(t,"=").concat(e):"$set(".concat(n.exp,", ").concat(n.key,", ").concat(e,")")}function Jo(){return xo.charCodeAt(++ko)}function qo(){return ko>=wo}function Wo(t){return 34===t||39===t}function Zo(t){var e=1;for(So=ko;!qo();)if(Wo(t=Jo()))Go(t);else if(91===t&&e++,93===t&&e--,0===e){Oo=ko;break}}function Go(t){for(var e=t;!qo()&&(t=Jo())!==e;);}var Xo,Yo="__r";function Qo(t,e,n){var r=Xo;return function o(){var i=e.apply(null,arguments);null!==i&&ni(t,o,n,r)}}var ti=mn&&!(Q&&Number(Q[1])<=53);function ei(t,e,n,r){if(ti){var o=We,i=e;e=i._wrapper=function(t){if(t.target===t.currentTarget||t.timeStamp>=o||t.timeStamp<=0||t.target.ownerDocument!==document)return i.apply(this,arguments)}}Xo.addEventListener(t,e,et?{capture:n,passive:r}:n)}function ni(t,e,n,r){(r||Xo).removeEventListener(t,e._wrapper||e,n)}function ri(t,e){if(!n(t.data.on)||!n(e.data.on)){var o=e.data.on||{},i=t.data.on||{};Xo=e.elm||t.elm,function(t){if(r(t.__r)){var e=W?"change":"input";t[e]=[].concat(t.__r,t[e]||[]),delete t.__r}r(t.__c)&&(t.change=[].concat(t.__c,t.change||[]),delete t.__c)}(o),Wt(o,i,ei,ni,Qo,e.context),Xo=void 0}}var oi,ii={create:ri,update:ri,destroy:function(t){return ri(t,io)}};function ai(t,e){if(!n(t.data.domProps)||!n(e.data.domProps)){var i,a,s=e.elm,c=t.data.domProps||{},u=e.data.domProps||{};for(i in(r(u.__ob__)||o(u._v_attr_proxy))&&(u=e.data.domProps=T({},u)),c)i in u||(s[i]="");for(i in u){if(a=u[i],"textContent"===i||"innerHTML"===i){if(e.children&&(e.children.length=0),a===c[i])continue;1===s.childNodes.length&&s.removeChild(s.childNodes[0])}if("value"===i&&"PROGRESS"!==s.tagName){s._value=a;var l=n(a)?"":String(a);si(s,l)&&(s.value=l)}else if("innerHTML"===i&&Zr(s.tagName)&&n(s.innerHTML)){(oi=oi||document.createElement("div")).innerHTML="<svg>".concat(a,"</svg>");for(var f=oi.firstChild;s.firstChild;)s.removeChild(s.firstChild);for(;f.firstChild;)s.appendChild(f.firstChild)}else if(a!==c[i])try{s[i]=a}catch(t){}}}}function si(t,e){return!t.composing&&("OPTION"===t.tagName||function(t,e){var n=!0;try{n=document.activeElement!==t}catch(t){}return n&&t.value!==e}(t,e)||function(t,e){var n=t.value,o=t._vModifiers;if(r(o)){if(o.number)return p(n)!==p(e);if(o.trim)return n.trim()!==e.trim()}return n!==e}(t,e))}var ci={create:ai,update:ai},ui=b((function(t){var e={},n=/:(.+)/;return t.split(/;(?![^(]*\))/g).forEach((function(t){if(t){var r=t.split(n);r.length>1&&(e[r[0].trim()]=r[1].trim())}})),e}));function li(t){var e=fi(t.style);return t.staticStyle?T(t.staticStyle,e):e}function fi(t){return Array.isArray(t)?A(t):"string"==typeof t?ui(t):t}var di,pi=/^--/,vi=/\s*!important$/,hi=function(t,e,n){if(pi.test(e))t.style.setProperty(e,n);else if(vi.test(n))t.style.setProperty(k(e),n.replace(vi,""),"important");else{var r=gi(e);if(Array.isArray(n))for(var o=0,i=n.length;o<i;o++)t.style[r]=n[o];else t.style[r]=n}},mi=["Webkit","Moz","ms"],gi=b((function(t){if(di=di||document.createElement("div").style,"filter"!==(t=w(t))&&t in di)return t;for(var e=t.charAt(0).toUpperCase()+t.slice(1),n=0;n<mi.length;n++){var r=mi[n]+e;if(r in di)return r}}));function yi(t,e){var o=e.data,i=t.data;if(!(n(o.staticStyle)&&n(o.style)&&n(i.staticStyle)&&n(i.style))){var a,s,c=e.elm,u=i.staticStyle,l=i.normalizedStyle||i.style||{},f=u||l,d=fi(e.data.style)||{};e.data.normalizedStyle=r(d.__ob__)?T({},d):d;var p=function(t,e){var n,r={};if(e)for(var o=t;o.componentInstance;)(o=o.componentInstance._vnode)&&o.data&&(n=li(o.data))&&T(r,n);(n=li(t.data))&&T(r,n);for(var i=t;i=i.parent;)i.data&&(n=li(i.data))&&T(r,n);return r}(e,!0);for(s in f)n(p[s])&&hi(c,s,"");for(s in p)(a=p[s])!==f[s]&&hi(c,s,null==a?"":a)}}var _i={create:yi,update:yi},bi=/\s+/;function $i(t,e){if(e&&(e=e.trim()))if(t.classList)e.indexOf(" ")>-1?e.split(bi).forEach((function(e){return t.classList.add(e)})):t.classList.add(e);else{var n=" ".concat(t.getAttribute("class")||""," ");n.indexOf(" "+e+" ")<0&&t.setAttribute("class",(n+e).trim())}}function wi(t,e){if(e&&(e=e.trim()))if(t.classList)e.indexOf(" ")>-1?e.split(bi).forEach((function(e){return t.classList.remove(e)})):t.classList.remove(e),t.classList.length||t.removeAttribute("class");else{for(var n=" ".concat(t.getAttribute("class")||""," "),r=" "+e+" ";n.indexOf(r)>=0;)n=n.replace(r," ");(n=n.trim())?t.setAttribute("class",n):t.removeAttribute("class")}}function xi(t){if(t){if("object"==typeof t){var e={};return!1!==t.css&&T(e,Ci(t.name||"v")),T(e,t),e}return"string"==typeof t?Ci(t):void 0}}var Ci=b((function(t){return{enterClass:"".concat(t,"-enter"),enterToClass:"".concat(t,"-enter-to"),enterActiveClass:"".concat(t,"-enter-active"),leaveClass:"".concat(t,"-leave"),leaveToClass:"".concat(t,"-leave-to"),leaveActiveClass:"".concat(t,"-leave-active")}})),ki=J&&!Z,Si="transition",Oi="animation",Ti="transition",Ai="transitionend",ji="animation",Ei="animationend";ki&&(void 0===window.ontransitionend&&void 0!==window.onwebkittransitionend&&(Ti="WebkitTransition",Ai="webkitTransitionEnd"),void 0===window.onanimationend&&void 0!==window.onwebkitanimationend&&(ji="WebkitAnimation",Ei="webkitAnimationEnd"));var Ni=J?window.requestAnimationFrame?window.requestAnimationFrame.bind(window):setTimeout:function(t){return t()};function Pi(t){Ni((function(){Ni(t)}))}function Di(t,e){var n=t._transitionClasses||(t._transitionClasses=[]);n.indexOf(e)<0&&(n.push(e),$i(t,e))}function Mi(t,e){t._transitionClasses&&g(t._transitionClasses,e),wi(t,e)}function Ii(t,e,n){var r=Ri(t,e),o=r.type,i=r.timeout,a=r.propCount;if(!o)return n();var s=o===Si?Ai:Ei,c=0,u=function(){t.removeEventListener(s,l),n()},l=function(e){e.target===t&&++c>=a&&u()};setTimeout((function(){c<a&&u()}),i+1),t.addEventListener(s,l)}var Li=/\b(transform|all)(,|$)/;function Ri(t,e){var n,r=window.getComputedStyle(t),o=(r[Ti+"Delay"]||"").split(", "),i=(r[Ti+"Duration"]||"").split(", "),a=Fi(o,i),s=(r[ji+"Delay"]||"").split(", "),c=(r[ji+"Duration"]||"").split(", "),u=Fi(s,c),l=0,f=0;return e===Si?a>0&&(n=Si,l=a,f=i.length):e===Oi?u>0&&(n=Oi,l=u,f=c.length):f=(n=(l=Math.max(a,u))>0?a>u?Si:Oi:null)?n===Si?i.length:c.length:0,{type:n,timeout:l,propCount:f,hasTransform:n===Si&&Li.test(r[Ti+"Property"])}}function Fi(t,e){for(;t.length<e.length;)t=t.concat(t);return Math.max.apply(null,e.map((function(e,n){return Hi(e)+Hi(t[n])})))}function Hi(t){return 1e3*Number(t.slice(0,-1).replace(",","."))}function Bi(t,e){var o=t.elm;r(o._leaveCb)&&(o._leaveCb.cancelled=!0,o._leaveCb());var i=xi(t.data.transition);if(!n(i)&&!r(o._enterCb)&&1===o.nodeType){for(var c=i.css,u=i.type,l=i.enterClass,f=i.enterToClass,d=i.enterActiveClass,v=i.appearClass,h=i.appearToClass,m=i.appearActiveClass,g=i.beforeEnter,y=i.enter,_=i.afterEnter,b=i.enterCancelled,$=i.beforeAppear,w=i.appear,x=i.afterAppear,C=i.appearCancelled,k=i.duration,S=Ie,O=Ie.$vnode;O&&O.parent;)S=O.context,O=O.parent;var T=!S._isMounted||!t.isRootInsert;if(!T||w||""===w){var A=T&&v?v:l,j=T&&m?m:d,E=T&&h?h:f,N=T&&$||g,P=T&&a(w)?w:y,D=T&&x||_,I=T&&C||b,L=p(s(k)?k.enter:k),R=!1!==c&&!Z,F=Vi(P),H=o._enterCb=M((function(){R&&(Mi(o,E),Mi(o,j)),H.cancelled?(R&&Mi(o,A),I&&I(o)):D&&D(o),o._enterCb=null}));t.data.show||Zt(t,"insert",(function(){var e=o.parentNode,n=e&&e._pending&&e._pending[t.key];n&&n.tag===t.tag&&n.elm._leaveCb&&n.elm._leaveCb(),P&&P(o,H)})),N&&N(o),R&&(Di(o,A),Di(o,j),Pi((function(){Mi(o,A),H.cancelled||(Di(o,E),F||(zi(L)?setTimeout(H,L):Ii(o,u,H)))}))),t.data.show&&(e&&e(),P&&P(o,H)),R||F||H()}}}function Ui(t,e){var o=t.elm;r(o._enterCb)&&(o._enterCb.cancelled=!0,o._enterCb());var i=xi(t.data.transition);if(n(i)||1!==o.nodeType)return e();if(!r(o._leaveCb)){var a=i.css,c=i.type,u=i.leaveClass,l=i.leaveToClass,f=i.leaveActiveClass,d=i.beforeLeave,v=i.leave,h=i.afterLeave,m=i.leaveCancelled,g=i.delayLeave,y=i.duration,_=!1!==a&&!Z,b=Vi(v),$=p(s(y)?y.leave:y),w=o._leaveCb=M((function(){o.parentNode&&o.parentNode._pending&&(o.parentNode._pending[t.key]=null),_&&(Mi(o,l),Mi(o,f)),w.cancelled?(_&&Mi(o,u),m&&m(o)):(e(),h&&h(o)),o._leaveCb=null}));g?g(x):x()}function x(){w.cancelled||(!t.data.show&&o.parentNode&&((o.parentNode._pending||(o.parentNode._pending={}))[t.key]=t),d&&d(o),_&&(Di(o,u),Di(o,f),Pi((function(){Mi(o,u),w.cancelled||(Di(o,l),b||(zi($)?setTimeout(w,$):Ii(o,c,w)))}))),v&&v(o,w),_||b||w())}}function zi(t){return"number"==typeof t&&!isNaN(t)}function Vi(t){if(n(t))return!1;var e=t.fns;return r(e)?Vi(Array.isArray(e)?e[0]:e):(t._length||t.length)>1}function Ki(t,e){!0!==e.data.show&&Bi(e)}var Ji=function(t){var a,s,c={},u=t.modules,l=t.nodeOps;for(a=0;a<ao.length;++a)for(c[ao[a]]=[],s=0;s<u.length;++s)r(u[s][ao[a]])&&c[ao[a]].push(u[s][ao[a]]);function f(t){var e=l.parentNode(t);r(e)&&l.removeChild(e,t)}function d(t,e,n,i,a,s,u){if(r(t.elm)&&r(s)&&(t=s[u]=pt(t)),t.isRootInsert=!a,!function(t,e,n,i){var a=t.data;if(r(a)){var s=r(t.componentInstance)&&a.keepAlive;if(r(a=a.hook)&&r(a=a.init)&&a(t,!1),r(t.componentInstance))return p(t,e),h(n,t.elm,i),o(s)&&function(t,e,n,o){var i,a=t;for(;a.componentInstance;)if(r(i=(a=a.componentInstance._vnode).data)&&r(i=i.transition)){for(i=0;i<c.activate.length;++i)c.activate[i](io,a);e.push(a);break}h(n,t.elm,o)}(t,e,n,i),!0}}(t,e,n,i)){var f=t.data,d=t.children,v=t.tag;r(v)?(t.elm=t.ns?l.createElementNS(t.ns,v):l.createElement(v,t),_(t),m(t,d,e),r(f)&&y(t,e),h(n,t.elm,i)):o(t.isComment)?(t.elm=l.createComment(t.text),h(n,t.elm,i)):(t.elm=l.createTextNode(t.text),h(n,t.elm,i))}}function p(t,e){r(t.data.pendingInsert)&&(e.push.apply(e,t.data.pendingInsert),t.data.pendingInsert=null),t.elm=t.componentInstance.$el,g(t)?(y(t,e),_(t)):(ro(t),e.push(t))}function h(t,e,n){r(t)&&(r(n)?l.parentNode(n)===t&&l.insertBefore(t,e,n):l.appendChild(t,e))}function m(t,n,r){if(e(n))for(var o=0;o<n.length;++o)d(n[o],r,t.elm,null,!0,n,o);else i(t.text)&&l.appendChild(t.elm,l.createTextNode(String(t.text)))}function g(t){for(;t.componentInstance;)t=t.componentInstance._vnode;return r(t.tag)}function y(t,e){for(var n=0;n<c.create.length;++n)c.create[n](io,t);r(a=t.data.hook)&&(r(a.create)&&a.create(io,t),r(a.insert)&&e.push(t))}function _(t){var e;if(r(e=t.fnScopeId))l.setStyleScope(t.elm,e);else for(var n=t;n;)r(e=n.context)&&r(e=e.$options._scopeId)&&l.setStyleScope(t.elm,e),n=n.parent;r(e=Ie)&&e!==t.context&&e!==t.fnContext&&r(e=e.$options._scopeId)&&l.setStyleScope(t.elm,e)}function b(t,e,n,r,o,i){for(;r<=o;++r)d(n[r],i,t,e,!1,n,r)}function $(t){var e,n,o=t.data;if(r(o))for(r(e=o.hook)&&r(e=e.destroy)&&e(t),e=0;e<c.destroy.length;++e)c.destroy[e](t);if(r(e=t.children))for(n=0;n<t.children.length;++n)$(t.children[n])}function w(t,e,n){for(;e<=n;++e){var o=t[e];r(o)&&(r(o.tag)?(x(o),$(o)):f(o.elm))}}function x(t,e){if(r(e)||r(t.data)){var n,o=c.remove.length+1;for(r(e)?e.listeners+=o:e=function(t,e){function n(){0==--n.listeners&&f(t)}return n.listeners=e,n}(t.elm,o),r(n=t.componentInstance)&&r(n=n._vnode)&&r(n.data)&&x(n,e),n=0;n<c.remove.length;++n)c.remove[n](t,e);r(n=t.data.hook)&&r(n=n.remove)?n(t,e):e()}else f(t.elm)}function C(t,e,n,o){for(var i=n;i<o;i++){var a=e[i];if(r(a)&&so(t,a))return i}}function k(t,e,i,a,s,u){if(t!==e){r(e.elm)&&r(a)&&(e=a[s]=pt(e));var f=e.elm=t.elm;if(o(t.isAsyncPlaceholder))r(e.asyncFactory.resolved)?T(t.elm,e,i):e.isAsyncPlaceholder=!0;else if(o(e.isStatic)&&o(t.isStatic)&&e.key===t.key&&(o(e.isCloned)||o(e.isOnce)))e.componentInstance=t.componentInstance;else{var p,v=e.data;r(v)&&r(p=v.hook)&&r(p=p.prepatch)&&p(t,e);var h=t.children,m=e.children;if(r(v)&&g(e)){for(p=0;p<c.update.length;++p)c.update[p](t,e);r(p=v.hook)&&r(p=p.update)&&p(t,e)}n(e.text)?r(h)&&r(m)?h!==m&&function(t,e,o,i,a){for(var s,c,u,f=0,p=0,v=e.length-1,h=e[0],m=e[v],g=o.length-1,y=o[0],_=o[g],$=!a;f<=v&&p<=g;)n(h)?h=e[++f]:n(m)?m=e[--v]:so(h,y)?(k(h,y,i,o,p),h=e[++f],y=o[++p]):so(m,_)?(k(m,_,i,o,g),m=e[--v],_=o[--g]):so(h,_)?(k(h,_,i,o,g),$&&l.insertBefore(t,h.elm,l.nextSibling(m.elm)),h=e[++f],_=o[--g]):so(m,y)?(k(m,y,i,o,p),$&&l.insertBefore(t,m.elm,h.elm),m=e[--v],y=o[++p]):(n(s)&&(s=co(e,f,v)),n(c=r(y.key)?s[y.key]:C(y,e,f,v))?d(y,i,t,h.elm,!1,o,p):so(u=e[c],y)?(k(u,y,i,o,p),e[c]=void 0,$&&l.insertBefore(t,u.elm,h.elm)):d(y,i,t,h.elm,!1,o,p),y=o[++p]);f>v?b(t,n(o[g+1])?null:o[g+1].elm,o,p,g,i):p>g&&w(e,f,v)}(f,h,m,i,u):r(m)?(r(t.text)&&l.setTextContent(f,""),b(f,null,m,0,m.length-1,i)):r(h)?w(h,0,h.length-1):r(t.text)&&l.setTextContent(f,""):t.text!==e.text&&l.setTextContent(f,e.text),r(v)&&r(p=v.hook)&&r(p=p.postpatch)&&p(t,e)}}}function S(t,e,n){if(o(n)&&r(t.parent))t.parent.data.pendingInsert=e;else for(var i=0;i<e.length;++i)e[i].data.hook.insert(e[i])}var O=v("attrs,class,staticClass,staticStyle,key");function T(t,e,n,i){var a,s=e.tag,c=e.data,u=e.children;if(i=i||c&&c.pre,e.elm=t,o(e.isComment)&&r(e.asyncFactory))return e.isAsyncPlaceholder=!0,!0;if(r(c)&&(r(a=c.hook)&&r(a=a.init)&&a(e,!0),r(a=e.componentInstance)))return p(e,n),!0;if(r(s)){if(r(u))if(t.hasChildNodes())if(r(a=c)&&r(a=a.domProps)&&r(a=a.innerHTML)){if(a!==t.innerHTML)return!1}else{for(var l=!0,f=t.firstChild,d=0;d<u.length;d++){if(!f||!T(f,u[d],n,i)){l=!1;break}f=f.nextSibling}if(!l||f)return!1}else m(e,u,n);if(r(c)){var v=!1;for(var h in c)if(!O(h)){v=!0,y(e,n);break}!v&&c.class&&Bn(c.class)}}else t.data!==e.text&&(t.data=e.text);return!0}return function(t,e,i,a){if(!n(e)){var s,u=!1,f=[];if(n(t))u=!0,d(e,f);else{var p=r(t.nodeType);if(!p&&so(t,e))k(t,e,f,null,null,a);else{if(p){if(1===t.nodeType&&t.hasAttribute(L)&&(t.removeAttribute(L),i=!0),o(i)&&T(t,e,f))return S(e,f,!0),t;s=t,t=new lt(l.tagName(s).toLowerCase(),{},[],void 0,s)}var v=t.elm,h=l.parentNode(v);if(d(e,f,v._leaveCb?null:h,l.nextSibling(v)),r(e.parent))for(var m=e.parent,y=g(e);m;){for(var _=0;_<c.destroy.length;++_)c.destroy[_](m);if(m.elm=e.elm,y){for(var b=0;b<c.create.length;++b)c.create[b](io,m);var x=m.data.hook.insert;if(x.merged)for(var C=1;C<x.fns.length;C++)x.fns[C]()}else ro(m);m=m.parent}r(h)?w([t],0,0):r(t.tag)&&$(t)}}return S(e,f,u),e.elm}r(t)&&$(t)}}({nodeOps:eo,modules:[bo,To,ii,ci,_i,J?{create:Ki,activate:Ki,remove:function(t,e){!0!==t.data.show?Ui(t,e):e()}}:{}].concat(mo)});Z&&document.addEventListener("selectionchange",(function(){var t=document.activeElement;t&&t.vmodel&&ta(t,"input")}));var qi={inserted:function(t,e,n,r){"select"===n.tag?(r.elm&&!r.elm._vOptions?Zt(n,"postpatch",(function(){qi.componentUpdated(t,e,n)})):Wi(t,e,n.context),t._vOptions=[].map.call(t.options,Xi)):("textarea"===n.tag||Qr(t.type))&&(t._vModifiers=e.modifiers,e.modifiers.lazy||(t.addEventListener("compositionstart",Yi),t.addEventListener("compositionend",Qi),t.addEventListener("change",Qi),Z&&(t.vmodel=!0)))},componentUpdated:function(t,e,n){if("select"===n.tag){Wi(t,e,n.context);var r=t._vOptions,o=t._vOptions=[].map.call(t.options,Xi);if(o.some((function(t,e){return!P(t,r[e])})))(t.multiple?e.value.some((function(t){return Gi(t,o)})):e.value!==e.oldValue&&Gi(e.value,o))&&ta(t,"change")}}};function Wi(t,e,n){Zi(t,e),(W||G)&&setTimeout((function(){Zi(t,e)}),0)}function Zi(t,e,n){var r=e.value,o=t.multiple;if(!o||Array.isArray(r)){for(var i,a,s=0,c=t.options.length;s<c;s++)if(a=t.options[s],o)i=D(r,Xi(a))>-1,a.selected!==i&&(a.selected=i);else if(P(Xi(a),r))return void(t.selectedIndex!==s&&(t.selectedIndex=s));o||(t.selectedIndex=-1)}}function Gi(t,e){return e.every((function(e){return!P(e,t)}))}function Xi(t){return"_value"in t?t._value:t.value}function Yi(t){t.target.composing=!0}function Qi(t){t.target.composing&&(t.target.composing=!1,ta(t.target,"input"))}function ta(t,e){var n=document.createEvent("HTMLEvents");n.initEvent(e,!0,!0),t.dispatchEvent(n)}function ea(t){return!t.componentInstance||t.data&&t.data.transition?t:ea(t.componentInstance._vnode)}var na={bind:function(t,e,n){var r=e.value,o=(n=ea(n)).data&&n.data.transition,i=t.__vOriginalDisplay="none"===t.style.display?"":t.style.display;r&&o?(n.data.show=!0,Bi(n,(function(){t.style.display=i}))):t.style.display=r?i:"none"},update:function(t,e,n){var r=e.value;!r!=!e.oldValue&&((n=ea(n)).data&&n.data.transition?(n.data.show=!0,r?Bi(n,(function(){t.style.display=t.__vOriginalDisplay})):Ui(n,(function(){t.style.display="none"}))):t.style.display=r?t.__vOriginalDisplay:"none")},unbind:function(t,e,n,r,o){o||(t.style.display=t.__vOriginalDisplay)}},ra={model:qi,show:na},oa={name:String,appear:Boolean,css:Boolean,mode:String,type:String,enterClass:String,leaveClass:String,enterToClass:String,leaveToClass:String,enterActiveClass:String,leaveActiveClass:String,appearClass:String,appearActiveClass:String,appearToClass:String,duration:[Number,String,Object]};function ia(t){var e=t&&t.componentOptions;return e&&e.Ctor.options.abstract?ia(Ee(e.children)):t}function aa(t){var e={},n=t.$options;for(var r in n.propsData)e[r]=t[r];var o=n._parentListeners;for(var r in o)e[w(r)]=o[r];return e}function sa(t,e){if(/\d-keep-alive$/.test(e.tag))return t("keep-alive",{props:e.componentOptions.propsData})}var ca=function(t){return t.tag||_e(t)},ua=function(t){return"show"===t.name},la={name:"transition",props:oa,abstract:!0,render:function(t){var e=this,n=this.$slots.default;if(n&&(n=n.filter(ca)).length){var r=this.mode,o=n[0];if(function(t){for(;t=t.parent;)if(t.data.transition)return!0}(this.$vnode))return o;var a=ia(o);if(!a)return o;if(this._leaving)return sa(t,o);var s="__transition-".concat(this._uid,"-");a.key=null==a.key?a.isComment?s+"comment":s+a.tag:i(a.key)?0===String(a.key).indexOf(s)?a.key:s+a.key:a.key;var c=(a.data||(a.data={})).transition=aa(this),u=this._vnode,l=ia(u);if(a.data.directives&&a.data.directives.some(ua)&&(a.data.show=!0),l&&l.data&&!function(t,e){return e.key===t.key&&e.tag===t.tag}(a,l)&&!_e(l)&&(!l.componentInstance||!l.componentInstance._vnode.isComment)){var f=l.data.transition=T({},c);if("out-in"===r)return this._leaving=!0,Zt(f,"afterLeave",(function(){e._leaving=!1,e.$forceUpdate()})),sa(t,o);if("in-out"===r){if(_e(a))return u;var d,p=function(){d()};Zt(c,"afterEnter",p),Zt(c,"enterCancelled",p),Zt(f,"delayLeave",(function(t){d=t}))}}return o}}},fa=T({tag:String,moveClass:String},oa);delete fa.mode;var da={props:fa,beforeMount:function(){var t=this,e=this._update;this._update=function(n,r){var o=Le(t);t.__patch__(t._vnode,t.kept,!1,!0),t._vnode=t.kept,o(),e.call(t,n,r)}},render:function(t){for(var e=this.tag||this.$vnode.data.tag||"span",n=Object.create(null),r=this.prevChildren=this.children,o=this.$slots.default||[],i=this.children=[],a=aa(this),s=0;s<o.length;s++){(l=o[s]).tag&&null!=l.key&&0!==String(l.key).indexOf("__vlist")&&(i.push(l),n[l.key]=l,(l.data||(l.data={})).transition=a)}if(r){var c=[],u=[];for(s=0;s<r.length;s++){var l;(l=r[s]).data.transition=a,l.data.pos=l.elm.getBoundingClientRect(),n[l.key]?c.push(l):u.push(l)}this.kept=t(e,null,c),this.removed=u}return t(e,null,i)},updated:function(){var t=this.prevChildren,e=this.moveClass||(this.name||"v")+"-move";t.length&&this.hasMove(t[0].elm,e)&&(t.forEach(pa),t.forEach(va),t.forEach(ha),this._reflow=document.body.offsetHeight,t.forEach((function(t){if(t.data.moved){var n=t.elm,r=n.style;Di(n,e),r.transform=r.WebkitTransform=r.transitionDuration="",n.addEventListener(Ai,n._moveCb=function t(r){r&&r.target!==n||r&&!/transform$/.test(r.propertyName)||(n.removeEventListener(Ai,t),n._moveCb=null,Mi(n,e))})}})))},methods:{hasMove:function(t,e){if(!ki)return!1;if(this._hasMove)return this._hasMove;var n=t.cloneNode();t._transitionClasses&&t._transitionClasses.forEach((function(t){wi(n,t)})),$i(n,e),n.style.display="none",this.$el.appendChild(n);var r=Ri(n);return this.$el.removeChild(n),this._hasMove=r.hasTransform}}};function pa(t){t.elm._moveCb&&t.elm._moveCb(),t.elm._enterCb&&t.elm._enterCb()}function va(t){t.data.newPos=t.elm.getBoundingClientRect()}function ha(t){var e=t.data.pos,n=t.data.newPos,r=e.left-n.left,o=e.top-n.top;if(r||o){t.data.moved=!0;var i=t.elm.style;i.transform=i.WebkitTransform="translate(".concat(r,"px,").concat(o,"px)"),i.transitionDuration="0s"}}var ma={Transition:la,TransitionGroup:da};Cr.config.mustUseProp=Mr,Cr.config.isReservedTag=Gr,Cr.config.isReservedAttr=Pr,Cr.config.getTagNamespace=Xr,Cr.config.isUnknownElement=function(t){if(!J)return!0;if(Gr(t))return!1;if(t=t.toLowerCase(),null!=Yr[t])return Yr[t];var e=document.createElement(t);return t.indexOf("-")>-1?Yr[t]=e.constructor===window.HTMLUnknownElement||e.constructor===window.HTMLElement:Yr[t]=/HTMLUnknownElement/.test(e.toString())},T(Cr.options.directives,ra),T(Cr.options.components,ma),Cr.prototype.__patch__=J?Ji:j,Cr.prototype.$mount=function(t,e){return function(t,e,n){var r;t.$el=e,t.$options.render||(t.$options.render=ft),Be(t,"beforeMount"),r=function(){t._update(t._render(),n)},new Vn(t,r,j,{before:function(){t._isMounted&&!t._isDestroyed&&Be(t,"beforeUpdate")}},!0),n=!1;var o=t._preWatchers;if(o)for(var i=0;i<o.length;i++)o[i].run();return null==t.$vnode&&(t._isMounted=!0,Be(t,"mounted")),t}(this,t=t&&J?to(t):void 0,e)},J&&setTimeout((function(){H.devtools&&ot&&ot.emit("init",Cr)}),0);var ga=/\{\{((?:.|\r?\n)+?)\}\}/g,ya=/[-.*+?^${}()|[\]\/\\]/g,_a=b((function(t){var e=t[0].replace(ya,"\\$&"),n=t[1].replace(ya,"\\$&");return new RegExp(e+"((?:.|\\n)+?)"+n,"g")}));var ba={staticKeys:["staticClass"],transformNode:function(t,e){e.warn;var n=Bo(t,"class");n&&(t.staticClass=JSON.stringify(n.replace(/\s+/g," ").trim()));var r=Ho(t,"class",!1);r&&(t.classBinding=r)},genData:function(t){var e="";return t.staticClass&&(e+="staticClass:".concat(t.staticClass,",")),t.classBinding&&(e+="class:".concat(t.classBinding,",")),e}};var $a,wa={staticKeys:["staticStyle"],transformNode:function(t,e){e.warn;var n=Bo(t,"style");n&&(t.staticStyle=JSON.stringify(ui(n)));var r=Ho(t,"style",!1);r&&(t.styleBinding=r)},genData:function(t){var e="";return t.staticStyle&&(e+="staticStyle:".concat(t.staticStyle,",")),t.styleBinding&&(e+="style:(".concat(t.styleBinding,"),")),e}},xa=function(t){return($a=$a||document.createElement("div")).innerHTML=t,$a.textContent},Ca=v("area,base,br,col,embed,frame,hr,img,input,isindex,keygen,link,meta,param,source,track,wbr"),ka=v("colgroup,dd,dt,li,options,p,td,tfoot,th,thead,tr,source"),Sa=v("address,article,aside,base,blockquote,body,caption,col,colgroup,dd,details,dialog,div,dl,dt,fieldset,figcaption,figure,footer,form,h1,h2,h3,h4,h5,h6,head,header,hgroup,hr,html,legend,li,menuitem,meta,optgroup,option,param,rp,rt,source,style,summary,tbody,td,tfoot,th,thead,title,tr,track"),Oa=/^\s*([^\s"'<>\/=]+)(?:\s*(=)\s*(?:"([^"]*)"+|'([^']*)'+|([^\s"'=<>`]+)))?/,Ta=/^\s*((?:v-[\w-]+:|@|:|#)\[[^=]+?\][^\s"'<>\/=]*)(?:\s*(=)\s*(?:"([^"]*)"+|'([^']*)'+|([^\s"'=<>`]+)))?/,Aa="[a-zA-Z_][\\-\\.0-9_a-zA-Z".concat(B.source,"]*"),ja="((?:".concat(Aa,"\\:)?").concat(Aa,")"),Ea=new RegExp("^<".concat(ja)),Na=/^\s*(\/?)>/,Pa=new RegExp("^<\\/".concat(ja,"[^>]*>")),Da=/^<!DOCTYPE [^>]+>/i,Ma=/^<!\--/,Ia=/^<!\[/,La=v("script,style,textarea",!0),Ra={},Fa={"&lt;":"<","&gt;":">","&quot;":'"',"&amp;":"&","&#10;":"\n","&#9;":"\t","&#39;":"'"},Ha=/&(?:lt|gt|quot|amp|#39);/g,Ba=/&(?:lt|gt|quot|amp|#39|#10|#9);/g,Ua=v("pre,textarea",!0),za=function(t,e){return t&&Ua(t)&&"\n"===e[0]};function Va(t,e){var n=e?Ba:Ha;return t.replace(n,(function(t){return Fa[t]}))}function Ka(t,e){for(var n,r,o=[],i=e.expectHTML,a=e.isUnaryTag||E,s=e.canBeLeftOpenTag||E,c=0,u=function(){if(n=t,r&&La(r)){var u=0,d=r.toLowerCase(),p=Ra[d]||(Ra[d]=new RegExp("([\\s\\S]*?)(</"+d+"[^>]*>)","i"));w=t.replace(p,(function(t,n,r){return u=r.length,La(d)||"noscript"===d||(n=n.replace(/<!\--([\s\S]*?)-->/g,"$1").replace(/<!\[CDATA\[([\s\S]*?)]]>/g,"$1")),za(d,n)&&(n=n.slice(1)),e.chars&&e.chars(n),""}));c+=t.length-w.length,t=w,f(d,c-u,c)}else{var v=t.indexOf("<");if(0===v){if(Ma.test(t)){var h=t.indexOf("--\x3e");if(h>=0)return e.shouldKeepComment&&e.comment&&e.comment(t.substring(4,h),c,c+h+3),l(h+3),"continue"}if(Ia.test(t)){var m=t.indexOf("]>");if(m>=0)return l(m+2),"continue"}var g=t.match(Da);if(g)return l(g[0].length),"continue";var y=t.match(Pa);if(y){var _=c;return l(y[0].length),f(y[1],_,c),"continue"}var b=function(){var e=t.match(Ea);if(e){var n={tagName:e[1],attrs:[],start:c};l(e[0].length);for(var r=void 0,o=void 0;!(r=t.match(Na))&&(o=t.match(Ta)||t.match(Oa));)o.start=c,l(o[0].length),o.end=c,n.attrs.push(o);if(r)return n.unarySlash=r[1],l(r[0].length),n.end=c,n}}();if(b)return function(t){var n=t.tagName,c=t.unarySlash;i&&("p"===r&&Sa(n)&&f(r),s(n)&&r===n&&f(n));for(var u=a(n)||!!c,l=t.attrs.length,d=new Array(l),p=0;p<l;p++){var v=t.attrs[p],h=v[3]||v[4]||v[5]||"",m="a"===n&&"href"===v[1]?e.shouldDecodeNewlinesForHref:e.shouldDecodeNewlines;d[p]={name:v[1],value:Va(h,m)}}u||(o.push({tag:n,lowerCasedTag:n.toLowerCase(),attrs:d,start:t.start,end:t.end}),r=n);e.start&&e.start(n,d,u,t.start,t.end)}(b),za(b.tagName,t)&&l(1),"continue"}var $=void 0,w=void 0,x=void 0;if(v>=0){for(w=t.slice(v);!(Pa.test(w)||Ea.test(w)||Ma.test(w)||Ia.test(w)||(x=w.indexOf("<",1))<0);)v+=x,w=t.slice(v);$=t.substring(0,v)}v<0&&($=t),$&&l($.length),e.chars&&$&&e.chars($,c-$.length,c)}if(t===n)return e.chars&&e.chars(t),"break"};t;){if("break"===u())break}function l(e){c+=e,t=t.substring(e)}function f(t,n,i){var a,s;if(null==n&&(n=c),null==i&&(i=c),t)for(s=t.toLowerCase(),a=o.length-1;a>=0&&o[a].lowerCasedTag!==s;a--);else a=0;if(a>=0){for(var u=o.length-1;u>=a;u--)e.end&&e.end(o[u].tag,n,i);o.length=a,r=a&&o[a-1].tag}else"br"===s?e.start&&e.start(t,[],!0,n,i):"p"===s&&(e.start&&e.start(t,[],!1,n,i),e.end&&e.end(t,n,i))}f()}var Ja,qa,Wa,Za,Ga,Xa,Ya,Qa,ts=/^@|^v-on:/,es=/^v-|^@|^:|^#/,ns=/([\s\S]*?)\s+(?:in|of)\s+([\s\S]*)/,rs=/,([^,\}\]]*)(?:,([^,\}\]]*))?$/,os=/^\(|\)$/g,is=/^\[.*\]$/,as=/:(.*)$/,ss=/^:|^\.|^v-bind:/,cs=/\.[^.\]]+(?=[^\]]*$)/g,us=/^v-slot(:|$)|^#/,ls=/[\r\n]/,fs=/[ \f\t\r\n]+/g,ds=b(xa),ps="_empty_";function vs(t,e,n){return{type:1,tag:t,attrsList:e,attrsMap:$s(e),rawAttrsMap:{},parent:n,children:[]}}function hs(t,e){Ja=e.warn||No,Xa=e.isPreTag||E,Ya=e.mustUseProp||E,Qa=e.getTagNamespace||E,e.isReservedTag,Wa=Po(e.modules,"transformNode"),Za=Po(e.modules,"preTransformNode"),Ga=Po(e.modules,"postTransformNode"),qa=e.delimiters;var n,r,o=[],i=!1!==e.preserveWhitespace,a=e.whitespace,s=!1,c=!1;function u(t){if(l(t),s||t.processed||(t=ms(t,e)),o.length||t===n||n.if&&(t.elseif||t.else)&&ys(n,{exp:t.elseif,block:t}),r&&!t.forbidden)if(t.elseif||t.else)a=t,u=function(t){for(var e=t.length;e--;){if(1===t[e].type)return t[e];t.pop()}}(r.children),u&&u.if&&ys(u,{exp:a.elseif,block:a});else{if(t.slotScope){var i=t.slotTarget||'"default"';(r.scopedSlots||(r.scopedSlots={}))[i]=t}r.children.push(t),t.parent=r}var a,u;t.children=t.children.filter((function(t){return!t.slotScope})),l(t),t.pre&&(s=!1),Xa(t.tag)&&(c=!1);for(var f=0;f<Ga.length;f++)Ga[f](t,e)}function l(t){if(!c)for(var e=void 0;(e=t.children[t.children.length-1])&&3===e.type&&" "===e.text;)t.children.pop()}return Ka(t,{warn:Ja,expectHTML:e.expectHTML,isUnaryTag:e.isUnaryTag,canBeLeftOpenTag:e.canBeLeftOpenTag,shouldDecodeNewlines:e.shouldDecodeNewlines,shouldDecodeNewlinesForHref:e.shouldDecodeNewlinesForHref,shouldKeepComment:e.comments,outputSourceRange:e.outputSourceRange,start:function(t,i,a,l,f){var d=r&&r.ns||Qa(t);W&&"svg"===d&&(i=function(t){for(var e=[],n=0;n<t.length;n++){var r=t[n];ws.test(r.name)||(r.name=r.name.replace(xs,""),e.push(r))}return e}(i));var p,v=vs(t,i,r);d&&(v.ns=d),"style"!==(p=v).tag&&("script"!==p.tag||p.attrsMap.type&&"text/javascript"!==p.attrsMap.type)||rt()||(v.forbidden=!0);for(var h=0;h<Za.length;h++)v=Za[h](v,e)||v;s||(!function(t){null!=Bo(t,"v-pre")&&(t.pre=!0)}(v),v.pre&&(s=!0)),Xa(v.tag)&&(c=!0),s?function(t){var e=t.attrsList,n=e.length;if(n)for(var r=t.attrs=new Array(n),o=0;o<n;o++)r[o]={name:e[o].name,value:JSON.stringify(e[o].value)},null!=e[o].start&&(r[o].start=e[o].start,r[o].end=e[o].end);else t.pre||(t.plain=!0)}(v):v.processed||(gs(v),function(t){var e=Bo(t,"v-if");if(e)t.if=e,ys(t,{exp:e,block:t});else{null!=Bo(t,"v-else")&&(t.else=!0);var n=Bo(t,"v-else-if");n&&(t.elseif=n)}}(v),function(t){null!=Bo(t,"v-once")&&(t.once=!0)}(v)),n||(n=v),a?u(v):(r=v,o.push(v))},end:function(t,e,n){var i=o[o.length-1];o.length-=1,r=o[o.length-1],u(i)},chars:function(t,e,n){if(r&&(!W||"textarea"!==r.tag||r.attrsMap.placeholder!==t)){var o,u=r.children;if(t=c||t.trim()?"script"===(o=r).tag||"style"===o.tag?t:ds(t):u.length?a?"condense"===a&&ls.test(t)?"":" ":i?" ":"":""){c||"condense"!==a||(t=t.replace(fs," "));var l=void 0,f=void 0;!s&&" "!==t&&(l=function(t,e){var n=e?_a(e):ga;if(n.test(t)){for(var r,o,i,a=[],s=[],c=n.lastIndex=0;r=n.exec(t);){(o=r.index)>c&&(s.push(i=t.slice(c,o)),a.push(JSON.stringify(i)));var u=jo(r[1].trim());a.push("_s(".concat(u,")")),s.push({"@binding":u}),c=o+r[0].length}return c<t.length&&(s.push(i=t.slice(c)),a.push(JSON.stringify(i))),{expression:a.join("+"),tokens:s}}}(t,qa))?f={type:2,expression:l.expression,tokens:l.tokens,text:t}:" "===t&&u.length&&" "===u[u.length-1].text||(f={type:3,text:t}),f&&u.push(f)}}},comment:function(t,e,n){if(r){var o={type:3,text:t,isComment:!0};r.children.push(o)}}}),n}function ms(t,e){var n,r;(r=Ho(n=t,"key"))&&(n.key=r),t.plain=!t.key&&!t.scopedSlots&&!t.attrsList.length,function(t){var e=Ho(t,"ref");e&&(t.ref=e,t.refInFor=function(t){var e=t;for(;e;){if(void 0!==e.for)return!0;e=e.parent}return!1}(t))}(t),function(t){var e;"template"===t.tag?(e=Bo(t,"scope"),t.slotScope=e||Bo(t,"slot-scope")):(e=Bo(t,"slot-scope"))&&(t.slotScope=e);var n=Ho(t,"slot");n&&(t.slotTarget='""'===n?'"default"':n,t.slotTargetDynamic=!(!t.attrsMap[":slot"]&&!t.attrsMap["v-bind:slot"]),"template"===t.tag||t.slotScope||Mo(t,"slot",n,function(t,e){return t.rawAttrsMap[":"+e]||t.rawAttrsMap["v-bind:"+e]||t.rawAttrsMap[e]}(t,"slot")));if("template"===t.tag){if(a=Uo(t,us)){var r=_s(a),o=r.name,i=r.dynamic;t.slotTarget=o,t.slotTargetDynamic=i,t.slotScope=a.value||ps}}else{var a;if(a=Uo(t,us)){var s=t.scopedSlots||(t.scopedSlots={}),c=_s(a),u=c.name,l=(i=c.dynamic,s[u]=vs("template",[],t));l.slotTarget=u,l.slotTargetDynamic=i,l.children=t.children.filter((function(t){if(!t.slotScope)return t.parent=l,!0})),l.slotScope=a.value||ps,t.children=[],t.plain=!1}}}(t),function(t){"slot"===t.tag&&(t.slotName=Ho(t,"name"))}(t),function(t){var e;(e=Ho(t,"is"))&&(t.component=e);null!=Bo(t,"inline-template")&&(t.inlineTemplate=!0)}(t);for(var o=0;o<Wa.length;o++)t=Wa[o](t,e)||t;return function(t){var e,n,r,o,i,a,s,c,u=t.attrsList;for(e=0,n=u.length;e<n;e++)if(r=o=u[e].name,i=u[e].value,es.test(r))if(t.hasBindings=!0,(a=bs(r.replace(es,"")))&&(r=r.replace(cs,"")),ss.test(r))r=r.replace(ss,""),i=jo(i),(c=is.test(r))&&(r=r.slice(1,-1)),a&&(a.prop&&!c&&"innerHtml"===(r=w(r))&&(r="innerHTML"),a.camel&&!c&&(r=w(r)),a.sync&&(s=Ko(i,"$event"),c?Fo(t,'"update:"+('.concat(r,")"),s,null,!1,0,u[e],!0):(Fo(t,"update:".concat(w(r)),s,null,!1,0,u[e]),k(r)!==w(r)&&Fo(t,"update:".concat(k(r)),s,null,!1,0,u[e])))),a&&a.prop||!t.component&&Ya(t.tag,t.attrsMap.type,r)?Do(t,r,i,u[e],c):Mo(t,r,i,u[e],c);else if(ts.test(r))r=r.replace(ts,""),(c=is.test(r))&&(r=r.slice(1,-1)),Fo(t,r,i,a,!1,0,u[e],c);else{var l=(r=r.replace(es,"")).match(as),f=l&&l[1];c=!1,f&&(r=r.slice(0,-(f.length+1)),is.test(f)&&(f=f.slice(1,-1),c=!0)),Lo(t,r,o,i,f,c,a,u[e])}else Mo(t,r,JSON.stringify(i),u[e]),!t.component&&"muted"===r&&Ya(t.tag,t.attrsMap.type,r)&&Do(t,r,"true",u[e])}(t),t}function gs(t){var e;if(e=Bo(t,"v-for")){var n=function(t){var e=t.match(ns);if(!e)return;var n={};n.for=e[2].trim();var r=e[1].trim().replace(os,""),o=r.match(rs);o?(n.alias=r.replace(rs,"").trim(),n.iterator1=o[1].trim(),o[2]&&(n.iterator2=o[2].trim())):n.alias=r;return n}(e);n&&T(t,n)}}function ys(t,e){t.ifConditions||(t.ifConditions=[]),t.ifConditions.push(e)}function _s(t){var e=t.name.replace(us,"");return e||"#"!==t.name[0]&&(e="default"),is.test(e)?{name:e.slice(1,-1),dynamic:!0}:{name:'"'.concat(e,'"'),dynamic:!1}}function bs(t){var e=t.match(cs);if(e){var n={};return e.forEach((function(t){n[t.slice(1)]=!0})),n}}function $s(t){for(var e={},n=0,r=t.length;n<r;n++)e[t[n].name]=t[n].value;return e}var ws=/^xmlns:NS\d+/,xs=/^NS\d+:/;function Cs(t){return vs(t.tag,t.attrsList.slice(),t.parent)}var ks=[ba,wa,{preTransformNode:function(t,e){if("input"===t.tag){var n=t.attrsMap;if(!n["v-model"])return;var r=void 0;if((n[":type"]||n["v-bind:type"])&&(r=Ho(t,"type")),n.type||r||!n["v-bind"]||(r="(".concat(n["v-bind"],").type")),r){var o=Bo(t,"v-if",!0),i=o?"&&(".concat(o,")"):"",a=null!=Bo(t,"v-else",!0),s=Bo(t,"v-else-if",!0),c=Cs(t);gs(c),Io(c,"type","checkbox"),ms(c,e),c.processed=!0,c.if="(".concat(r,")==='checkbox'")+i,ys(c,{exp:c.if,block:c});var u=Cs(t);Bo(u,"v-for",!0),Io(u,"type","radio"),ms(u,e),ys(c,{exp:"(".concat(r,")==='radio'")+i,block:u});var l=Cs(t);return Bo(l,"v-for",!0),Io(l,":type",r),ms(l,e),ys(c,{exp:o,block:l}),a?c.else=!0:s&&(c.elseif=s),c}}}}];var Ss,Os,Ts={model:function(t,e,n){var r=e.value,o=e.modifiers,i=t.tag,a=t.attrsMap.type;if(t.component)return Vo(t,r,o),!1;if("select"===i)!function(t,e,n){var r=n&&n.number,o='Array.prototype.filter.call($event.target.options,function(o){return o.selected}).map(function(o){var val = "_value" in o ? o._value : o.value;'+"return ".concat(r?"_n(val)":"val","})"),i="$event.target.multiple ? $$selectedVal : $$selectedVal[0]",a="var $$selectedVal = ".concat(o,";");a="".concat(a," ").concat(Ko(e,i)),Fo(t,"change",a,null,!0)}(t,r,o);else if("input"===i&&"checkbox"===a)!function(t,e,n){var r=n&&n.number,o=Ho(t,"value")||"null",i=Ho(t,"true-value")||"true",a=Ho(t,"false-value")||"false";Do(t,"checked","Array.isArray(".concat(e,")")+"?_i(".concat(e,",").concat(o,")>-1")+("true"===i?":(".concat(e,")"):":_q(".concat(e,",").concat(i,")"))),Fo(t,"change","var $$a=".concat(e,",")+"$$el=$event.target,"+"$$c=$$el.checked?(".concat(i,"):(").concat(a,");")+"if(Array.isArray($$a)){"+"var $$v=".concat(r?"_n("+o+")":o,",")+"$$i=_i($$a,$$v);"+"if($$el.checked){$$i<0&&(".concat(Ko(e,"$$a.concat([$$v])"),")}")+"else{$$i>-1&&(".concat(Ko(e,"$$a.slice(0,$$i).concat($$a.slice($$i+1))"),")}")+"}else{".concat(Ko(e,"$$c"),"}"),null,!0)}(t,r,o);else if("input"===i&&"radio"===a)!function(t,e,n){var r=n&&n.number,o=Ho(t,"value")||"null";o=r?"_n(".concat(o,")"):o,Do(t,"checked","_q(".concat(e,",").concat(o,")")),Fo(t,"change",Ko(e,o),null,!0)}(t,r,o);else if("input"===i||"textarea"===i)!function(t,e,n){var r=t.attrsMap.type,o=n||{},i=o.lazy,a=o.number,s=o.trim,c=!i&&"range"!==r,u=i?"change":"range"===r?Yo:"input",l="$event.target.value";s&&(l="$event.target.value.trim()");a&&(l="_n(".concat(l,")"));var f=Ko(e,l);c&&(f="if($event.target.composing)return;".concat(f));Do(t,"value","(".concat(e,")")),Fo(t,u,f,null,!0),(s||a)&&Fo(t,"blur","$forceUpdate()")}(t,r,o);else if(!H.isReservedTag(i))return Vo(t,r,o),!1;return!0},text:function(t,e){e.value&&Do(t,"textContent","_s(".concat(e.value,")"),e)},html:function(t,e){e.value&&Do(t,"innerHTML","_s(".concat(e.value,")"),e)}},As={expectHTML:!0,modules:ks,directives:Ts,isPreTag:function(t){return"pre"===t},isUnaryTag:Ca,mustUseProp:Mr,canBeLeftOpenTag:ka,isReservedTag:Gr,getTagNamespace:Xr,staticKeys:function(t){return t.reduce((function(t,e){return t.concat(e.staticKeys||[])}),[]).join(",")}(ks)},js=b((function(t){return v("type,tag,attrsList,attrsMap,plain,parent,children,attrs,start,end,rawAttrsMap"+(t?","+t:""))}));function Es(t,e){t&&(Ss=js(e.staticKeys||""),Os=e.isReservedTag||E,Ns(t),Ps(t,!1))}function Ns(t){if(t.static=function(t){if(2===t.type)return!1;if(3===t.type)return!0;return!(!t.pre&&(t.hasBindings||t.if||t.for||h(t.tag)||!Os(t.tag)||function(t){for(;t.parent;){if("template"!==(t=t.parent).tag)return!1;if(t.for)return!0}return!1}(t)||!Object.keys(t).every(Ss)))}(t),1===t.type){if(!Os(t.tag)&&"slot"!==t.tag&&null==t.attrsMap["inline-template"])return;for(var e=0,n=t.children.length;e<n;e++){var r=t.children[e];Ns(r),r.static||(t.static=!1)}if(t.ifConditions)for(e=1,n=t.ifConditions.length;e<n;e++){var o=t.ifConditions[e].block;Ns(o),o.static||(t.static=!1)}}}function Ps(t,e){if(1===t.type){if((t.static||t.once)&&(t.staticInFor=e),t.static&&t.children.length&&(1!==t.children.length||3!==t.children[0].type))return void(t.staticRoot=!0);if(t.staticRoot=!1,t.children)for(var n=0,r=t.children.length;n<r;n++)Ps(t.children[n],e||!!t.for);if(t.ifConditions)for(n=1,r=t.ifConditions.length;n<r;n++)Ps(t.ifConditions[n].block,e)}}var Ds=/^([\w$_]+|\([^)]*?\))\s*=>|^function(?:\s+[\w$]+)?\s*\(/,Ms=/\([^)]*?\);*$/,Is=/^[A-Za-z_$][\w$]*(?:\.[A-Za-z_$][\w$]*|\['[^']*?']|\["[^"]*?"]|\[\d+]|\[[A-Za-z_$][\w$]*])*$/,Ls={esc:27,tab:9,enter:13,space:32,up:38,left:37,right:39,down:40,delete:[8,46]},Rs={esc:["Esc","Escape"],tab:"Tab",enter:"Enter",space:[" ","Spacebar"],up:["Up","ArrowUp"],left:["Left","ArrowLeft"],right:["Right","ArrowRight"],down:["Down","ArrowDown"],delete:["Backspace","Delete","Del"]},Fs=function(t){return"if(".concat(t,")return null;")},Hs={stop:"$event.stopPropagation();",prevent:"$event.preventDefault();",self:Fs("$event.target !== $event.currentTarget"),ctrl:Fs("!$event.ctrlKey"),shift:Fs("!$event.shiftKey"),alt:Fs("!$event.altKey"),meta:Fs("!$event.metaKey"),left:Fs("'button' in $event && $event.button !== 0"),middle:Fs("'button' in $event && $event.button !== 1"),right:Fs("'button' in $event && $event.button !== 2")};function Bs(t,e){var n=e?"nativeOn:":"on:",r="",o="";for(var i in t){var a=Us(t[i]);t[i]&&t[i].dynamic?o+="".concat(i,",").concat(a,","):r+='"'.concat(i,'":').concat(a,",")}return r="{".concat(r.slice(0,-1),"}"),o?n+"_d(".concat(r,",[").concat(o.slice(0,-1),"])"):n+r}function Us(t){if(!t)return"function(){}";if(Array.isArray(t))return"[".concat(t.map((function(t){return Us(t)})).join(","),"]");var e=Is.test(t.value),n=Ds.test(t.value),r=Is.test(t.value.replace(Ms,""));if(t.modifiers){var o="",i="",a=[],s=function(e){if(Hs[e])i+=Hs[e],Ls[e]&&a.push(e);else if("exact"===e){var n=t.modifiers;i+=Fs(["ctrl","shift","alt","meta"].filter((function(t){return!n[t]})).map((function(t){return"$event.".concat(t,"Key")})).join("||"))}else a.push(e)};for(var c in t.modifiers)s(c);a.length&&(o+=function(t){return"if(!$event.type.indexOf('key')&&"+"".concat(t.map(zs).join("&&"),")return null;")}(a)),i&&(o+=i);var u=e?"return ".concat(t.value,".apply(null, arguments)"):n?"return (".concat(t.value,").apply(null, arguments)"):r?"return ".concat(t.value):t.value;return"function($event){".concat(o).concat(u,"}")}return e||n?t.value:"function($event){".concat(r?"return ".concat(t.value):t.value,"}")}function zs(t){var e=parseInt(t,10);if(e)return"$event.keyCode!==".concat(e);var n=Ls[t],r=Rs[t];return"_k($event.keyCode,"+"".concat(JSON.stringify(t),",")+"".concat(JSON.stringify(n),",")+"$event.key,"+"".concat(JSON.stringify(r))+")"}var Vs={on:function(t,e){t.wrapListeners=function(t){return"_g(".concat(t,",").concat(e.value,")")}},bind:function(t,e){t.wrapData=function(n){return"_b(".concat(n,",'").concat(t.tag,"',").concat(e.value,",").concat(e.modifiers&&e.modifiers.prop?"true":"false").concat(e.modifiers&&e.modifiers.sync?",true":"",")")}},cloak:j},Ks=function(t){this.options=t,this.warn=t.warn||No,this.transforms=Po(t.modules,"transformCode"),this.dataGenFns=Po(t.modules,"genData"),this.directives=T(T({},Vs),t.directives);var e=t.isReservedTag||E;this.maybeComponent=function(t){return!!t.component||!e(t.tag)},this.onceId=0,this.staticRenderFns=[],this.pre=!1};function Js(t,e){var n=new Ks(e),r=t?"script"===t.tag?"null":qs(t,n):'_c("div")';return{render:"with(this){return ".concat(r,"}"),staticRenderFns:n.staticRenderFns}}function qs(t,e){if(t.parent&&(t.pre=t.pre||t.parent.pre),t.staticRoot&&!t.staticProcessed)return Ws(t,e);if(t.once&&!t.onceProcessed)return Zs(t,e);if(t.for&&!t.forProcessed)return Ys(t,e);if(t.if&&!t.ifProcessed)return Gs(t,e);if("template"!==t.tag||t.slotTarget||e.pre){if("slot"===t.tag)return function(t,e){var n=t.slotName||'"default"',r=nc(t,e),o="_t(".concat(n).concat(r?",function(){return ".concat(r,"}"):""),i=t.attrs||t.dynamicAttrs?ic((t.attrs||[]).concat(t.dynamicAttrs||[]).map((function(t){return{name:w(t.name),value:t.value,dynamic:t.dynamic}}))):null,a=t.attrsMap["v-bind"];!i&&!a||r||(o+=",null");i&&(o+=",".concat(i));a&&(o+="".concat(i?"":",null",",").concat(a));return o+")"}(t,e);var n=void 0;if(t.component)n=function(t,e,n){var r=e.inlineTemplate?null:nc(e,n,!0);return"_c(".concat(t,",").concat(Qs(e,n)).concat(r?",".concat(r):"",")")}(t.component,t,e);else{var r=void 0,o=e.maybeComponent(t);(!t.plain||t.pre&&o)&&(r=Qs(t,e));var i=void 0,a=e.options.bindings;o&&a&&!1!==a.__isScriptSetup&&(i=function(t,e){var n=w(e),r=x(n),o=function(o){return t[e]===o?e:t[n]===o?n:t[r]===o?r:void 0},i=o("setup-const")||o("setup-reactive-const");if(i)return i;var a=o("setup-let")||o("setup-ref")||o("setup-maybe-ref");if(a)return a}(a,t.tag)),i||(i="'".concat(t.tag,"'"));var s=t.inlineTemplate?null:nc(t,e,!0);n="_c(".concat(i).concat(r?",".concat(r):"").concat(s?",".concat(s):"",")")}for(var c=0;c<e.transforms.length;c++)n=e.transforms[c](t,n);return n}return nc(t,e)||"void 0"}function Ws(t,e){t.staticProcessed=!0;var n=e.pre;return t.pre&&(e.pre=t.pre),e.staticRenderFns.push("with(this){return ".concat(qs(t,e),"}")),e.pre=n,"_m(".concat(e.staticRenderFns.length-1).concat(t.staticInFor?",true":"",")")}function Zs(t,e){if(t.onceProcessed=!0,t.if&&!t.ifProcessed)return Gs(t,e);if(t.staticInFor){for(var n="",r=t.parent;r;){if(r.for){n=r.key;break}r=r.parent}return n?"_o(".concat(qs(t,e),",").concat(e.onceId++,",").concat(n,")"):qs(t,e)}return Ws(t,e)}function Gs(t,e,n,r){return t.ifProcessed=!0,Xs(t.ifConditions.slice(),e,n,r)}function Xs(t,e,n,r){if(!t.length)return r||"_e()";var o=t.shift();return o.exp?"(".concat(o.exp,")?").concat(i(o.block),":").concat(Xs(t,e,n,r)):"".concat(i(o.block));function i(t){return n?n(t,e):t.once?Zs(t,e):qs(t,e)}}function Ys(t,e,n,r){var o=t.for,i=t.alias,a=t.iterator1?",".concat(t.iterator1):"",s=t.iterator2?",".concat(t.iterator2):"";return t.forProcessed=!0,"".concat(r||"_l","((").concat(o,"),")+"function(".concat(i).concat(a).concat(s,"){")+"return ".concat((n||qs)(t,e))+"})"}function Qs(t,e){var n="{",r=function(t,e){var n=t.directives;if(!n)return;var r,o,i,a,s="directives:[",c=!1;for(r=0,o=n.length;r<o;r++){i=n[r],a=!0;var u=e.directives[i.name];u&&(a=!!u(t,i,e.warn)),a&&(c=!0,s+='{name:"'.concat(i.name,'",rawName:"').concat(i.rawName,'"').concat(i.value?",value:(".concat(i.value,"),expression:").concat(JSON.stringify(i.value)):"").concat(i.arg?",arg:".concat(i.isDynamicArg?i.arg:'"'.concat(i.arg,'"')):"").concat(i.modifiers?",modifiers:".concat(JSON.stringify(i.modifiers)):"","},"))}if(c)return s.slice(0,-1)+"]"}(t,e);r&&(n+=r+","),t.key&&(n+="key:".concat(t.key,",")),t.ref&&(n+="ref:".concat(t.ref,",")),t.refInFor&&(n+="refInFor:true,"),t.pre&&(n+="pre:true,"),t.component&&(n+='tag:"'.concat(t.tag,'",'));for(var o=0;o<e.dataGenFns.length;o++)n+=e.dataGenFns[o](t);if(t.attrs&&(n+="attrs:".concat(ic(t.attrs),",")),t.props&&(n+="domProps:".concat(ic(t.props),",")),t.events&&(n+="".concat(Bs(t.events,!1),",")),t.nativeEvents&&(n+="".concat(Bs(t.nativeEvents,!0),",")),t.slotTarget&&!t.slotScope&&(n+="slot:".concat(t.slotTarget,",")),t.scopedSlots&&(n+="".concat(function(t,e,n){var r=t.for||Object.keys(e).some((function(t){var n=e[t];return n.slotTargetDynamic||n.if||n.for||tc(n)})),o=!!t.if;if(!r)for(var i=t.parent;i;){if(i.slotScope&&i.slotScope!==ps||i.for){r=!0;break}i.if&&(o=!0),i=i.parent}var a=Object.keys(e).map((function(t){return ec(e[t],n)})).join(",");return"scopedSlots:_u([".concat(a,"]").concat(r?",null,true":"").concat(!r&&o?",null,false,".concat(function(t){var e=5381,n=t.length;for(;n;)e=33*e^t.charCodeAt(--n);return e>>>0}(a)):"",")")}(t,t.scopedSlots,e),",")),t.model&&(n+="model:{value:".concat(t.model.value,",callback:").concat(t.model.callback,",expression:").concat(t.model.expression,"},")),t.inlineTemplate){var i=function(t,e){var n=t.children[0];if(n&&1===n.type){var r=Js(n,e.options);return"inlineTemplate:{render:function(){".concat(r.render,"},staticRenderFns:[").concat(r.staticRenderFns.map((function(t){return"function(){".concat(t,"}")})).join(","),"]}")}}(t,e);i&&(n+="".concat(i,","))}return n=n.replace(/,$/,"")+"}",t.dynamicAttrs&&(n="_b(".concat(n,',"').concat(t.tag,'",').concat(ic(t.dynamicAttrs),")")),t.wrapData&&(n=t.wrapData(n)),t.wrapListeners&&(n=t.wrapListeners(n)),n}function tc(t){return 1===t.type&&("slot"===t.tag||t.children.some(tc))}function ec(t,e){var n=t.attrsMap["slot-scope"];if(t.if&&!t.ifProcessed&&!n)return Gs(t,e,ec,"null");if(t.for&&!t.forProcessed)return Ys(t,e,ec);var r=t.slotScope===ps?"":String(t.slotScope),o="function(".concat(r,"){")+"return ".concat("template"===t.tag?t.if&&n?"(".concat(t.if,")?").concat(nc(t,e)||"undefined",":undefined"):nc(t,e)||"undefined":qs(t,e),"}"),i=r?"":",proxy:true";return"{key:".concat(t.slotTarget||'"default"',",fn:").concat(o).concat(i,"}")}function nc(t,e,n,r,o){var i=t.children;if(i.length){var a=i[0];if(1===i.length&&a.for&&"template"!==a.tag&&"slot"!==a.tag){var s=n?e.maybeComponent(a)?",1":",0":"";return"".concat((r||qs)(a,e)).concat(s)}var c=n?function(t,e){for(var n=0,r=0;r<t.length;r++){var o=t[r];if(1===o.type){if(rc(o)||o.ifConditions&&o.ifConditions.some((function(t){return rc(t.block)}))){n=2;break}(e(o)||o.ifConditions&&o.ifConditions.some((function(t){return e(t.block)})))&&(n=1)}}return n}(i,e.maybeComponent):0,u=o||oc;return"[".concat(i.map((function(t){return u(t,e)})).join(","),"]").concat(c?",".concat(c):"")}}function rc(t){return void 0!==t.for||"template"===t.tag||"slot"===t.tag}function oc(t,e){return 1===t.type?qs(t,e):3===t.type&&t.isComment?function(t){return"_e(".concat(JSON.stringify(t.text),")")}(t):function(t){return"_v(".concat(2===t.type?t.expression:ac(JSON.stringify(t.text)),")")}(t)}function ic(t){for(var e="",n="",r=0;r<t.length;r++){var o=t[r],i=ac(o.value);o.dynamic?n+="".concat(o.name,",").concat(i,","):e+='"'.concat(o.name,'":').concat(i,",")}return e="{".concat(e.slice(0,-1),"}"),n?"_d(".concat(e,",[").concat(n.slice(0,-1),"])"):e}function ac(t){return t.replace(/\u2028/g,"\\u2028").replace(/\u2029/g,"\\u2029")}function sc(t,e){try{return new Function(t)}catch(n){return e.push({err:n,code:t}),j}}function cc(t){var e=Object.create(null);return function(n,r,o){(r=T({},r)).warn,delete r.warn;var i=r.delimiters?String(r.delimiters)+n:n;if(e[i])return e[i];var a=t(n,r),s={},c=[];return s.render=sc(a.render,c),s.staticRenderFns=a.staticRenderFns.map((function(t){return sc(t,c)})),e[i]=s}}new RegExp("\\b"+"do,if,for,let,new,try,var,case,else,with,await,break,catch,class,const,super,throw,while,yield,delete,export,import,return,switch,default,extends,finally,continue,debugger,function,arguments".split(",").join("\\b|\\b")+"\\b"),new RegExp("\\b"+"delete,typeof,void".split(",").join("\\s*\\([^\\)]*\\)|\\b")+"\\s*\\([^\\)]*\\)");var uc,lc,fc=(uc=function(t,e){var n=hs(t.trim(),e);!1!==e.optimize&&Es(n,e);var r=Js(n,e);return{ast:n,render:r.render,staticRenderFns:r.staticRenderFns}},function(t){function e(e,n){var r=Object.create(t),o=[],i=[];if(n)for(var a in n.modules&&(r.modules=(t.modules||[]).concat(n.modules)),n.directives&&(r.directives=T(Object.create(t.directives||null),n.directives)),n)"modules"!==a&&"directives"!==a&&(r[a]=n[a]);r.warn=function(t,e,n){(n?i:o).push(t)};var s=uc(e.trim(),r);return s.errors=o,s.tips=i,s}return{compile:e,compileToFunctions:cc(e)}}),dc=fc(As).compileToFunctions;function pc(t){return(lc=lc||document.createElement("div")).innerHTML=t?'<a href="\n"/>':'<div a="\n"/>',lc.innerHTML.indexOf("&#10;")>0}var vc=!!J&&pc(!1),hc=!!J&&pc(!0),mc=b((function(t){var e=to(t);return e&&e.innerHTML})),gc=Cr.prototype.$mount;return Cr.prototype.$mount=function(t,e){if((t=t&&to(t))===document.body||t===document.documentElement)return this;var n=this.$options;if(!n.render){var r=n.template;if(r)if("string"==typeof r)"#"===r.charAt(0)&&(r=mc(r));else{if(!r.nodeType)return this;r=r.innerHTML}else t&&(r=function(t){if(t.outerHTML)return t.outerHTML;var e=document.createElement("div");return e.appendChild(t.cloneNode(!0)),e.innerHTML}(t));if(r){var o=dc(r,{outputSourceRange:!1,shouldDecodeNewlines:vc,shouldDecodeNewlinesForHref:hc,delimiters:n.delimiters,comments:n.comments},this),i=o.render,a=o.staticRenderFns;n.render=i,n.staticRenderFns=a}}return gc.call(this,t,e)},Cr.compile=dc,T(Cr,Fn),Cr.effect=function(t,e){var n=new Vn(ct,t,j,{sync:!0});e&&(n.update=function(){e((function(){return n.run()}))})},Cr}));
  </script>
  <title>__TITLE__</title>
  <style>
    .row {
      display: flex;
      flex-wrap: wrap;
    }

    .column {
      flex: 1;
      padding: 10px;
    }

    .table-header {
      font-weight: bold;
      border-bottom: 1px solid black;
    }

    /* 更换卡片背景色 */
    .table-row {
      border-bottom: 1px solid lightgray;
    }

    .table-cell {
      padding: 5px;
    }
  </style>
</head>
<!-- 更换整体页面背景色 -->

<body style="padding: 0 200px;background-color: #f5f5f5;">
  <div id="app">
    <h1 style="padding-left: 20px;font-size: 40px;">文章目录</h1>

    <ul>
      <li v-if="num == 1 || num == 0" v-for="(i,index) in contentList" :key="index"><a :href="`#${i.primary_col.header}`">{{ i.primary_col.header ? i.primary_col.header : '' }}</a></li>
      <li v-if="num == 2 " v-for="(i,index) in contentList" :key="index"><a :href="`#${i.secondary_rol.header}`">{{i.secondary_rol.header ?i.secondary_rol.header : '' }}</a></li </ul>
      <!-- 按钮的样式调整 -->
      <button style="cursor: pointer;cursor: pointer;height: 30px;" @click="showStatus">{{ text }}</button>
      <!-- border-radius调整圆角弧度 box-shadow调整阴影-->
      <div class="row table-row" v-for="(i,index) in contentList" :key="index" style="border-radius: 10px;background-color: rgb(255, 255, 255);margin: 40px 0px;padding: 20px 40px;position: relative;box-shadow: 0px 0px 15px -8px;">
        <div class="column table-cell" v-if="num == 1 || num == 0">
          <div class="markdown-body">
            <h1 :id="i.primary_col.header">{{ i.primary_col.header }}</h1>
            <div v-html="i.primary_col.msg"></div>
          </div>
        </div>
        <div class="column table-cell" v-if="num == 2 || num == 0">
          <div class="markdown-body">
            <h1 :id="i.secondary_rol.header">{{ i.secondary_rol.header }}</h1>
            <div v-html="i.secondary_rol.msg"></div>
          </div>
        </div>
      </div>
  </div>
</body>

<script>
  new Vue({
    el: '#app',
    data() {
      return {
        // 添加内容的数组，示例请看数组底部
        contentList: [
          
            {
                primary_col: {
                    header: String.raw``,
                    msg: String.raw`<div class="markdown-body"></div>`,
                },
                secondary_rol: {
                    header: String.raw``,
                    msg: String.raw`<div class="markdown-body"></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`一、论文概况`,
                    msg: String.raw`<div class="markdown-body"><p>一、论文概况</p></div>`,
                },
                secondary_rol: {
                    header: String.raw``,
                    msg: String.raw`<div class="markdown-body"></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`Abstract`,
                    msg: String.raw`<div class="markdown-body"><p>Abstract</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`标题：使用经过策划的输入设计低风险蜜罐的生成预训练变换模型`,
                    msg: String.raw`<div class="markdown-body"><p>作者：Jarrod Ragsdale; Rajendra V Boppana; L Spitzner; I Mokube; M Adams; M F Razali; M N Razali; F Z Mans</p>
<p>摘要：蜜罐被用作监控环境中的防御工具，以吸引攻击者并收集用于指标牺牲的证据。然而，一旦部署了这些蜜罐，它们很少更新，使其随着时间的推移变得过时且容易被指纹识别。此外，将完全功能的计算和网络设备用作蜜罐存在着攻击者从受控环境中突破的风险。大规模文本生成模型，通常称为大语言模型（LLMs），已广泛应用生成预训练变换（GPT）模型。这些模型已经广受欢迎，并针对各种用例进行了调整。本文研究了使用这些模型模拟蜜罐，这些蜜罐能够适应威胁的参与而无意间突破的风险。研究发现，这些模型生成输出的方法存在限制，在长时间会话中可能会向专注的攻击者揭示欺骗行为。为了克服这一挑战，本文提出了一种管理输入和输出的方法，以减少模型生成文本的非确定性输出和令牌使用，以模拟终端的方式。通过对一个示例蜜罐与传统低风险蜜罐Cowrie进行评估，本文实现了单个命令的更接近实际机器的相似性。此外，在多步攻击场景中，所提出的架构将令牌使用量与不管理输入和输出的基线情况相比减少了最多77%。关于LLMs在网络欺骗中的利用讨论，以及阻碍其广泛应用的限制，表明LLMs在网络欺骗方面表现出了潜力，但在实现广泛应用之前需要进一步研究。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`二、论文翻译`,
                    msg: String.raw`<div class="markdown-body"><p>二、论文翻译</p></div>`,
                },
                secondary_rol: {
                    header: String.raw``,
                    msg: String.raw`<div class="markdown-body"></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# I. INTRODUCTION`,
                    msg: String.raw`<div class="markdown-body"><p>Engaging threat actors using cyber deception assets is one of the most effective ways of understanding the threat in its entirety, as it enables defenders to monitor live attacks and to collect data for post-mortem analyses. Honeypots, used for cyber deception, are an excellent security resource whose primary utility is to be probed, attacked, and otherwise
The associate editor coordinating the review of this manuscript and approving it for publication was Anandakumar Haldorai . compromised so that the attacker's modus operandi can be dissected [1].
Such honeypots can be deployed with an emphasis on attack discovery in a research context or an emphasis on protection and mitigation in a production context [2]. Both versions rely on deception and their ability to realistically respond to an attacker's interaction. The aim is to maximize attacker interaction while minimizing detection for research honeypots and to distract attackers effectively for production honeypots. For the purpose of this paper, the design of research honeypots is investigated.
The sophistication of a deployed honeypot can be described by the level of interaction it affords to the attacker. For example, full device emulation provides a higher level of interaction, allowing for greater deception while opening the possibility of aiding and propagating attacks. Conversely, service simulation offers a lower level of interaction but is cheaper to deploy [3]. Most honeypots are designed to offer a fixed level of interaction within this range [4].</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# I. INTRODUCTION`,
                    msg: String.raw`<div class="markdown-body"><p>I. 引言</p>
<p>利用网络欺骗资源与威胁行为者进行交互是理解整体威胁的最有效方式之一，因为它使得防御者能够监视实时攻击并收集数据进行事后分析。用于网络欺骗的蜜罐是一种优秀的安全资源，其主要用途是被探测、攻击和以其他方式被破坏，从而可以解剖攻击者的作业模式 [1]。</p>
<p>这些蜜罐可以部署时侧重于攻击发现，无论是在研究环境中还是在生产环境中强调保护和缓解 [2]。两种版本都依赖于欺骗性质及其对攻击者互动的真实响应能力。其目的是在研究蜜罐中最大化攻击者互动同时最小化检测，而在生产蜜罐中则是有效地分散攻击者注意力。本文将重点探讨研究蜜罐的设计。</p>
<p>已部署蜜罐的复杂性可以通过其提供给攻击者的互动级别来描述。例如，完全设备仿真提供了更高级别的互动，允许更大范围的欺骗，但也可能帮助并传播攻击。相反，服务模拟提供了较低级别的互动，但部署成本更低 [3]。大多数蜜罐被设计为在此范围内提供固定级别的互动 [4]。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# A. DETECTION AND BREAKOUT RISKS`,
                    msg: String.raw`<div class="markdown-body"><p>The main threats to honeypots lie in two areas: an attacker's ability to detect they are in a deceiving environment and an attacker's ability to break out of the defined scope of actions and use the honeypot for their intended purpose. These phenomena will be referred to as the detection of deception and risk respectively. It is the goal of the honeypot to maintain deception while minimizing detection and risk.
These threats are proven founded when observing the current state of widely deployed honeypots across the internet. Of lower interaction honeypots, 7000 were fingerprinted due to their interactions in a study by Vetterl et al. of which over a quarter hadn't been updated in over two years [5]. This highlights a lack of consistent maintenance for static implementations. Other studies also find a potential risk of breakout in higher interaction honeypots due to their complicated or volatile implementation in giving an attacker free rein on a system [4]. These issues present a potential avenue for using an alternative method to maintain modern interaction without presenting extra risk.
Generative models can fill this gap where natural language is a core part of the deception. GPTs (Generative Pre-trained Transformers) are transformer models that are pre-trained by an organization to fulfill a wide range of generative use cases. GPT models such as ChatGPT have seen an explosion in popularity of late as a summarizer, conversationalist, and recommendation system, giving responses (also called answers) based on the context and questions provided by the user [6], [7]. These transformer models operate by predicting the next token in a sequence based on the observed history [8]. A token in this case is a character sequence of around 0.75 words [9].
Large Language Models (LLMs) are an implementation of GPTs that focus on natural language use cases. These models have billions of parameters and are trained using a large dataset, limiting their deployment [10]. Alternatively, GPTs trained for a more specific use case with fewer parameters can be used instead of a general LLM. In this paper, the model is treated as a black box, and the two terms are used interchangeably.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# A. DETECTION AND BREAKOUT RISKS`,
                    msg: String.raw`<div class="markdown-body"><p>主要威胁于蜜罐的两个方面：攻击者能够探测到它们处于欺骗环境中，并且攻击者能够越过定义的行动范围，将蜜罐用于其预期目的。这些现象将分别被称为欺骗探测和风险。蜜罐的目标是在最小化探测和风险的同时保持欺骗。</p>
<p>观察互联网上广泛部署的蜜罐的当前状态可以证明这些威胁是存在的。在一项由Vetterl等人进行的研究中，针对低交互蜜罐，有7000个蜜罐因其交互而被指纹识别，其中超过四分之一在两年以上没有更新[5]。这突显了静态实施蜜罐缺乏一致的维护。其他研究还发现，由于其在系统上提供给攻击者自由行动的复杂或不稳定实施，高交互蜜罐存在突破风险[4]。这些问题提供了一个潜在的途径，即采用替代方法来保持现代交互而不增加额外风险。</p>
<p>生成模型可以填补这一空白，其中自然语言是欺骗的核心部分。GPT（生成预训练变换器）是预先由组织训练以满足广泛生成用途的变换器模型。近来，像ChatGPT这样的GPT模型作为总结者、对话者和推荐系统而广受欢迎，根据用户提供的上下文和问题给出响应（也称为答案）[6]，[7]。这些变换器模型通过根据观察到的历史来预测序列中的下一个标记[8]。在这种情况下，标记是一个大约0.75个单词的字符序列[9]。</p>
<p>大语言模型（LLMs）是GPT的一种实现，专注于自然语言使用案例。这些模型具有数十亿个参数，并使用大型数据集进行训练，限制了它们的部署[10]。作为替代，可以使用针对更具体用例进行训练并具有更少参数的GPT。在本文中，该模型被视为黑盒，并且这两个术语可以互换使用。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# B. PROBLEM STATEMENT`,
                    msg: String.raw`<div class="markdown-body"><p>This paper investigates the design of research honeypots that are designed to be easy to update and maintain via publicly available generative models without sacrificing interactivity or risking unintended breakouts by attackers. As proof of concept, the use of GPT is explored where a large language model (GPT3.5-Turbo-0301 [11]) is instructed to behave as a honeypot that accepts terminal commands as its input. Since these models operate on prediction based on probabilities to formulate their answers and have limits on the size (measured in tokens or words) of questions and answers, merely passing an attacker's commands as questions to the model simulating a honeypot and its answers to the attacker presents limitations that could reveal the deception.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# B. PROBLEM STATEMENT`,
                    msg: String.raw`<div class="markdown-body"><p>本文探讨设计研究蜜罐的问题，这些蜜罐通过公开可用的生成模型设计，旨在易于更新和维护，同时不牺牲交互性，也不会因攻击者的意外突破而存在风险。作为概念验证，本文探讨了使用GPT进行研究，在此过程中，一个大型语言模型（GPT3.5-Turbo-0301 [11]）被指示模拟一个接受终端命令作为输入的蜜罐。由于这些模型基于概率预测生成答案，并且在问题和答案的大小（以token或词汇计量）上存在限制，仅仅将攻击者的命令作为问题传递给模型，模拟蜜罐并将其答案返回给攻击者，可能会显露出欺骗性的限制。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# C. CONTRIBUTIONS`,
                    msg: String.raw`<div class="markdown-body"><p>This paper addresses the challenges of simulating and maintaining honeypots by presenting an architecture to manage the inputs and outputs of a generative model simulating a honeypot to reduce non-deterministic outputs and token usage while making the deception more realistic. An example honeypot following this methodology is evaluated against a traditional low-risk honeypot, Cowrie, where greater similarity to an actual machine is achieved for single commands. Effectiveness in different multi-step scenarios is also measured to evaluate deception and token usage through five simulated attacks based on MITRE ATT&amp;CK techniques, tactics, and procedures (TTPs) [12]: system reconnaissance, data obfuscation and ransomware, scanning and lateral propagation, persistence, and data reconnaissance and exfiltration. This paper makes the following contributions.
1) The paper presents a methodology to minimize randomness and token use while preserving the illusion of an attackable system when processing commands passed to an example LLM.
2) The presented methodology is used as the basis for an architecture presented through a block diagram and a series of algorithms for manipulating the input/output (I/O) of a generative pre-trained model. This example architecture is used as a proof of concept and can be modified to simulate any system using any sufficiently trained transformer or language model. 3) The interactivity and effectiveness of the proposed honeypot implementation is evaluated against a commonly used medium-interaction honeypot, Cowrie. For single commands, the proposed honeypot achieved greater similarity to an actual machine than Cowrie. Furthermore, in several multi-step attack scenarios, the proposed architecture maintained sessions for longer than Cowrie and reduced the token usage by up to 77% when compared to a baseline scenario that did not manage the inputs to and outputs from the LLM. 4) The paper discusses the limitations of using generative models for cyber deception and potential directions for future research.
The remainder of the paper is organized as follows: Section II provides background and context for intelligent and contextual interaction in honeypots and generative model usage. Section III detail how to alter the model's input to facilitate a base honeypot deployment. Section IV compares the proposed honeypot to a traditional honeypot to determine if the  proposed implementation deceives attackers better. Section V details the limitations of using generative models such as GPT as a honeypot backend and their mitigations. Section VI concludes the paper with a pointer to further work.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# C. CONTRIBUTIONS`,
                    msg: String.raw`<div class="markdown-body"><p>这篇论文通过提出一种架构来管理生成模型模拟蜜罐的输入和输出，以减少非确定性输出和令牌使用，并使欺骗更加真实。按照这种方法设计的一个示例蜜罐与传统低风险蜜罐Cowrie进行了评估，实现了对单个命令更接近实际机器的相似性。还通过五种基于MITRE ATT&amp;CK技术、策略和过程（TTPs）的模拟攻击（系统侦察、数据混淆和勒索软件、扫描和横向传播、持久性、数据侦察和外部传输）来评估在不同多步攻击方案中的有效性以及欺骗和令牌使用情况。本文的贡献如下：
1) 本文提出了一种方法论，通过处理传递给示例LLM的命令，最小化随机性和令牌使用，同时保持对可攻击系统的幻觉。
2) 所提出的方法作为一个架构的基础，通过一个块图和一系列用于操作生成预训练模型的输入/输出（I/O）的算法来呈现。这个示例架构被用作概念验证，并可以修改以模拟使用任何充分训练的转换器或语言模型的任何系统。
3) 对所提出的蜜罐实现的互动性和有效性进行了评估，与常用的中等互动蜜罐Cowrie进行了比较。对于单个命令，所提出的蜜罐比Cowrie更接近实际机器。此外，在几种多步攻击方案中，所提出的架构比Cowrie维持了更长时间的会话，并在与未管理LLM输入和输出的基准方案相比，将令牌使用量降低了高达77%。
4) 讨论了使用生成模型进行网络欺骗的局限性以及未来研究的可能方向。
该论文的剩余部分组织如下：第II节为蜜罐中智能和环境互动的背景和背景提供了背景。第III节详细介绍了如何改变模型的输入以促进基础蜜罐的部署。第IV节比较了所提出的蜜罐与传统蜜罐，以确定所提出的实现是否更好地欺骗了攻击者。第V节详细介绍了使用生成模型（如GPT）作为蜜罐后端的局限性及其缓解措施。第VI节总结了论文，并指出了进一步工作的方向。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# II. BACKGROUND AND RELATED WORK`,
                    msg: String.raw`<div class="markdown-body"><p>A preliminary investigation into the background and related work is necessitated when exploring the realm of LLMs and their potential application for cyber deception. To this end, Honeypots, LLMs, and their combination are explored. For honeypots, their classifications, distinguishing features, and behaviors are explored. For LLMs, their underlying functionality and their potential use for a cyber deception scenario are examined. Both domains along with pertinent research within them are examined in the following subsections.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# II. BACKGROUND AND RELATED WORK`,
                    msg: String.raw`<div class="markdown-body"><p>在探索大型预训练转换模型（LLMs）及其在网络欺骗中潜在应用时，首先需要对背景和相关工作进行初步调查。为此，研究了蜜罐、LLMs以及它们的结合应用。对于蜜罐，探讨了其分类、显著特征及行为特性。对于LLMs，分析了它们的基本功能及其在网络欺骗场景中的潜在应用。在以下各小节中，综合考察了这两个领域以及相关的研究成果。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# A. BACKGROUND`,
                    msg: String.raw`<div class="markdown-body"><p>Honeypots and large language models, the two key technologies relevant to the proposed honeypot design, are outlined.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# A. BACKGROUND`,
                    msg: String.raw`<div class="markdown-body"><p>背景：概述了两项与提出的蜜罐设计相关的关键技术，即蜜罐和大型语言模型。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 1) HONEYPOTS AND INTERACTIVITY`,
                    msg: String.raw`<div class="markdown-body"><p>Honeypots at their core aim to engage attackers without their knowledge [1]. Honeypots have been used for threat engagement since 1997 through the use of Fred Cohen's Deception Toolkit (DTK) to emulate seemingly real devices and systems [13]. From that initial idea, honeypots of varying levels of interaction have proliferated and been incorporated into research and production environments.
Research honeypots serve to gather information and collect artifacts from attackers. The value they add is derived from their ability to discover new trends in attack vectors and patterns [2]. Production honeypots protect a network by serving as a sandbox to discover potential entry vectors for patching in the form of active defense [2], [14]. Production honeypots can also operate alongside their real counterpart, serving as a ''jail'' by reactively switching the connection to the honeypot. Zarca et al. [15] demonstrated this idea by using software-defined networking (SDN) to redirect flows to virtual versions of IoT devices by a security orchestrator after the initial compromise.
Most honeypots exist on a spectrum between low and high interaction determined by the scope of available actions in the environment that is afforded to the attacker [2]. A more barebones environment is provided by Low-interaction honeypots (LIHs), meant to be easy to deploy with low setup costs. Services are emulated by an LIH in a limited capacity in a way that they cannot be fully exploited. One such way is through hard-coded outputs, which prevent complete access from being gained by attackers, as there is no operating system for them to interact with.
A much more realistic emulation of a target is provided by high-interaction honeypots (HIHs), granting attackers more freedom in their actions. Virtual machines or quarantined physical devices can be used as HIHs. Dynamically existing on the spectrum of interaction in various ways can also be achieved by honeypots, with one way being the adjustment of their level of interactivity depending on the provided context.
Greater deception is available in a more robust environment via static HIHs. However, with this higher level of interaction and interoperability, a greater risk of out-of-scope compromises and an increased cost to set up, maintain, and scale are also introduced. Conversely, LIHs are operated at a reduced cost and risk of exploitation, allowing for greater scalability while sacrificing detectability and capability [3]. Another option is to take a different or ''intelligent'' approach to generating output. This is further expounded in the related work section. The pros and cons of each level are shown in Table 1. The goal of the proposed LLM-based honeypot is to reach the perceived level of capability of HIHs while maintaining the safety and cost of LIHs in addition to avoiding being fingerprinted.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 1) HONEYPOTS AND INTERACTIVITY`,
                    msg: String.raw`<div class="markdown-body"><p>Honeypots at their core aim to engage attackers without their knowledge [1]. Honeypots have been used for threat engagement since 1997 through the use of Fred Cohen's Deception Toolkit (DTK) to emulate seemingly real devices and systems [13]. From that initial idea, honeypots of varying levels of interaction have proliferated and been incorporated into research and production environments.</p>
<p>Research honeypots serve to gather information and collect artifacts from attackers. The value they add is derived from their ability to discover new trends in attack vectors and patterns [2]. Production honeypots protect a network by serving as a sandbox to discover potential entry vectors for patching in the form of active defense [2], [14]. Production honeypots can also operate alongside their real counterpart, serving as a ''jail'' by reactively switching the connection to the honeypot. Zarca et al. [15] demonstrated this idea by using software-defined networking (SDN) to redirect flows to virtual versions of IoT devices by a security orchestrator after the initial compromise.</p>
<p>Most honeypots exist on a spectrum between low and high interaction determined by the scope of available actions in the environment that is afforded to the attacker [2]. A more barebones environment is provided by Low-interaction honeypots (LIHs), meant to be easy to deploy with low setup costs. Services are emulated by an LIH in a limited capacity in a way that they cannot be fully exploited. One such way is through hard-coded outputs, which prevent complete access from being gained by attackers, as there is no operating system for them to interact with.</p>
<p>A much more realistic emulation of a target is provided by high-interaction honeypots (HIHs), granting attackers more freedom in their actions. Virtual machines or quarantined physical devices can be used as HIHs. Dynamically existing on the spectrum of interaction in various ways can also be achieved by honeypots, with one way being the adjustment of their level of interactivity depending on the provided context.</p>
<p>Greater deception is available in a more robust environment via static HIHs. However, with this higher level of interaction and interoperability, a greater risk of out-of-scope compromises and an increased cost to set up, maintain, and scale are also introduced. Conversely, LIHs are operated at a reduced cost and risk of exploitation, allowing for greater scalability while sacrificing detectability and capability [3]. Another option is to take a different or ''intelligent'' approach to generating output. This is further expounded in the related work section. The pros and cons of each level are shown in Table 1. The goal of the proposed LLM-based honeypot is to reach the perceived level of capability of HIHs while maintaining the safety and cost of LIHs in addition to avoiding being fingerprinted.</p>
<p>翻译如下：</p>
<p>蜜罐的核心目标是在不被攻击者察觉的情况下与其进行互动[1]。自1997年以来，蜜罐通过使用Fred Cohen的欺骗工具包（DTK）模拟看似真实的设备和系统来用于威胁互动[13]。从最初的想法开始，各种交互级别的蜜罐已经大量涌现并被纳入研究和生产环境。</p>
<p>研究型蜜罐旨在收集攻击者的信息并收集相关的证据。它们的价值在于能够发现攻击向量和模式的新趋势[2]。生产型蜜罐通过充当沙盒来保护网络，发现可能用于补丁的入口向量，作为主动防御的一部分[2]，[14]。生产型蜜罐还可以与其真实对应物并存，通过将连接反应性地切换到蜜罐来充当“监狱”。Zarca等人[15]通过使用软件定义网络（SDN），在初始入侵后由安全编排器将流量重定向到虚拟版本的物联网设备上，展示了这一理念。</p>
<p>大多数蜜罐存在于低交互和高交互之间的光谱上，这取决于攻击者在环境中可执行的行动范围[2]。低交互蜜罐（LIHs）提供了更为简洁的环境，易于部署且成本低廉。LIHs通过有限的方式模拟服务，使得攻击者无法完全利用。一种方式是通过硬编码输出，防止攻击者获取完全访问权限，因为他们无法与操作系统进行交互。</p>
<p>高交互蜜罐（HIHs）提供了更为真实的目标仿真，允许攻击者在其行动中拥有更多自由。可以使用虚拟机或隔离的物理设备作为HIHs。蜜罐还可以动态地在交互级别的光谱上存在，一种方式是根据所提供的上下文调整其交互水平。</p>
<p>通过静态的高交互蜜罐提供了更大的欺骗效果。然而，这种更高级别的交互和互操作性也带来了超出范围的妥协风险，以及设置、维护和扩展的成本增加[3]。相反，LIHs以较低的成本和风险运行，允许更大规模的部署，但牺牲了可检测性和能力。另一种选择是采用不同或“智能”的方法生成输出，相关工作部分进一步详述了这一点。每个级别的优缺点在表1中显示。提议的基于LLM的蜜罐的目标是在保持LIHs的安全性和成本的同时，达到HIHs感知能力水平，并避免被指纹识别。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 2) GENERATIVE AND LARGE LANGUAGE MODELS (LLMs)`,
                    msg: String.raw`<div class="markdown-body"><p>LLMs are large-scale transformer models that use the self-attention mechanism [27] to generate text. Self-attention allows the model to dynamically weigh the importance and relevance of different parts of an input sequence to the whole, enabling it to adapt and capture long-range dependencies effectively. The attention mechanism is crucial in allowing generative models to predict what word or token is likely to follow in a sequence [28]. This allows a model to understand the importance of those tokens and their sequence, allowing for greater understanding than previous NLP models [27].
A key strength of these models is the ability to handle a wide range of tasks, input types, and sizes, which is made possible by allowing them to learn from vast and diverse data sources and generalize to new examples, further enhancing their adaptability. Additionally, their adaptability can be further enhanced by fine-tuning or extending them to accommodate specific requirements, making them versatile in their implementation.
ChatGPT and its underlying models, gpt-3.5-turbo and gpt-4 [6], [29] will be more closely examined due to OpenAI's GPT model's explosion in popularity. gpt-3.5-turbo is made up of 175 billion features trained on Internet data [30]. gpt-4 is an improvement in knowledge and implementation of gpt-3.5-turbo, where is was able to score in the 90th percentile on the Uniform BAR Exam while gpt-3.5-turbo scored in the 10th [29].
ChatGPT's models can have varying levels of randomness set by its temperature parameter. A higher temperature permits the model to be allowed to take risks and select a token sequence that may not have been the most probable. Compounding randomness can be introduced in the output as future probabilities will be altered by earlier differences in choices. In the context of cyber deception, it is preferred that the token generation be made as deterministic as possible to avoid detection, so a lower temperature is applied.
OpenAI's chat completion models also provide the ability to provide a persona-defining system prompt to further fine-tune the use case easily [31]. This is explored further in Section III.
The models follow a QA (question-answer) completion paradigm that can be made to behave in different ways by using the current question or prompt and past QA pairs to craft its response [7]. Based on this input, the model generates the most likely sequence of tokens. gpt-3.5-turbo,gpt-4, and gpt-4-32k each have memory limits brought from attention and the positional encoding of each token with each token being approximately 0.75 words [9], [32]. This limits the number of tokens that the model can operate within both its input and output completion. These token limits are 4k, 8k, and 32k respectively.
OpenAI's models charge a small amount per 1k tokens sent to their model via their API [33]. The rate varies depending on the model used. For gpt-3.5-turbo and gpt-3.5-turbo-0301, it is <font color="#00FF00">$</font><font color="#FF00FF">0.002 per 1k tokens for both prompt and completion. The vastly improved gpt-4 has two versions: a base and an extended context model referred to as gpt-4 and gpt-4-32k respectively. With the improved knowledge base and size of GPT4 comes an increased cost with gpt-4 costing </font><font color="#00FF00">$</font>0.03 per 1k prompt tokens and <font color="#00FF00">$</font><font color="#FF00FF">0.06 per 1k completion tokens. gpt-4-32k is double this at </font><font color="#00FF00">$</font>0.06 per 1k prompt tokens and $0.12 per 1k completion tokens. Compared to GPT3.5, GPT4 models cost 15x-30x for prompts and 30x-60x as much for completion.
These operating constraints place certain limitations on how much context can be given to the model for it to perform its completion as efficiently and prudently as possible. However, even with limited context windows, the prompt can be crafted to provide desirable output personalities such as writing code based on human description or interacting with the user as a command terminal [7], [34].</p><hr /><p>LLMs are large-scale transformer models that use the self-attention mechanism [27] to generate text. Self-attention allows the model to dynamically weigh the importance and relevance of different parts of an input sequence to the whole, enabling it to adapt and capture long-range dependencies effectively. The attention mechanism is crucial in allowing generative models to predict what word or token is likely to follow in a sequence [28]. This allows a model to understand the importance of those tokens and their sequence, allowing for greater understanding than previous NLP models [27].
A key strength of these models is the ability to handle a wide range of tasks, input types, and sizes, which is made possible by allowing them to learn from vast and diverse data sources and generalize to new examples, further enhancing their adaptability. Additionally, their adaptability can be further enhanced by fine-tuning or extending them to accommodate specific requirements, making them versatile in their implementation.
ChatGPT and its underlying models, gpt-3.5-turbo and gpt-4 [6], [29] will be more closely examined due to OpenAI's GPT model's explosion in popularity. gpt-3.5-turbo is made up of 175 billion features trained on Internet data [30]. gpt-4 is an improvement in knowledge and implementation of gpt-3.5-turbo, where is was able to score in the 90th percentile on the Uniform BAR Exam while gpt-3.5-turbo scored in the 10th [29].
ChatGPT's models can have varying levels of randomness set by its temperature parameter. A higher temperature permits the model to be allowed to take risks and select a token sequence that may not have been the most probable. Compounding randomness can be introduced in the output as future probabilities will be altered by earlier differences in choices. In the context of cyber deception, it is preferred that the token generation be made as deterministic as possible to avoid detection, so a lower temperature is applied.
OpenAI's chat completion models also provide the ability to provide a persona-defining system prompt to further fine-tune the use case easily [31]. This is explored further in Section III.
The models follow a QA (question-answer) completion paradigm that can be made to behave in different ways by using the current question or prompt and past QA pairs to craft its response [7]. Based on this input, the model generates the most likely sequence of tokens. gpt-3.5-turbo,gpt-4, and gpt-4-32k each have memory limits brought from attention and the positional encoding of each token with each token being approximately 0.75 words [9], [32]. This limits the number of tokens that the model can operate within both its input and output completion. These token limits are 4k, 8k, and 32k respectively.
OpenAI's models charge a small amount per 1k tokens sent to their model via their API [33]. The rate varies depending on the model used. For gpt-3.5-turbo and gpt-3.5-turbo-0301, it is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>0.002</mn><mi>p</mi><mi>e</mi><mi>r</mi><mn>1</mn><mi>k</mi><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>b</mi><mi>o</mi><mi>t</mi><mi>h</mi><mi>p</mi><mi>r</mi><mi>o</mi><mi>m</mi><mi>p</mi><mi>t</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>c</mi><mi>o</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>&#x0002E;</mo><mi>T</mi><mi>h</mi><mi>e</mi><mi>v</mi><mi>a</mi><mi>s</mi><mi>t</mi><mi>l</mi><mi>y</mi><mi>i</mi><mi>m</mi><mi>p</mi><mi>r</mi><mi>o</mi><mi>v</mi><mi>e</mi><mi>d</mi><mi>g</mi><mi>p</mi><mi>t</mi><mo>&#x02212;</mo><mn>4</mn><mi>h</mi><mi>a</mi><mi>s</mi><mi>t</mi><mi>w</mi><mi>o</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>s</mi><mi>:</mi><mi>a</mi><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>a</mi><mi>n</mi><mi>e</mi><mi>x</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>d</mi><mi>e</mi><mi>d</mi><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi><mi>r</mi><mi>e</mi><mi>f</mi><mi>e</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>t</mi><mi>o</mi><mi>a</mi><mi>s</mi><mi>g</mi><mi>p</mi><mi>t</mi><mo>&#x02212;</mo><mn>4</mn><mi>a</mi><mi>n</mi><mi>d</mi><mi>g</mi><mi>p</mi><mi>t</mi><mo>&#x02212;</mo><mn>4</mn><mo>&#x02212;</mo><mn>32</mn><mi>k</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>p</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>l</mi><mi>y</mi><mo>&#x0002E;</mo><mi>W</mi><mi>i</mi><mi>t</mi><mi>h</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>i</mi><mi>m</mi><mi>p</mi><mi>r</mi><mi>o</mi><mi>v</mi><mi>e</mi><mi>d</mi><mi>k</mi><mi>n</mi><mi>o</mi><mi>w</mi><mi>l</mi><mi>e</mi><mi>d</mi><mi>g</mi><mi>e</mi><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi><mi>o</mi><mi>f</mi><mi>G</mi><mi>P</mi><mi>T</mi><mn>4</mn><mi>c</mi><mi>o</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>a</mi><mi>n</mi><mi>i</mi><mi>n</mi><mi>c</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi>d</mi><mi>c</mi><mi>o</mi><mi>s</mi><mi>t</mi><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mi>g</mi><mi>p</mi><mi>t</mi><mo>&#x02212;</mo><mn>4</mn><mi>c</mi><mi>o</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow></math>0.03 per 1k prompt tokens and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>0.06</mn><mi>p</mi><mi>e</mi><mi>r</mi><mn>1</mn><mi>k</mi><mi>c</mi><mi>o</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><mi>n</mi><mi>s</mi><mo>&#x0002E;</mo><mi>g</mi><mi>p</mi><mi>t</mi><mo>&#x02212;</mo><mn>4</mn><mo>&#x02212;</mo><mn>32</mn><mi>k</mi><mi>i</mi><mi>s</mi><mi>d</mi><mi>o</mi><mi>u</mi><mi>b</mi><mi>l</mi><mi>e</mi><mi>t</mi><mi>h</mi><mi>i</mi><mi>s</mi><mi>a</mi><mi>t</mi></mrow></math>0.06 per 1k prompt tokens and $0.12 per 1k completion tokens. Compared to GPT3.5, GPT4 models cost 15x-30x for prompts and 30x-60x as much for completion.
These operating constraints place certain limitations on how much context can be given to the model for it to perform its completion as efficiently and prudently as possible. However, even with limited context windows, the prompt can be crafted to provide desirable output personalities such as writing code based on human description or interacting with the user as a command terminal [7], [34].</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 2) GENERATIVE AND LARGE LANGUAGE MODELS (LLMs)`,
                    msg: String.raw`<div class="markdown-body"><p>LLMs是利用自注意力机制[27]生成文本的大规模Transformer模型。自注意力机制使模型能够动态加权输入序列中不同部分的重要性和相关性，从而有效地捕捉长距离依赖关系。注意力机制在生成模型中至关重要，它使得模型能够预测序列中接下来可能出现的词语或标记[28]。这使得模型能够理解这些标记及其顺序的重要性，比之前的自然语言处理模型具有更深的理解能力[27]。</p>
<p>这些模型的一个关键优势是它们能够处理各种任务、输入类型和规模，这得益于它们能够从广泛和多样的数据源中学习，并且能够推广到新的示例，进一步增强了它们的适应性。此外，通过微调或扩展，它们可以进一步提升适应特定需求的能力，使得它们在实施中更加多才多艺。</p>
<p>由于OpenAI的GPT模型在人气上迅速爆发，将更详细地审视ChatGPT及其基础模型gpt-3.5-turbo和gpt-4 [6]，[29]。gpt-3.5-turbo由1750亿特征组成，训练于互联网数据[30]。gpt-4在知识和实现上有所改进，相较于gpt-3.5-turbo，其在统一法学院入学考试中得分位于90%分位数，而gpt-3.5-turbo则位于10%分位数[29]。</p>
<p>ChatGPT的模型可以通过其温度参数设置不同的随机性水平。较高的温度允许模型冒险选择一种可能性不高的标记序列。这种输出中引入了复合随机性，因为之后的概率将受早期选择的不同而改变。在网络欺骗的背景下，为了避免被检测，更希望令牌生成尽可能确定性，因此应用较低的温度。</p>
<p>OpenAI的聊天完成模型还提供了提供定义人物特质的系统提示的能力，以进一步轻松微调使用案例[31]。这在第三节中进一步探讨。</p>
<p>这些模型遵循了一种QA（问答）完成范式，可以通过使用当前的问题或提示以及过去的QA对来制定其响应，使其表现出不同的行为方式[7]。基于这种输入，模型生成最可能的标记序列。gpt-3.5-turbo，gpt-4和gpt-4-32k分别由于注意力和每个标记的位置编码而具有记忆限制，每个标记约为0.75个单词[9]，[32]。这限制了模型在输入和输出完成中能够操作的标记数量。这些标记限制分别为4k，8k和32k。</p>
<p>OpenAI的模型通过其API每发送1k标记收取少量费用[33]。费率根据使用的模型而异。对于gpt-3.5-turbo和gpt-3.5-turbo-0301，每1k标记的提示和完成费用为0.002美元。改进的gpt-4有两个版本：基础版本和扩展上下文模型称为gpt-4和gpt-4-32k，其成本更高，gpt-4的提示标记为每1k0.03美元，完成标记为每1k0.06美元。gpt-4-32k为此的两倍，即每1k0.06美元和0.12美元。与GPT3.5相比，GPT4模型的提示费用增加了15倍至30倍，完成费用增加了30倍至60倍。</p>
<p>这些操作约束限制了可以提供给模型以使其完成尽可能高效和谨慎的上下文的数量。但即使有了有限的上下文窗口，仍然可以制定提示以提供理想的输出人格，例如基于人类描述编写代码或作为命令终端与用户交互[7]，[34]。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# B. RELATED WORK`,
                    msg: String.raw`<div class="markdown-body"><p>This section provides a summary of relevant prior work in the areas of honeypots, adaptive interaction, and LLMs used for cyber deception. Table 2 summarizes the mentioned honeypots.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# B. RELATED WORK`,
                    msg: String.raw`<div class="markdown-body"><p>本节提供了关于蜜罐、自适应交互和用于网络欺骗的LLM相关先前工作的总结。表2总结了提到的蜜罐。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 1) STATIC HONEYPOTS`,
                    msg: String.raw`<div class="markdown-body"><p>Extensive work has been done in the design and implementation of LIHs and HIHs over the years. Notable work includes the development of various LIH frameworks such as Honeyd [16], which provide lightweight and scalable solutions for emulating vulnerable services through static response mechanisms. However, any traffic outside of the defined behavior may cause a failure in deception that could be used to fingerprint the honeypot [35].
Affording slightly more freedom to the attacker are medium interaction honeypots such as Cowrie and Thingpot [17], [18]. An entire system is simulated by these honeypots, providing more freedom than a single service. However, these honeypots are still detectable if they receive an input that they are unable to handle.
The most advanced and interactive of honeypots are HIHs, of which much research has been done. These honeypots are either entire virtualizations of systems or are physical devices. For instance, Siphon [19] is a network of HIHs where each device is physically present and connected to attackers via SSH forwarding. However, this setup is costly to maintain, requiring a dedicated space for the devices as well as a separate monitoring agent to watch the health of each device. Honware [20] is a virtualized high-interaction honeypot in which unique firmware and filesystems of embedded devices are served via a custom kernel.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 1) STATIC HONEYPOTS`,
                    msg: String.raw`<div class="markdown-body"><p>在设计和实施LIHs和HIHs方面进行了大量工作。显著的工作包括开发各种LIH框架，如Honeyd [16]，它通过静态响应机制提供了轻量级和可扩展的解决方案，模拟易受攻击的服务。然而，任何超出定义行为范围的流量可能会导致欺骗失败，从而被用于指纹识别这个蜜罐 [35]。</p>
<p>中交互蜜罐（如Cowrie和Thingpot [17], [18]）为攻击者提供了更多自由度。这些蜜罐模拟整个系统，比单一服务提供更多自由度。然而，如果它们收到无法处理的输入，这些蜜罐仍然是可以检测到的。</p>
<p>最先进和互动性最强的蜜罐是HIHs，对其进行了大量研究。这些蜜罐要么是系统的完整虚拟化，要么是物理设备。例如，Siphon [19]是一个HIHs网络，每个设备都是物理存在的，并通过SSH转发与攻击者连接。然而，这种设置维护成本高昂，需要专门的空间放置设备，以及一个单独的监控代理来监视每个设备的健康状态。Honware [20]是一个虚拟化的高交互蜜罐，通过自定义内核提供嵌入式设备的唯一固件和文件系统。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 2) INTELLIGENT AND DYNAMIC HONEYPOTS`,
                    msg: String.raw`<div class="markdown-body"><p>Knowing the pitfalls of static interaction honeypots, some work has been done on an alternate path to define deceivable interaction as a dynamic and adaptive process rather than a static one. Luo et al. [22] coined the idea of ''intelligent interaction'' in IoTCandyJar, where the honeypot works toward achieving a ''correct'' conversation with attackers and becoming more interactive over time as the expected responses are discovered starting from zero knowledge. Using intelligent interaction, Yamamoto et al. present Firmpot as a framework that uses firmware images and machine learning to learn the behaviors of those devices for intelligent interaction.
Mfogo et al. presented AIIPOT, a novel transformer-based honeypot using chatbots to capture vulnerabilities for Internet of Things (IoT) devices [24]. They follow intelligent interaction principles to learn and interact with each attacker. Their approach using reinforcement learning and transformer models is novel and requires comparison to using LLMs, of which is related work.
Similarly, ''dynamic'' honeypots exist that adapt themselves based on environmental stimuli [36]. This dynamic honeypot design pattern has been used by Pauna et al. via Q-learning to deploy a self-adaptive SSH honeypot that modifies its state based on the observed environment and attacker input [21]. Intelligence and adaptiveness in honeypots aid in the discovery of new threats while avoiding fingerprinting campaigns [5], [37]. However, these honeypots can have varying costs and dependencies depending on the technology used such as IoTCandyJar requiring a preliminary internet-wide scan [22].</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 2) INTELLIGENT AND DYNAMIC HONEYPOTS`,
                    msg: String.raw`<div class="markdown-body"><p>在了解静态交互蜜罐存在的缺陷后，一些工作已经在另一条路上进行，即将易受欺骗的交互定义为一种动态和自适应的过程，而不是静态的过程。Luo等人[22]在IoTCandyJar中提出了"智能交互"的概念，蜜罐致力于与攻击者进行“正确”的对话，并随着时间推移而变得更具交互性，以便从零开始发现预期的响应。通过智能交互，Yamamoto等人提出了Firmpot作为一个框架，使用固件映像和机器学习来学习这些设备的行为，以实现智能交互。</p>
<p>Mfogo等人提出了AIIPOT，这是一个基于变压器的新型蜜罐，利用聊天机器人捕获物联网设备的漏洞[24]。他们遵循智能交互原则，与每个攻击者学习和互动。他们的方法使用强化学习和变压器模型是新颖的，并需要与使用LLM的相关工作进行比较。</p>
<p>类似地，存在“动态”蜜罐，根据环境刺激自适应自身设计模式[36]。Pauna等人通过Q-learning使用了这种动态蜜罐设计，部署了一种自适应SSH蜜罐，根据观察到的环境和攻击者输入修改其状态[21]。蜜罐中的智能和适应性有助于发现新的威胁，同时避免指纹识别攻击[5]，[37]。然而，这些蜜罐的成本和依赖性可能因使用的技术而异，如IoTCandyJar需要初步的全网扫描[22]。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 3) LANGUAGE MODELS FOR CYBER DECEPTION`,
                    msg: String.raw`<div class="markdown-body"><p>For a honeypot to be effective, it must convince the attacker it's both a vulnerable target and a real system. Both charades must be convincing so that a motivated attacker is unable to make the distinction from that of a real system. This idea is reminiscent of the Turing Test for artificial intelligence where an artificial intelligence (AI) can pass if its behavior is indistinguishable from that of a human [38]. Our ''honeypot test'' is if an the honeypot using model-generated output is indistinguishable from that of a real system and is able to avoid honeypot detection through a breakdown in communication.
Mckee and Noever use ChatGPT to model different honeypot tasks an attacker might execute [25]. They mention a token limit of 8,000 for ChatGPT but believe the token limit is a non-issue. However, with the cost of use and long outputs found in testing, this can still remain an issue. When the token limit is reached, old context is thrown out, which can lead to older but still relevant context-changing commands being lost, leading to detection or breakdown in the attack. Further limitations are detailed in Section V.
Sladic et al. investigate the deceptive potential of using generative models as a honeypot [26]. In their work, they survey users and security experts to see if they are able to differentiate output from a real system and that from an LLM. While their work supports the use of LLMs for cyber deception, the focus of this paper is on deception to a dedicated attacker and how that deception might fail.
Cambiaso and Caviglione take a different approach to language model-assisted cyber deception [39]. In their work, they use ChatGPT's ability to craft realistic human interactions to engage email scammers in an effort to waste their time, providing a proactive defense.
Using ChatGPT's context-aware QA functionality and its ability to create the illusion of an attackable interface, adaptive and intelligent interaction can be employed to design an interactive honeypot. This type of honeypot can simulate a number of internal services through its ability to understand and respond to natural language queries. Using generative models to behave as a honeypot is as safe as LIHs since no commands are actually executed. Additionally, as more context is gathered, the honeypot can adjust its behavior and responses to better mimic a real system, thereby increasing the chances of capturing and analyzing new tactics and techniques. This allows for high-interaction targets to be created at a much lower upfront cost. Depending on the use case, these honeypots can be deployed in a research or production environment, as long as they have the proper context.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 3) LANGUAGE MODELS FOR CYBER DECEPTION`,
                    msg: String.raw`<div class="markdown-body"><p>为了使蜜罐有效，它必须让攻击者相信它既是一个易受攻击的目标，又是一个真实的系统。这两种伪装都必须令人信服，以至于一个积极主动的攻击者无法区分它与真实系统的区别。这个想法让人想起了人工智能的图灵测试，即如果一个人工智能的行为与人类的行为无法区分，那么它就能通过测试 [38]。我们的“蜜罐测试”是指，如果一个使用模型生成输出的蜜罐无法与真实系统区分，并且能够通过沟通失效来避免被检测到，那么它就是成功的。</p>
<p>Mckee 和 Noever 使用 ChatGPT 对攻击者可能执行的不同蜜罐任务进行建模 [25]。他们提到 ChatGPT 的标记限制为 8,000，但认为标记限制不是问题。然而，通过测试发现，由于使用成本高和输出较长，这仍然可能是一个问题。当达到标记限制时，旧的上下文被丢弃，这可能导致丢失旧的但仍然相关的上下文改变命令，从而导致被检测或攻击中断。更多限制细节请参阅第五节。</p>
<p>Sladic 等人研究了使用生成模型作为蜜罐的欺骗潜力 [26]。在他们的工作中，他们调查用户和安全专家是否能区分来自真实系统和来自 LLM 的输出。尽管他们的工作支持使用 LLM 进行网络欺骗，但本文的重点是对专门的攻击者进行欺骗以及这种欺骗可能失败的方式。</p>
<p>Cambiaso 和 Caviglione 采用了一种不同的语言模型辅助网络欺骗方法 [39]。在他们的工作中，他们利用 ChatGPT 创造逼真的人类互动来与电子邮件诈骗者进行交流，以浪费他们的时间，提供主动防御。</p>
<p>利用 ChatGPT 的上下文感知问答功能以及其创建可攻击界面的能力，可以采用自适应和智能的交互来设计一个交互式蜜罐。这种类型的蜜罐可以通过其理解和回应自然语言查询的能力模拟许多内部服务。使用生成模型来行为像一个蜜罐一样安全，因为实际上没有执行任何命令。此外，随着获取更多的上下文，蜜罐可以调整其行为和响应，以更好地模仿真实系统，从而增加捕获和分析新的战术和技术的机会。这使得可以以更低的前期成本创建高交互目标。根据使用情况，这些蜜罐可以部署在研究环境或生产环境中，只要它们有适当的上下文。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# III. METHODOLOGY`,
                    msg: String.raw`<div class="markdown-body"><p>Large Language Models (LLMs) can serve a variety of use cases that rely on processing language input. However, for cyber deception through asset simulation, how and what one passes to the model must be examined, and more specifically, how that input and certain directives can be used in preprocessing to direct the output deterministically. Due to the availability and ease of implementation, gpt-3.5-turbo is the model chosen to generate output for the methodology and evaluation of this work.
The methodology is built following the same general pattern of steps where a question is received, the input is modified to fit the model's use case, sent to the model, output is saved for the future, and the response is returned to the user. A flowchart of the general steps taken for the methodology is given in Fig. 1.
Once this methodology is established, pre and postprocessing can be modeled for a naive approach. Then, a more comprehensive approach can be taken to mitigate any limitations in the generating model's design. The last subsection fully outlines the proposed architecture to be evaluated in Section IV.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# III. METHODOLOGY`,
                    msg: String.raw`<div class="markdown-body"><p>大型语言模型（LLMs）可以用于处理依赖语言输入的各种用例。然而，对于通过资产模拟进行网络欺骗，必须审查如何以及向模型传递什么内容，特别是如何在预处理中使用该输入和某些指令以确定性地指导输出。由于其可用性和易于实施性，选择了gpt-3.5-turbo模型来生成本文方法和评估的输出。</p>
<p>方法建立在相同的一般步骤模式下，首先接收问题，修改输入以适应模型的用例，然后将其发送至模型，保存输出以备将来使用，并将响应返回给用户。方法的一般步骤流程图如图1所示。</p>
<p>一旦建立了这一方法，可以对预处理和后处理进行建模以进行一种朴素方法的尝试。然后，可以采取更全面的方法来减轻生成模型设计中的任何限制。最后一个小节完整地概述了将在第四节评估的拟议架构。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# A. PROMPT REFINEMENT`,
                    msg: String.raw`<div class="markdown-body"><p>GPT (Generative Pre-trained Transformer), is a publicly available large language model that crafts its persona based on the user-provided prompt that directs the pre-trained model to generate an output answer for the given question [7], [40]. This means the implementation and expected output can  Additionally, the quality of the response increases with the level of detail and completeness of the prompt, as it provides additional context to the model. Moreover, model parameters such as the temperature can be modified to increase the randomness of the response, leading to greater creativity. However, in this use case, the temperature should be as low as possible. For these experiments and implementation, OpenAI's GPT chat completion models are used. These are used due to their sophistication, popularity, and extendability in context administration [11]. Among these, gpt-3.5-turbo is chosen over gpt-4 due to its reduced usage cost and public availability.
To begin with, in order to make the responses appear as normal conversations, the questions must be crafted carefully. This is illustrated in Tables 345where the question is refined through multiple iterations to make the response more natural. As can be seen, the answer moves from a generic answer indicating that it's an AI language model to a more conclusive and succinct response by providing the model with more context and predetermined biases. To create the illusion of an attackable cyber asset, prompts must be enhanced to ensure that the output to hackers does not include additional explanations that ChatGPT tends to add.
Using the prompt to guide the response can be extended to provide context for future questions. In tables 3-5, the model operates with only a single question as context. However, gpt-3.5-turbo can be used to provide a context history for that session in the form of previous questions and answers [32]. The prompt is split into two sections by GPT's API implementation of questions and answers: system and user-assistant. The system prompt provides the overarching context that the model operates in for all future conversations, such as ''You are a baseball expert'' or ''You are a security professional.'' This guides the knowledge base, even when no further context is given.
The user-assistant section provides example questions and answers as a ''QA pair'' to provide more context, guiding the conversation within the guidelines of the system prompt. For example, in Table 6, the second question is able to pick up from the first QA pair in its context history and use it to answer the next question. In the second conversation in Table 7, that context is not provided to the model, so the model has no idea what food is being asked about. For a honeypot use case, including prior context is useful for preserving changes made by an attacker, such as directory traversal or file changes.
Interestingly, during Conversation 1 in Table 6, the model provided an excessive amount of information regarding the calories, surpassing what would typically be expected in a to-the-point human conversation by repeating the subject. However, when the prompt is modified to minimize the information output, the model responded with just the calorie amount, aligning better with conversational norms. This observation inspired us to delve into manipulating the input and output of the model to create cyber deception assets. If these assets are ''to-the-point'' in their responses, they are able to minimize detection and risk of exploitation beyond the intended scope.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# A. PROMPT REFINEMENT`,
                    msg: String.raw`<div class="markdown-body"><p>GPT（Generative Pre-trained Transformer）是一种公开可用的大型语言模型，根据用户提供的提示来塑造其个性，从而引导预训练模型生成给定问题的输出答案[7]，[40]。这意味着实现和预期输出可以通过详细和完整的提示提高响应的质量。此外，可以修改模型参数（如温度）以增加响应的随机性，从而提高创造力。然而，在这种用例中，温度应尽量低。为了这些实验和实施，采用了OpenAI的GPT聊天完成模型。选择这些模型是因为它们在上下文管理方面的复杂性、流行度和可扩展性[11]。在这些模型中，由于其使用成本较低且公开可用，gpt-3.5-turbo被选择而非gpt-4。</p>
<p>首先，为了使响应看起来像正常对话，必须仔细制定问题。这在表3-5中有所说明，问题通过多次迭代精化，使响应更加自然。可以看到，答案从一个泛泛的回答（表明它是一个AI语言模型）转变为通过为模型提供更多上下文和预定偏见而更加确凿和简洁的回应。为了营造可攻击的网络资产的假象，必须增强提示，以确保输出给黑客的内容不包括ChatGPT倾向于添加的额外解释。</p>
<p>利用提示来指导响应可以扩展到为未来的问题提供上下文。在表3-5中，该模型仅以单个问题作为上下文运行。然而，gpt-3.5-turbo可以通过前面的问题和答案形式为该会话提供上下文历史记录[32]。提示通过GPT的API实现分为两个部分：系统和用户助理。系统提示为所有未来对话操作提供了模型所处的整体上下文，例如“您是棒球专家”或“您是安全专业人员”。即使没有进一步的上下文，这也指导了知识库。</p>
<p>用户助理部分提供了作为“QA对”示例的问题和答案，以提供更多上下文，并在系统提示的指导下引导对话。例如，在表6中，第二个问题能够从第一个QA对的上下文历史中继续，并用它来回答下一个问题。而在表7中的第二次对话中，模型没有提供该上下文，因此对于被问及的食物，模型并不了解。对于蜜罐用例，包括先前的上下文对于保留由攻击者所做的更改（如目录遍历或文件更改）非常有用。</p>
<p>有趣的是，在表6的第一次对话中，模型提供了关于卡路里的大量信息，超出了通常在直接的人类对话中预期的内容，并重复了主题。然而，当提示被修改以最小化信息输出时，模型仅回应了卡路里数量，与会话规范更加契合。这一观察启发我们深入研究操作模型的输入和输出，以创建网络欺骗资产。如果这些资产在响应时“言之凿凿”，它们能够最小化被检测到的风险，以及超出预期范围的利用风险。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# B. SIMPLE LLM CONTEXT HONEYPOT PRINCIPLES`,
                    msg: String.raw`<div class="markdown-body"><p>Establishing that the responses can be guided using the prompt and that responses can use the provided context to determine the output, it can be formulated how to use LLMs like GPT as a honeypot.
Mckee and Noever [25] illustrate this idea for Windows and Linux terminals using ChatGPT to test possible honeypot scenarios. Their approach outlined terminal behavior in the system prompt and appended all past commands and their outputs to be used by the model for completion with the output. The generation of answer a by the LLM presented with question q ′ is given as:
a = LLM(q ′ )(1)
LLM input q ′ is comprised of the combination of the system prompt S, context history of past questions and answers C, and question q defined as:
q ′ = S ∪ C ∪ q (2)
The context history C must then be updated with each new answer and the question that generated it in order for use with the next question in the session:
C = C ∪ {q, a}(3)
These steps are combined to create the following algorithm for building a question in which a large-scale generative model is tuned and prepped to answer.
An architecture using these basic building blocks is illustrated in Fig. 2 with accompanying pseudocode in Algorithm 1. The answer a is derived from the completion of the system prompt S, context history C, and the most recent question q by Algorithm 1 Simple LLM Honeypot Input: q: Attacker-provided question C: Session Context History S: Persona-defining System Prompt Output: a: LLM-generated answer 1: a ← LLM(S, C, q)
2: C ← C ∪ {[q, a]} 3: return a FIGURE 2. Simple LLM honeypot.
the model. However, the context history can indefinitely grow at a fast rate, becoming an issue for longer sessions where the context will be truncated due to token limits or future responses will take time to be calculated.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# B. SIMPLE LLM CONTEXT HONEYPOT PRINCIPLES`,
                    msg: String.raw`<div class="markdown-body"><p>建立回应可以通过提示进行引导，并且可以利用提供的上下文来确定输出的观点，可以明确如何使用像GPT这样的LLM作为蜜罐。Mckee和Noever [25] 使用ChatGPT在Windows和Linux终端中测试可能的蜜罐场景，阐述了他们的方法在系统提示符中概述了终端行为，并附加了所有过去命令及其输出，供模型完成输出使用。当LLM被呈现为问题q'时，其生成的答案a定义为：
<font color="#00FF00">$$</font><font color="#FF00FF"> a = LLM(q') \quad (1) </font><font color="#00FF00">$$</font>
LLM输入<font color="#00FF00">$</font><font color="#FF00FF"> q' </font><font color="#00FF00">$</font>由系统提示符S、过去问题与答案的上下文历史C的组合以及问题q定义：
<font color="#00FF00">$$</font><font color="#FF00FF"> q' = S \cup C \cup q \quad (2) </font><font color="#00FF00">$$</font>
然后，上下文历史C必须更新以包含每个新答案及其生成它的问题，以便在会话中用于下一个问题：
<font color="#00FF00">$$</font><font color="#FF00FF"> C = C \cup \{q, a\} \quad (3) </font><font color="#00FF00">$$</font>
这些步骤结合在一起形成以下构建问题的算法，其中大规模生成模型被调整和准备好回答。</p>
<p>使用这些基本构建模块的架构如图2所示，并附有算法1的伪代码。</p>
<p>答案a来自于系统提示S、上下文历史C以及最近的问题q的完成，由算法1确定。简单LLM蜜罐的输入：q：攻击者提供的问题；C：会话上下文历史；S：定义人物的系统提示。输出：a：LLM生成的答案。
1: <font color="#00FF00">$</font><font color="#FF00FF"> a \leftarrow LLM(S, C, q) </font><font color="#00FF00">$</font>
2: <font color="#00FF00">$</font><font color="#FF00FF"> C \leftarrow C \cup \{[q, a]\} </font><font color="#00FF00">$</font>
3: 返回a</p>
<p><strong>图2.</strong> 简单LLM蜜罐。</p>
<p>然而，上下文历史可以在较长会话中无限增长，由于令牌限制而被截断，或导致未来的响应需要时间计算。</p><hr /><p>建立回应可以通过提示进行引导，并且可以利用提供的上下文来确定输出的观点，可以明确如何使用像GPT这样的LLM作为蜜罐。Mckee和Noever [25] 使用ChatGPT在Windows和Linux终端中测试可能的蜜罐场景，阐述了他们的方法在系统提示符中概述了终端行为，并附加了所有过去命令及其输出，供模型完成输出使用。当LLM被呈现为问题q'时，其生成的答案a定义为：
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>a</mi><mo>&#x0003D;</mo><mi>L</mi><mi>L</mi><mi>M</mi><mo stretchy="false">&#x00028;</mo><msup><mi>q</mi><mi>&#x02032;</mi></msup><mo stretchy="false">&#x00029;</mo><mspace width="1em" /><mo stretchy="false">&#x00028;</mo><mn>1</mn><mo stretchy="false">&#x00029;</mo></mrow></math>
LLM输入<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msup><mi>q</mi><mi>&#x02032;</mi></msup></mrow></math>由系统提示符S、过去问题与答案的上下文历史C的组合以及问题q定义：
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msup><mi>q</mi><mi>&#x02032;</mi></msup><mo>&#x0003D;</mo><mi>S</mi><mo>&#x0222A;</mo><mi>C</mi><mo>&#x0222A;</mo><mi>q</mi><mspace width="1em" /><mo stretchy="false">&#x00028;</mo><mn>2</mn><mo stretchy="false">&#x00029;</mo></mrow></math>
然后，上下文历史C必须更新以包含每个新答案及其生成它的问题，以便在会话中用于下一个问题：
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>C</mi><mo>&#x0003D;</mo><mi>C</mi><mo>&#x0222A;</mo><mo stretchy="false">&#x0007B;</mo><mi>q</mi><mo>&#x0002C;</mo><mi>a</mi><mo stretchy="false">&#x0007D;</mo><mspace width="1em" /><mo stretchy="false">&#x00028;</mo><mn>3</mn><mo stretchy="false">&#x00029;</mo></mrow></math>
这些步骤结合在一起形成以下构建问题的算法，其中大规模生成模型被调整和准备好回答。</p>
<p>使用这些基本构建模块的架构如图2所示，并附有算法1的伪代码。</p>
<p>答案a来自于系统提示S、上下文历史C以及最近的问题q的完成，由算法1确定。简单LLM蜜罐的输入：q：攻击者提供的问题；C：会话上下文历史；S：定义人物的系统提示。输出：a：LLM生成的答案。
1: <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>a</mi><mo>&#x02190;</mo><mi>L</mi><mi>L</mi><mi>M</mi><mo stretchy="false">&#x00028;</mo><mi>S</mi><mo>&#x0002C;</mo><mi>C</mi><mo>&#x0002C;</mo><mi>q</mi><mo stretchy="false">&#x00029;</mo></mrow></math>
2: <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>C</mi><mo>&#x02190;</mo><mi>C</mi><mo>&#x0222A;</mo><mo stretchy="false">&#x0007B;</mo><mo stretchy="false">[</mo><mi>q</mi><mo>&#x0002C;</mo><mi>a</mi><mo stretchy="false">]</mo><mo stretchy="false">&#x0007D;</mo></mrow></math>
3: 返回a</p>
<p><strong>图2.</strong> 简单LLM蜜罐。</p>
<p>然而，上下文历史可以在较长会话中无限增长，由于令牌限制而被截断，或导致未来的响应需要时间计算。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# C. ADAPTIVE LLM CONTEXT HONEYPOTS PRINCIPLES`,
                    msg: String.raw`<div class="markdown-body"><p>To save on tokens, useful context is preloaded into the system prompt to streamline the attacker's use. For example, if an attacker attempts to install a non-standard package like Nmap, a line can be added to the system prompt such as All packages are installed to ensure the attempted execution behaves deterministically. This has the added benefit of potentially saving hundreds of tokens by avoiding a lengthy install output. This handling can also be used to filter interactive packages that will break deception, such as Vim. This is all defined before any session and remains unchanged.
To address the issue of losing context owing to the token limit, past QA pairs can be selectively appended to the context history for that session based on whether they alter the answer to subsequent questions. This context history is then passed to the model. This process extends the length of the session by prolonging the time it takes for the token limit to be reached. For example, ls, a command that shows all files in a directory, will have a different output depending on the current working directory. To ensure the correct output is supplied, any working directory changes are appended to the context history passed to the model. Since ls has no direct downstream effect on other commands, it is not necessary to save that QA pair for future questions. This saves tokens.
In the case of a honeypot session, any context-changing commands are saved to the context history, thereby saving space by discarding QA pairs that don't affect future inputs. This process also has the added benefit of reducing response time as there are fewer calculations for inter-token meaning and sentence structure when predicting the next sequence.
However, some UNIX commands, such as history, require a full input history to have the correct output. If only the context history as previously described is maintained, the full input history needed for that command would not be available. Alternatively, If all inputs and their outputs are preserved similar to Algorithm 1 rather than just the contextchanging ones, the token limit would be reached. Instead, two histories can be maintained: a context history of filtered QA pairs as previously described to replace the context history C in Algorithm 1 and a global history H containing all inputs for that session to be used when needed.
Both histories need to be maintained and updated with each new question and answer. This method of having two histories gives the option of using the question-only history if the question-answer context history becomes too large. Just using the inputs may be lossy, but is preferred to losing deterministicness by running out of context memory. This is preferable since the model fails when the knowledge does not exist but it can make some inferences with incomplete (lossy) knowledge. The global history must also be maintained to ensure it doesn't exceed memory constraints either.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# C. ADAPTIVE LLM CONTEXT HONEYPOTS PRINCIPLES`,
                    msg: String.raw`<div class="markdown-body"><p>为了节省token，有用的上下文被预先加载到系统提示中，以简化攻击者的使用。例如，如果攻击者尝试安装非标准包如Nmap，可以在系统提示中添加一行，例如"All packages are installed"，以确保尝试的执行行为具有确定性。这样做的额外好处是通过避免冗长的安装输出，可能节省数百个token。这种处理也可用于过滤交互式包，如Vim，这些包可能会破坏欺骗效果。所有这些定义在任何会话之前，并保持不变。</p>
<p>为了解决由于token限制而丢失上下文的问题，可以选择性地将过去的问答对追加到该会话的上下文历史中，基于它们是否会改变后续问题的答案。然后，将这个上下文历史传递给模型。这个过程通过延长token限制到达时间来延长会话长度。例如，ls命令显示目录中的所有文件，其输出取决于当前工作目录。为了确保提供正确的输出，任何工作目录更改都会追加到传递给模型的上下文历史中。由于ls对其他命令没有直接的下游影响，所以不需要为未来的问题保存那个问答对。这样做可以节省token。</p>
<p>在honeypot会话中，任何改变上下文的命令都会保存到上下文历史中，从而通过丢弃对未来输入无影响的问答对来节省空间。这个过程还有额外的好处，即减少响应时间，因为在预测下一个序列时需要计算的token间含义和句子结构更少。</p>
<p>然而，一些UNIX命令，如history，需要完整的输入历史记录才能输出正确。如果仅维护如上文所述的上下文历史，那么这些命令所需的完整输入历史记录将无法获得。相反，如果像算法1那样保留所有输入及其输出，将会达到token限制。因此，可以维护两种历史记录：如上文所述的过滤后的问答对上下文历史C来替代算法1中的上下文历史，以及包含会话中所有输入的全局历史H，以便在需要时使用。</p>
<p>两种历史记录都需要随每个新的问题和答案进行维护和更新。这种方法维护两个历史记录的方式提供了使用仅问题历史的选择，如果问答对上下文历史变得太大。虽然仅使用输入可能会有损失，但相对于因为上下文内存耗尽而失去确定性更为可取。这是因为当模型缺乏知识时，它会失败，但可以利用不完整的（有损）知识进行一些推断。全局历史也必须进行维护，以确保它也不超出内存限制。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# D. DESIGN`,
                    msg: String.raw`<div class="markdown-body"><p>The above methodology is formulated to create a model for deterministic interaction when employing generative models such as LLMs as a black box. The proposed model aims to establish a reliable framework for effectively utilizing language models such as GPT as a honeypot backend, ensuring deceptive interaction throughout the process.
The described updating is modeled with the session context history C 1 being updated with the most recent question q and answer a if that question is a member of the set C 0 . C 0 is a set of questions that have the capability to immediately change the context of future questions.
The session global history H 1 is updated with each new question unless H 1 extends to be greater than the max token limit. If the max token limit is exceeded, earlier questions are removed to make room as represented by the Last function. These histories are updated last after answer generation. However, the process is defined here to introduce C 1 and H 1 , which are used for calculating C ′ :
C 1 = (q ∈ C 0 → C 1 ∪ {[q, a]}) ∧ (¬(q ∈ C 0 ) → C 1 ) (4) H 1 = (H 1 ∪ q &lt; MAX → H 1 ∪ q) ∧ (¬(H 1 ∪ q &lt; MAX) → Last(H 1 ∪ q, MAX)) (5)
Whether C 1 or H 1 is used as context for q ′ would be dependent on if a question requires a global history or if the token limit is reached when using the more robust context history. This behavior is modeled as such with C ′ being the chosen history to be used by q ′ when passed to the LLM:
C ′ = (len(C 1 ) &gt; MAX ∨ q ∈ H 0 → H 1 ) ∧ (¬(len(C 1 ) &gt; MAX ∨ q ∈ H 0 ) → C 1 )(6)
q ′ = S ∪ C ′ ∪ q (7)
Once the appropriate history is chosen to be passed to the model as context, the full question can be built using system prompt S, chosen context C ′ , and attacker question q. The fully formulated question is then passed to the model for answer generation. If the generated answer has some breakdown in deception i.e. responding as an ''AI language model'' like in Table 3, that answer needs to be sanitized before being returned. This is handled by the Sanitize function and is modeled as such:
a ′ = LLM(q ′ ) (8) a = Sanitize(a ′ )(9)
Eq. ( 4)-Eq. ( 9) collectively describe the operation of the proposed honeypot. This model is implemented as described below for future evaluation.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# D. DESIGN`,
                    msg: String.raw`<div class="markdown-body"><p>以上方法论旨在创建一个模型，用于在使用生成模型如LLMs作为黑盒时进行确定性交互。所提出的模型旨在建立一个可靠的框架，有效地利用诸如GPT之类的语言模型作为蜜罐后端，确保整个过程中的欺骗性交互。</p>
<p>所描述的更新是基于会话上下文历史C₁，当最新问题q及其答案a属于集合C₀时，更新C₁。C₀是一组具有立即改变未来问题上下文能力的问题。会话全局历史H₁会随着每个新问题的出现而更新，除非H₁超过最大标记限制。如果超过，则通过Last函数删除较早的问题以腾出空间。这些历史在生成答案后进行最后更新。然而，本过程的定义在此引入了C₁和H₁，这些用于计算C′：
[ C₁ = (q ∈ C₀ → C₁ ∪ {[q, a]}) ∧ (¬(q ∈ C₀ ) → C₁) ]
[ H₁ = (H₁ ∪ q &lt; MAX → H₁ ∪ q) ∧ (¬(H₁ ∪ q &lt; MAX) → Last(H₁ ∪ q, MAX)) ]</p>
<p>无论是C₁还是H₁被用作q′的上下文，取决于是否问题需要全局历史，或者是否达到了更稳健的上下文历史的标记限制。这种行为如下建模，C′是传递给LLM的选择历史，用于q′：
[ C′ = (len(C₁) &gt; MAX ∨ q ∈ H₀ → H₁) ∧ (¬(len(C₁) &gt; MAX ∨ q ∈ H₀) → C₁) ]
[ q′ = S ∪ C′ ∪ q ]</p>
<p>一旦选择了适当的历史作为传递给模型的上下文，就可以使用系统提示S、选择的上下文C′和攻击者问题q构建完整问题。然后将完整的问题传递给模型进行答案生成。如果生成的答案在欺骗性方面有所破绽，例如像表3中作为"AI语言模型"的回复，那么需要在返回之前对答案进行清理。这由Sanitize函数处理，如下所示建模：
[ a′ = LLM(q′) ]
[ a = Sanitize(a′) ]</p>
<p>方程(4)到方程(9)共同描述了所提出蜜罐的运作。该模型按照以下描述进行了实现，以便进行未来的评估。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# E. PROPOSED FRAMEWORK`,
                    msg: String.raw`<div class="markdown-body"><p>In the proposed framework, the following six actions are executed for each new attacker input to maintain context and deception for extended interaction efficiently.
1) Select the context history or the more lossy global history for the current question. (Eq. ( 6)) 2) Generate an answer for the question in the chosen context. (Eq. ( 7), Eq. ( 8)) 3) Sanitize answers to maintain the deception. (Eq. ( 9)) 4) Maintain global session history of questions for cases where all questions are needed. (Eq. ( 5)) 5) Maintain context-changing questions and answers in the session context history for future interactions. (Eq. ( 4)) 6) Return the answer to the user. An algorithm implementing the proposed framework is designed. The algorithm examines the question and calls the context-choosing sub-algorithm to return the context required for that question. Once an answer is generated by the LLM, the algorithm calls another sub-algorithm that updates the context and input histories using the question and generated answer.
Algorithm 2 presents a pseudocode to implement the proposed framework. The first output creates the sanitized answer after selecting the context based on whether the provided question q requires a global history as defined by H 0 or if the request exceeds the defined q ← Input()
9:
if q ∈ KillCmds then break 10:</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# E. PROPOSED FRAMEWORK`,
                    msg: String.raw`<div class="markdown-body"><p>在提出的框架中，针对每个新的攻击者输入，执行以下六个步骤以有效地维护上下文和欺骗，支持长时间交互：
1) 选择当前问题的上下文历史或更具损失性的全局历史。（方程式 (6)）
2) 为选择的上下文中的问题生成答案。（方程式 (7), (8)）
3) 清理答案以维持欺骗效果。（方程式 (9)）
4) 维护问题的全局会话历史，适用于需要所有问题的情况。（方程式 (5)）
5) 在会话上下文历史中维护更改上下文的问题和答案，以供未来交互使用。（方程式 (4)）
6) 将答案返回给用户。设计了实施提出框架的算法。该算法检查问题并调用选择上下文的子算法返回该问题所需的上下文。一旦LLM生成了答案，算法调用另一个子算法使用问题和生成的答案更新上下文和输入历史。
算法2展示了实施提出框架的伪代码。首先输出根据问题 q 是否需要由 H 0 定义的全局历史来选择上下文，并创建清理过的答案。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# end if Part-1`,
                    msg: String.raw`<div class="markdown-body"><p>11:
h ′ ← ChooseContext(S, q, C 1 , H 1 , H 0 ) 12:
a ← LLM(S, h ′ , q)
13:
a ← Sanitize(a)
14:
C 1 , H 1 ← UpdateContext(q, a, C 1 , H 1 , C 0 ) 15:</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# end if`,
                    msg: String.raw`<div class="markdown-body"><p>11:
h' ← ChooseContext(S, q, C₁, H₁, H₀)
12:
a ← LLM(S, h', q)
13:
a ← Sanitize(a)
14:
C₁, H₁ ← UpdateContext(q, a, C₁, H₁, C₀)
15:</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# end if Part-2`,
                    msg: String.raw`<div class="markdown-body"><p>SendAnswer(a) 16: end while Algorithm 3 Choose Context Input: S: Persona-defining System Prompt q: Attacker-provided Question
C 1 : {[q ′ , a ′ ] | q ′ generates a ′ } (Session Context History) H 1 : {q ′ | q ′ 0 , . . . , q ′ q-1 } (Session Global History) H 0 : {q | q requires H 1 } Output: Chosen History Set 1: if q ∈ H 0 or len(S, C 1 , q) ≥ MAX_TOKENS then 2:
return H 1 3: end if 4: return C 1 token limit. The global history is updated with each question. The context history is updated with the question and answer if the question is defined by the set of context-changing questions C 0 . This formulation is described explicitly:</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# end if`,
                    msg: String.raw`<div class="markdown-body"><p>SendAnswer(a) 16: 结束当 算法3 选择上下文输入: S: 定义人物的系统提示 q: 攻击者提供的问题
C 1 : {[q ′ , a ′ ] | q ′ 生成 a ′ } (会话上下文历史) H 1 : {q ′ | q ′ 0 , . . . , q ′ q-1 } (会话全局历史) H 0 : {q | q 需要 H 1 } 输出: 选择的历史集合 1: 如果 q ∈ H 0 或者 len(S, C 1 , q) ≥ MAX_TOKENS 则 2:
返回 H 1 3: 结束如果 4: 返回 C 1 token 限制。全局历史随每个问题更新。如果问题由集合 C 0 中定义，则上下文历史也会随问题和答案更新。这种表述被明确描述:</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# end if Part-3`,
                    msg: String.raw`<div class="markdown-body"><p>1) If the input question q is a member of the set H 0 , the set of all questions that require a global history, or if the number of tokens used by the system prompt s, session context history C 1 , and input question q exceeds the MAX_TOKENS for LLM, then the session's global history of questions H 1 is used as context. Otherwise, the session's context history C 1 is used as the chosen context. (Algorithm 3 2) An answer a is generated by the LLM using the query formulated by s, context, q. (Algorithm 3) 3) If the answer a has portions that would potentially jeopardize deception, that answer is sanitized by the Sanitize function to remove said portions.(Algorithm 2 Line 13) 4) Append input question q to session's global history of questions H 1 if that appending does not exceed Algorithm 4 Update Context Input: q: Attacker-provided Question a: Sanitized LLM-generated answer C 1 : {[q ′ , a ′ ] | q ′ generates a ′ } (Session Context History) H 1 : {q ′ | q ′ 0 , . . . , q ′ q-1 } (Session Global History) C 0 : {q | q affects a q+1 } Output: C 1 : {[q ′ , a ′ ] | q ′ generates a ′ } (Updated Session Context History) H 1 : {q ′ | q ′ 0 , . . . , q ′ q-1 } (Updated Session Global History) 1: if len(H 1 ∪ q) &lt; MAX_TOKENS then 2:
H 1 ← H 1 ∪ {q} 3: else 4: H 1 ← Last(H 1 ∪ q, MAX_TOKENS) 5: end if 6: if q ∈ C 0 then 7: C 1 ← C 1 ∪ {[q, a]} 8: end if 9: return C 1 , H 1</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# end if`,
                    msg: String.raw`<div class="markdown-body"><p>1) 如果输入问题 q 属于集合 H 0（需要全局历史记录的所有问题的集合），或者系统提示 s、会话上下文历史 C 1 和输入问题 q 使用的令牌数量超过 LLM 的 MAX_TOKENS，则会话的全局问题历史 H 1 被用作上下文。否则，会话的上下文历史 C 1 被选择作为上下文。（算法 3）</p>
<p>2) LLM 使用由 s、上下文和问题 q 构成的查询生成答案 a。（算法 3）</p>
<p>3) 如果答案 a 的某些部分可能危及欺骗效果，则通过 Sanitize 函数对该答案进行清理，以移除这些部分。（算法 2 第 13 行）</p>
<p>4) 如果附加输入问题 q 到会话的全局问题历史 H 1 不会超过最大令牌数（MAX_TOKENS），则将其附加到会话的全局问题历史 H 1 中。否则，根据算法 4 更新上下文。（输入: q：攻击者提供的问题 a：经过清理的 LLM 生成的答案 C 1：{[q′, a′] | q′ 生成 a′}（会话上下文历史） H 1：{q′ | q′ 0，...，q′ q-1}（会话全局历史） C 0：{q | q 影响 q+1} 输出: C 1：{[q′, a′] | q′ 生成 a′}（更新后的会话上下文历史） H 1：{q′ | q′ 0，...，q′ q-1}（更新后的会话全局历史） 1: if len(H 1 ∪ q) &lt; MAX_TOKENS then 2: H 1 ← H 1 ∪ {q} 3: else 4: H 1 ← Last(H 1 ∪ q, MAX_TOKENS) 5: end if 6: if q ∈ C 0 then 7: C 1 ← C 1 ∪ {[q, a]} 8: end if 9: return C 1，H 1</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# end if Part-4`,
                    msg: String.raw`<div class="markdown-body"><p>the MAX_TOKENS for LLM. Else, remove the earliest questions until under MAX_TOKENS via the Max function. (Algorithm 4 Lines 1-5) 5) If the set of context-changing questions C 0 contains input question q, then the QA pair {[q, a]} is appended to that session's set of context-changing QA pairs C 1 .
(Algorithm 4 Lines 6-8) 6) Answer a is returned to the user while C 1 and H 1 are maintained until the session is terminated. (Algorithm 2 Lines 14-15)
These actions can be formalized as independent actions and checks as formulated in Algorithms 2-4. The controlling portion of the algorithm in Algorithm 2 continuously accepts input from the attacker until a command that would end the sequence is received as defined by KilllCmds. With each input, the required context is chosen to generate each answer. The contexts are then updated as previously formulated based on the provided question and its answer. ChooseContext selects which history to use with the question based on set membership as defined by H 0 . UpdateContexts updates the global and context histories based on actions 4 and 5 to be used for future questions.
The actions taken in the above algorithms are illustrated in Fig. 3. Context handling is implemented in a front-end interface (FEI) made up of an input and output handler. This FEI handles question generation and input curation on behalf of the generating model.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# end if`,
                    msg: String.raw`<div class="markdown-body"><p>以下是第四部分的翻译：</p>
<p>如果LLM的令牌数达到了MAX_TOKENS，则通过Max函数移除最早的问题，直到令牌数低于MAX_TOKENS。 （算法4，第1-5行）。如果上下文变化问题集C0包含输入问题q，则将QA对{[q, a]}附加到该会话的上下文变化QA对集C1中。 （算法4，第6-8行）。当会话终止前，将回答a返回给用户，同时保持C1和H1。 （算法2，第14-15行）。</p>
<p>这些操作可以形式化为独立的操作和检查，如算法2至算法4中所示。算法2中的控制部分持续接受来自攻击者的输入，直到接收到结束序列的命令，如KilllCmds所定义。每次输入时，根据提供的问题选择所需的上下文来生成每个答案。然后根据提供的问题及其答案更新上下文，如之前根据动作4和5所定义，更新全局和上下文历史。ChooseContext基于H0定义的集合成员资格选择用于问题的历史。UpdateContexts更新全局和上下文历史，以便将来使用。</p>
<p>上述算法中采取的行动在图3中有所说明。上下文处理实施在由输入和输出处理器组成的前端界面（FEI）中。这个FEI代表生成模型处理问题生成和输入策划。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 1) INPUT HANDLER`,
                    msg: String.raw`<div class="markdown-body"><p>The input handler accepts input from the attacker on behalf of the model and decides what context to use based on that input and the size of the context history. Once the proper context is chosen, the full question is built and sent to the model. This corresponds with actions 1 and 2 in Algorithm 2. </p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 1) INPUT HANDLER`,
                    msg: String.raw`<div class="markdown-body"><p>输入处理器接受攻击者的输入，并根据该输入和上下文历史的大小决定使用什么上下文。一旦选择了适当的上下文，就会构建完整的问题并发送给模型。这对应于算法2中的步骤1和2。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 2) OUTPUT HANDLER`,
                    msg: String.raw`<div class="markdown-body"><p>The output handler sanitizes the model's output by removing notes or comments that slip through the prompt. The output handler then updates that session's context and global history for future questions before returning the answer to the attacker. This corresponds with actions 3-6 in Algorithm 2.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 2) OUTPUT HANDLER`,
                    msg: String.raw`<div class="markdown-body"><p>输出处理程序通过删除从提示中滑过的注释或备注来清理模型的输出。然后，输出处理程序更新该会话的上下文和全局历史记录，以备将来的查询，并将答案返回给攻击者。这与算法2中的步骤3-6相对应。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 3) FEI EXAMPLE`,
                    msg: String.raw`<div class="markdown-body"><p>A sample session is provided in Table 8. When updating the context history for this example, Directory and file changes such as those in QA 4 and QA 8 are included in the context history. The global history use is shown in QA 10 where only past questions are passed to the model. By only passing context-changing QA pairs to the model and all questions for certain edge cases, 1054 unique tokens are eliminated over the course of short conversation while still maintaining believable output.
The FEI conservatively manages the context and questions to the model on the fly, allowing us to maintain consistency with what the attacker expects while using as few tokens as possible. minimizing token use is paramount due to a memory limit present in GPT [32]. Additionally, for a more robust use of the proposed FEI, the operating environment such as the operating system, hostname, or other data provided to the attacker can be changed by updating the prompt. While this new architecture is a significant improvement over the architecture in Figure 2, it has limitations, such as third-party interaction and cost. These limitations are further summarized in Section V.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 3) FEI EXAMPLE`,
                    msg: String.raw`<div class="markdown-body"><p>在表8中提供了一个示例会话。在更新此示例的上下文历史时，包括类似QA 4和QA 8中的目录和文件更改。全局历史的使用如QA 10所示，只传递过去的问题给模型。通过仅将改变上下文的问答对传递给模型以及对某些边缘情况下的所有问题，可以在短对话过程中消除1054个独特的令牌，同时仍然保持可信的输出。</p>
<p>FEI在运行时谨慎管理上下文和向模型提出的问题，使我们能够在尽可能使用少量令牌的同时保持与攻击者预期一致。由于GPT存在内存限制[32]，最小化令牌使用是至关重要的。</p>
<p>此外，为了更强大地使用提出的FEI，可以通过更新提示更改操作环境，例如操作系统、主机名或其他提供给攻击者的数据。尽管这种新的架构在图2中的架构上有显著改进，但它也存在一些限制，如第三方交互和成本。这些限制在第五节中进一步总结。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# IV. RESULTS AND DISCUSSION`,
                    msg: String.raw`<div class="markdown-body"><p>In this section, the performance of an adaptive LLM context honeypot as opposed to a traditional static interaction honeypot is evaluated. For this comparison, Cowrie is chosen to represent static honeypots of the same risk level due to its widespread use on the internet and its backend terminal emulation [41].
The subsections explore the setup for the following evaluation, single command similarity comparisons, and extended session preservation.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# IV. RESULTS AND DISCUSSION`,
                    msg: String.raw`<div class="markdown-body"><p>在本节中，评估了适应性LLM上下文蜜罐的性能，与传统的静态交互蜜罐进行对比。为此比较，选择了Cowrie来代表同一风险级别的静态蜜罐，因为它在互联网上广泛使用，并且具有后端终端仿真 [41]。</p>
<p>本节的各小节探讨了以下评估的设置，包括单个命令相似性比较和扩展会话保留。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# A. EVALUATION SETUP`,
                    msg: String.raw`<div class="markdown-body"><p>Cowrie is a medium to high-interaction honeypot based on Kippo that captures SSH and Telnet sessions [42]. For the medium interaction version, the backend is implemented via hard-coded responses for 34 commands as of 2018 [5]. The authors state that these commands are chosen due to most attackers only using those commands and that implementing all commands would take too much effort [42]. For the highinteraction version, Cowrie acts as a proxy to another system where it reroutes traffic from the login handler to a secondary high-interaction cyber asset. For the purpose of this evaluation, the LLM honeypot supported by the FEI described in Fig. 3 will be compared to the medium interaction Cowrie configuration. This comparison is chosen due to how both the medium-interaction Cowrie and the LLM honeypot return output without any execution, limiting the risk of misuse.
For the LLM honeypot, OpenAI's gpt-3.5-turbo is used due to its availability, consistency, and sophistication.  The temperature is set to 0 for all inputs. For the extended session, the architecture outlined in Fig. 3 </p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# A. EVALUATION SETUP`,
                    msg: String.raw`<div class="markdown-body"><p>Cowrie是基于Kippo的中高交互蜜罐，用于捕获SSH和Telnet会话[42]。在中交互版本中，后端通过对2018年已实施的34个命令进行硬编码响应来实现[5]。作者指出，选择这些命令是因为大多数攻击者仅使用这些命令，而实现所有命令将耗费过多的精力[42]。在高交互版本中，Cowrie充当另一系统的代理，将流量从登录处理程序重定向到次级高交互网络资产。</p>
<p>为了本次评估，将基于图3中FEI支持的LLM蜜罐与中交互的Cowrie配置进行比较。选择此比较是因为中交互的Cowrie和LLM蜜罐均在返回输出时没有执行任何操作，从而限制了误用的风险。</p>
<p>对于LLM蜜罐，采用了OpenAI的gpt-3.5-turbo，因其可用性、一致性和复杂性。所有输入的温度被设置为0。对于扩展会话，采用了图3中概述的架构。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# is used.`,
                    msg: String.raw`<div class="markdown-body"><p>A Debian 7 Wheezy virtual machine acts as a control since Cowrie in its base configuration mimics that operating system. Although the LLM can mimic any version or terminal via the system prompt, the prompt is set to guide the model to behave as similarly to Cowrie and the control as possible. The feasibility of using an LLM to generate honeypot output is first evaluated by assessing the similarity of outputs for single inputs before comparing deception, interactivity, and token usage in extended sessions in the next subsection.
The algorithms used to calculate these similarities are given where the first calculates a similarity ratio based on the distance and length calculated by the second:</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# is used.`,
                    msg: String.raw`<div class="markdown-body"><p>使用一个Debian 7 Wheezy虚拟机作为控制器，因为Cowrie在其基本配置中模仿该操作系统。尽管LLM可以通过系统提示模仿任何版本或终端，但提示设置为指导模型尽可能与Cowrie和控制器行为相似。首先通过评估单个输入的输出相似性来评估使用LLM生成蜜罐输出的可行性，然后在接下来的子节中比较欺骗性、交互性和扩展会话中的令牌使用情况。
用于计算这些相似性的算法如下，第一个基于第二个计算的距离和长度计算相似比率：</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# B. SINGLE COMMAND COMPARISON Part-1`,
                    msg: String.raw`<div class="markdown-body"><p>For this comparison, the system, filesystem, and perceived external connectivity of each honeypot are evaluated. To test the feasibility of using an LLM to mimic an attackable interface, the outputs for a single command are emulated by Cowrie and the LLM. These outputs are compared to those of a control virtual machine running Debian 7 Wheezy since Cowrie in its base configuration mimics that operating system. However, the LLM can mimic any version or terminal via the prompt. For fairness, the model's prompt is crafted to make the operating system as similar to Cowrie's as possible.
To see which of the two outputs is more similar, the average ratio (denoted as L-ratio) of similarity of the output's Levenshtein distance of ten outputs for each command from both honeypots and the control virtual machine is calculated [43]. The Levenshtein distance quantifies the number of edits required to make two strings identical to one another by deleting, inserting, or replacing characters. The L-ratio measures how similar the two texts are with 1.0 being identical. The  L-ratio and distance are calculated as shown in Algorithms 5 and 6 where the L-ratio of two strings is the size of the larger of the two strings subtracted by the number of edits necessary to make the two strings identical divided by that maximum length. These algorithms are not novel are provided for completeness.
d[i, j] ← min{d[i -1, j] + 1, d[i, j -1] + 1, d[i - 1, j -1] + cost}
The L-ratio for both honeypots to the control virtual machine is calculated. If the L-ratio is higher for one of the two honeypots, it means their output is more similar to that of the control virtual machine. If the command is not implemented or if the LLM reports it is a fake system, it is automatically given a score of 0.0 for the input. The average L-ratio of all inputs is calculated as well as the average inputs accepted by both honeypots as seen in Table 9 and Table 10. These L-ratios are used as coordinates in Fig. 4 where the x value is the L-ratio for the LLM output and the Y value is the L-ratio for the Cowrie output. If the LLM's output is more similar to the control, it will fall below the diagonal (x=y) line and above if Cowrie is more similar. Each command is separated into one of three categories: system, filesystem, and connectivity categories. These are represented by orange dots, purple dots, and green dots respectively. The specific distances and L-ratios for each command can be found in the provided data repository [44].
The system category is made up of commands that are used to gather system information and make system-modifying changes. These include system file reading, package installs, and kernel modifications. The filesystem category is made up of commands that handle filesystem traversal and modifications. These commands include listing, creation, deletion, and access control for files, directories, and links. For connectivity, since both backends are simulating the attacker's commands, no external communication is taking place with the backend with the exception of curl and wget for Cowrie.
Both backends then must simulate this behavior using believable addresses, response time, firewalls, and routing. To test each, A variety of net utilities are used targeting both domain names and IP addresses.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# B. SINGLE COMMAND COMPARISON`,
                    msg: String.raw`<div class="markdown-body"><p>为了进行比较，评估了每个蜜罐的系统、文件系统和感知外部连接。为了测试使用LLM模仿可攻击接口的可行性，通过对Cowrie和LLM模拟单个命令的输出进行比较。这些输出与运行Debian 7 Wheezy的控制虚拟机的输出进行比较，因为Cowrie在其基本配置中模拟该操作系统。然而，LLM可以通过提示模拟任何版本或终端。为了公平起见，模型的提示被设计成使操作系统尽可能与Cowrie的相似。</p>
<p>为了确定哪个输出更相似，计算了每个命令的十个输出的输出Levenshtein距离的相似性的平均比率（称为L-ratio），并与控制虚拟机进行比较。Levenshtein距离量化了通过删除、插入或替换字符使两个字符串相同所需的编辑次数。L-ratio测量两个文本的相似程度，1.0表示完全相同。L-ratio和距离的计算如算法5和6所示，其中两个字符串的L-ratio是较大的字符串的大小减去使两个字符串相同所需的编辑次数，然后除以该最大长度。这些算法并非新颖，仅供完整性参考。</p>
<p>计算了两个蜜罐与控制虚拟机的L-ratio。如果一个蜜罐的L-ratio更高，则意味着其输出更类似于控制虚拟机的输出。如果命令未实现或者LLM报告它是一个假系统，则自动为该输入赋予0.0的分数。计算了所有输入的平均L-ratio，以及两个蜜罐接受的平均输入，如表9和表10所示。这些L-ratio用作图4中的坐标，其中x值是LLM输出的L-ratio，y值是Cowrie输出的L-ratio。如果LLM的输出更类似于控制，则它将落在对角线（x=y）下方，如果Cowrie更相似，则在上方。每个命令被分为三类：系统、文件系统和连接类别。这些分别由橙色点、紫色点和绿色点表示。每个命令的具体距离和L-ratio可以在提供的数据存储库中找到。系统类别由用于收集系统信息和进行系统修改的命令组成。这些包括系统文件读取、软件包安装和内核修改。文件系统类别由处理文件系统遍历和修改的命令组成。这些命令包括文件、目录和链接的列出、创建、删除和访问控制。对于连接类别，由于两个后端都在模拟攻击者的命令，因此除了Cowrie的curl和wget外，不会与后端进行任何外部通信。然后，必须使用可信的地址、响应时间、防火墙和路由来模拟这种行为。为了测试每个，使用了各种针对域名和IP地址的net实用工具。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# B. SINGLE COMMAND COMPARISON Part-2`,
                    msg: String.raw`<div class="markdown-body"><p>As can be seen in Fig. 4, Table 9, and Table 10, the LLM honeypot had a higher L-ratio 70.5% of the time over Cowrie when compared to the control's output for all inputs, indicating a higher level of similarity to a real machine for single commands. The average L-ratio for all inputs was 0.444 for Cowrie and 0.617 for the LLM. It's worth noting that the LLM honeypot can emulate outputs for any valid command input while medium-interaction Cowrie can only emulate 34 commands in its default configuration. This allows the LLM honeypot to provide a greater level of interaction than Cowrie. For fairness, the L-ratios for inputs implemented by both honeypots are compared separately. The LLM had a higher L-ratio 64.9% of the time with an average L-ratio of 0.601 and Cowrie having an average of 0.552.
System command outputs from the LLM had a higher Lratio 80.6% of the time with Cowrie having an average L-ratio of 0.329 and the LLM having an average L-ratio of 0.6465. This was expected since Cowrie has limited system command support. For commands implemented by both, Cowrie had an average L-ratio for system commands of 0.511 and the LLM of 0.568.
Filesystem commands had a more incremental improvement with 59% of LLM commands having a higher L-ratio than Cowrie with the average L-ratios being 0.551 and for Cowrie and LLM respectively. This incremental improvement is due to most of Cowrie emulates being filesystem commands. For commands implemented by both, Cowrie had an average L-ratio for filesystem commands of 0.566 and the LLM of 0.606.
Connectivity commands were more surprising with 79.2% having a higher LLM L-ratio with the average L-ratios being 0.419 and 0.594 for Cowrie and LLM respectively. Actually downloading with Curl and Wget gave Cowrie a higher ratio since it actually executes the request. However, other commands such as ip were not implemented but should have been for a base system. IPTables was unable to handle and display changes. Netstat displayed the same output with different options. For commands implemented by both, Cowrie had an average L-ratio for connectivity commands of 0.572 and the LLM of 0.633.
Several factors contribute to the outliers observed. Firstly, Cowrie actually executes Curl and Wget commands, while the LLM only emulates them, resulting in L-ratios being skewed towards Cowrie in that specific circumstance. This aspect makes Cowrie more advantageous for malware capturing, as it can provide real-time logging of captured artifacts. However, it is worth noting that the capturing capabilities of the LLM honeypot can be extended in future work to include downloading by the FEI when these specific commands come up, reducing the disparity between the two honeypots.
Secondly, the LLM exhibited undesirable behavior with certain commands due to the use of Debian 7 in the prompt.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# B. SINGLE COMMAND COMPARISON`,
                    msg: String.raw`<div class="markdown-body"><p>正如图4、表9和表10所示，与所有输入的控制输出相比，LLM蜜罐有70.5%的时间内的L-ratio更高于Cowrie，表明在单个命令中与真实机器的相似度更高。所有输入的平均L-ratio为Cowrie为0.444，LLM为0.617。值得注意的是，LLM蜜罐可以模拟任何有效命令输入的输出，而中等交互式的Cowrie在其默认配置下只能模拟34个命令。这使得LLM蜜罐能够提供比Cowrie更高的交互水平。为了公平起见，分别比较了两个蜜罐实现的输入的L-ratio。LLM在64.9%的时间内具有更高的L-ratio，平均L-ratio为0.601，而Cowrie的平均值为0.552。</p>
<p>LLM的系统命令输出有80.6%的时间内具有更高的L-ratio，Cowrie的平均L-ratio为0.329，而LLM的平均L-ratio为0.6465。这是预期的，因为Cowrie的系统命令支持有限。对于两者都实现的命令，Cowrie的系统命令平均L-ratio为0.511，LLM为0.568。</p>
<p>文件系统命令有更大幅度的改进，59%的LLM命令具有比Cowrie更高的L-ratio，分别为Cowrie和LLM的平均L-ratio为0.551和0.606。这种增量改进是因为Cowrie模拟的大部分是文件系统命令。对于两者都实现的命令，Cowrie的文件系统命令平均L-ratio为0.566，LLM为0.606。</p>
<p>连接命令则更令人惊讶，79.2%的LLM L-ratio更高，Cowrie和LLM的平均L-ratio分别为0.419和0.594。实际使用Curl和Wget下载给Cowrie带来了更高的比率，因为它实际上执行了请求。然而，其他命令如ip未实现，但应该是基本系统的一部分。IPTables无法处理和显示更改。Netstat显示了不同选项的相同输出。对于两者都实现的命令，Cowrie的连接命令平均L-ratio为0.572，LLM为0.633。</p>
<p>观察到的异常值是由多个因素共同导致的。首先，Cowrie实际上执行Curl和Wget命令，而LLM只是模拟它们，导致L-ratio在这种特定情况下偏向于Cowrie。这一方面使得Cowrie在捕获恶意软件方面更有优势，因为它可以提供捕获到的工件的实时日志记录。然而，值得注意的是，在未来的工作中，LLM蜜罐的捕获功能可以通过在出现这些特定命令时由FEI进行下载来扩展，从而减少两个蜜罐之间的差异。</p>
<p>其次，由于使用了Debian 7提示符，LLM对某些命令表现出了不良行为。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# B. SINGLE COMMAND COMPARISON Part-3`,
                    msg: String.raw`<div class="markdown-body"><p>For instance, ifconfig, being deprecated, was not emulated, and the enable command was unrecognized. These issues can be alleviated through prompt refinement. Thirdly, the LLM occasionally generated excessive output or went offtopic, particularly when dealing with commands that had long outputs like dmesg and ps, resulting in the token limit being reached. This ''hallucinatory rambling'' is further discussed in Section V.
To conclude the single-command evaluation, the Levenshtein distance of outputs was used to calculate a similarity ratio of honeypot output to real system output as an initial indicator of deception. Despite the limitations and outliers mentioned, the evaluation results demonstrate the proposed honeypot's advantages over Cowrie in terms of emulating realistic machine behavior for single commands across all categories. The potential for greater attacker interaction and favorable L-ratios observed in the majority of cases further support this conclusion. Improvements such as prompt refinement, sanity checks, caching outputs, and exploring an extended architecture can be used to mitigate limitations and outliers for a honeypot where a language model to generate its output is used. While the L-ratio is not a perfect metric and different outputs may not necessarily indicate incorrect output, it serves as a valuable starting point for evaluating single command outputs and finding similarities in output formats.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# B. SINGLE COMMAND COMPARISON`,
                    msg: String.raw`<div class="markdown-body"><p>例如，已经停用的 ifconfig 命令未被模拟，而 enable 命令也未被识别。这些问题可以通过提示的优化来缓解。第三，LLM 偶尔生成过多的输出或者离题，特别是处理像 dmesg 和 ps 这样有长输出的命令时，会导致达到令牌限制。这种“幻觉般的闲扯”在第五节进一步讨论。</p>
<p>总结单个命令评估，使用编辑距离来计算蜜罐输出与真实系统输出的相似性比率，作为欺骗性的初步指标。尽管存在提到的限制和异常值，评估结果显示，所提议的蜜罐在模拟各类单个命令的实际机器行为方面，相较于 Cowrie 有显著优势。在大多数情况下观察到的更大的攻击者交互潜力和有利的 L-比率进一步支持这一结论。可以采用优化提示、健全性检查、缓存输出以及探索扩展架构等改进措施，用于减少使用语言模型生成输出的蜜罐的限制和异常值。虽然 L-比率不是完美的度量标准，而且不同的输出不一定表示不正确的输出，但它作为评估单个命令输出和查找输出格式相似性的有价值起点。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# C. EXTENDED SESSION INTEGRITY COMPARISON`,
                    msg: String.raw`<div class="markdown-body"><p>To determine if language models can be used for extended attacker deception, the number of interactions it takes for a honeypot session to break down is compared. This comparison is conducted between static low-risk honeypots and two LLM-based honeypots. Cowrie is chosen to represent traditional low-risk honeypots due to its widespread use and availability as well as its use in the previous evaluation [41]. The first LLM honeypot is implemented using gpt-3.5-turbo with the previously used system prompt that saves all questions and answers as context as a base honeypot (Fig. 2). This LLM honeypot is compared to one augmented with an FEI to represent the proposed implementation as detailed in Fig. 3. The number of tokens used in both LLM setups throughout each session is measured to measure how many tokens the FEI saved and if that affects overall deception.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# C. EXTENDED SESSION INTEGRITY COMPARISON`,
                    msg: String.raw`<div class="markdown-body"><p>为了确定语言模型是否可以用于延长攻击者误导，我们比较了蜜罐会话破裂所需的交互次数。这一比较是在静态低风险蜜罐和两种基于LLM的蜜罐之间进行的。Cowrie被选为传统低风险蜜罐的代表，因为它被广泛使用且易于获取，并且在先前的评估中已经使用过[41]。第一个LLM蜜罐使用gpt-3.5-turbo实现，系统提示如前所述保存所有问题和答案作为上下文作为基础蜜罐（见图2）。这个LLM蜜罐与一个增加了FEI的蜜罐进行比较，以代表详细说明如图3所示的提议实施方案。在每个会话期间测量了两种LLM设置使用的令牌数量，以评估FEI保存的令牌数量及其是否影响整体误导效果。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 1) SESSION PROGRESS`,
                    msg: String.raw`<div class="markdown-body"><p>To evaluate interaction and deception preservation for attack sessions, five attacker scenarios for the three honeypot setups are followed to see how long it takes for each honeypot's deception to break down. Each scenario contains tactics and techniques listed in the MITRE ATT&amp;CK matrix framework [12], which are outlined in our data repository [44].
The scenarios used for evaluation are system reconnaissance (SR), data obfuscation (DO), lateral propagation (LP), persistence (PE), and exfiltration (EX). These scenarios are chosen due to their relevance to real-world cybersecurity  It should be noted that the command sequences were not chosen with the intention of causing a breakdown for any of the honeypots. If a command fails and there is a suitable alternative available, that alternative is used for that step in the scenario instead of the intended input. This is done so long as the data needed for the following commands are given with the substitute i.e. ip addr and ifconfig. If no suitable substitution can be done, the scenario ends early. Therefore, this test measures the most favorable outcome for each scenario for each setup.
Each attack scenario has nine steps. In each step, one or more Linux Bash shell commands, scripts, or a combination of them are run. The intended commands used to accomplish each tactic in the scenario are further detailed in our data repository [44].
The SR scenario includes steps attackers might take to scout a recently compromised system, such as identifying the operating system, processor, services, disk usage, paths, open ports, log files, the passwd file, and active users. The DO scenario encompasses both the inputs that an attacker might employ to disrupt the use of files on a system and the methods they employ to conceal their actions. The LP scenario includes network reconnaissance, shellcode execution, and basic user authentication on a secondary system as well as commands to verify their actions. The PE scenario is made up of commands to ensure repeat access to the compromised system via backdoors and new user accounts. The DR scenario includes commands to search for and exfiltrate interesting files using different methods.
The number of steps each honeypot is able to execute in the sequence is measured, stopping when all possible inputs are rejected, an output is so unbelievable a rational attacker would not continue, or the scenario concludes. Each bar graph in Fig. 5 quantifies the number of steps achieved in each scenario with orange being Cowrie, green being the selectivecontext FEI-assisted LLM honeypot outlined at the end of Section III, and purple being an LLM honeypot that saves all context. For the honeypots using LLMs, the same system prompt is used as in the single command evaluation.
As can be seen in Fig. 5, the LLM and FEI-augmented LLM outlasted Cowrie in every scenario, completing three of the five while maintaining context. Cowrie was able to fully complete the SR scenario but fell short for the other four.
For data obfuscation and ransomware (DO), Cowrie failed when attempting to create and encrypt the archive whereas both LLMs made it to the end. However, later in that scenario, upon executing the history command, both LLMs failed to give proper output after clearing with history -c.
For scanning and lateral propagation (LP), Cowrie was unable to execute a for loop to ping all addresses on the subnet in the provided environment, cutting the interaction short. Both LLM honeypots were able to simulate this interaction. Later in the LP scenario, both the LLM and FEI-augmented LLM failed to execute an internet-fetched shell script and identified themselves as an AI agent.
For the persistence (PE) scenario, both LLM honeypots were able to complete the session while Cowrie failed to set up a Netcat backdoor that would start a shell upon connection.
Lastly, for DR, Cowrie failed when attempting to compress a file for exfiltration whereas the LLM was able to do so but failed when trying to view the password shadow file, citing invalid permissions. While the LLM honeypots had some mistakes in later outputs, they were able to emulate an attackable environment for longer than or for the same length as Cowrie.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 1) SESSION PROGRESS`,
                    msg: String.raw`<div class="markdown-body"><p>为了评估攻击会话的互动和欺骗保持情况，本文按照三种蜜罐设置进行了五种攻击者场景的评估，以查看每种蜜罐的欺骗效果何时会被揭穿。每个场景包含在MITRE ATT&amp;CK矩阵框架[12]中列出的策略和技术，详细列在我们的数据仓库中[44]。</p>
<p>评估使用的场景分别是系统侦察（SR）、数据混淆（DO）、横向传播（LP）、持久化（PE）和数据外泄（EX）。选择这些场景是因为它们与现实中的网络安全具有相关性。需要注意的是，命令序列并非旨在导致任何蜜罐失败。如果一个命令失败，并且存在适当的替代方案，则在该步骤中使用替代方案而不是预期的输入。只要提供了下一个命令所需的数据，比如IP地址和ifconfig，就可以进行替换。如果找不到合适的替代方案，则场景会提前结束。因此，这项测试衡量了每种设置中每种场景的最有利结果。</p>
<p>每个攻击场景包括九个步骤。在每个步骤中，运行一个或多个Linux Bash shell命令、脚本或它们的组合。用于完成每个策略的预期命令在我们的数据仓库[44]中进一步详细说明。</p>
<p>SR场景包括攻击者可能采取的步骤来侦察最近被攻击系统，如识别操作系统、处理器、服务、磁盘使用情况、路径、开放端口、日志文件、passwd文件和活动用户。DO场景涵盖了攻击者可能使用的输入来破坏系统文件的使用以及他们掩盖行动的方法。LP场景包括网络侦察、shellcode执行和在次要系统上进行基本用户验证以及验证他们操作的命令。PE场景由命令组成，以确保通过后门和新用户帐户重复访问被攻击系统。DR场景包括搜索和使用不同方法外泄有趣文件的命令。</p>
<p>测量每个蜜罐能够按顺序执行的步骤数量，在所有可能的输入被拒绝、输出不可信以至于理性攻击者不会继续，或者场景结束时停止。图5中的每个条形图定量化了每种场景中各个蜜罐实现的步骤数，橙色代表Cowrie，绿色代表在第三节末尾概述的具有选择性上下文FEI辅助LLM蜜罐，紫色代表保存所有上下文的LLM蜜罐。对于使用LLM的蜜罐，与单个命令评估中使用的系统提示相同。</p>
<p>如图5所示，LLM和FEI辅助LLM在每个场景中都比Cowrie持久更长时间，并完成了其中三种场景并保持了上下文。Cowrie能够完全完成SR场景，但在其他四个场景中表现不佳。</p>
<p>对于数据混淆和勒索软件（DO），Cowrie在试图创建和加密存档时失败，而两种LLM都能够完成。然而，在此场景后期，执行history命令时，两种LLM在清除历史记录（history -c）后未能给出适当的输出。</p>
<p>对于扫描和横向传播（LP），Cowrie无法执行在提供的环境中对子网上所有地址进行ping的for循环，从而提前结束了互动。两种LLM蜜罐能够模拟这种互动。在LP场景后期，LLM和FEI辅助LLM都未能执行从互联网获取的shell脚本，并且识别自己为AI代理。</p>
<p>对于持久化（PE）场景，两种LLM蜜罐能够完成会话，而Cowrie则无法设置Netcat后门以在连接时启动shell。</p>
<p>最后，在DR场景中，Cowrie在试图压缩文件以进行外泄时失败，而LLM能够成功，但在尝试查看密码影子文件时由于权限无效而失败。虽然LLM蜜罐在后续输出中存在一些错误，但它们能够模拟一个可攻击环境比Cowrie更长或相同的时间。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 2) SESSION TOKEN USE`,
                    msg: String.raw`<div class="markdown-body"><p>To measure the effectiveness and retention of only passing context-changing interactions to the LLM as chosen by an FEI, the number of tokens used for each step in each scenario is measured. Both implementations started by using 70 tokens for the system prompt. If an answer is determined to change the operating context for the session, that command input and output are used as a QA pair and included when passed to the model for future questions. If the session breaks deception before the scenario is concluded, the measurement of tokens will end on that step. This will be one step further than seen in Fig. 5 as tokens will be used to generate the deceptionbreaking answer. The FEI-augmented LLM honeypot was able to maintain interaction with the user for the same amount of steps as the LLM honeypot that saved all context while using up to 77.26% fewer tokens by the end of a session as seen by the system reconnaissance scenario in graph (a) in Fig. 6. On average across the five scenarios, the FEI-augmented LLM honeypot used 62.17% fewer tokens.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 2) SESSION TOKEN USE`,
                    msg: String.raw`<div class="markdown-body"><p>为了衡量只将由FEI选择的改变上下文的交互传递给LLM以提高有效性和保留率，对每个场景中每个步骤使用的令牌数量进行了测量。两种实现均从系统提示开始使用70个令牌。如果确定某个答案改变了会话的操作上下文，则该命令输入和输出将作为问答对包含在传递给模型的未来问题中。如果会话在场景结束前破坏了欺骗，那么令牌的测量将在该步骤结束。这将比图5中所见的更进一步，因为令牌将用于生成破坏欺骗的答案。通过图6中的图(a)中的系统侦察场景可以看出，FEI增强的LLM蜜罐能够与用户保持相同步骤数量的交互，同时在会话结束时比保存所有上下文的LLM蜜罐使用了多达77.26%的较少令牌。在五个场景中的平均值显示，FEI增强的LLM蜜罐使用了比例减少了62.17%的令牌。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# D. DISCUSSION`,
                    msg: String.raw`<div class="markdown-body"><p>Results from the evaluations show that a honeypot using an LLM as its backend is able to better emulate outputs than traditional honeypots of a similar level of risk (attacker using the honeypot outside the accepted scope). the LLM honeypot was also able to maintain attack sessions that used standard tools for the operating system it emulated at a high rate. The FEI proposed in Section III was able to reduce the number of tokens used over the course of a session while still maintaining deception. However, several limitations that may limit the use of LLMs for cyber deception were encountered.
In this work, a study on the effectiveness of LLMs for cyber deception is provided. Past work in using LLMs for attacker deception has proposed the idea of using LLMs to emulate terminal behavior [7], [25] or to generate artifacts to prolong attacker engagement [39]. A simple and elegant architecture is designed and evaluated. This architecture can be easily implemented while lowering the detection probability even compared to commonly used and well-designed medium-interaction honeypots such as Cowrie. The proposed architecture reduced token use by anywhere from 11-77% for attack scenarios that were simulated to completion when compared to a base architecture [25].</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# D. DISCUSSION`,
                    msg: String.raw`<div class="markdown-body"><p>评估结果显示，使用LLM作为后端的蜜罐能够比风险水平类似的传统蜜罐更好地模拟输出（攻击者在接受范围外使用蜜罐）。LLM蜜罐还能够以高频率维持使用其仿真操作系统标准工具的攻击会话。第三节中提出的FEI在会话过程中能够减少使用的标记数量，同时仍保持欺骗性。然而，我们遇到了几个可能限制LLM在网络欺骗中使用的局限性。</p>
<p>本研究提供了关于LLM在网络欺骗中有效性的研究。以往在使用LLM进行攻击者欺骗方面的工作中，提出了使用LLM模拟终端行为[7]，[25]或生成物件以延长攻击者参与的想法[39]。我们设计并评估了一个简单而优雅的架构。即使与像Cowrie这样常用且设计良好的中等交互蜜罐相比，这种架构也可以轻松实施，并降低检测概率。在模拟完成的攻击场景中，与基础架构[25]相比，所提出的架构将标记使用减少了11%至77%不等。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# V. LIMITATIONS`,
                    msg: String.raw`<div class="markdown-body"><p>Generative models used for cyber deception can change the operating environment and threat surface provided to the attacker by providing relevant context in the prompt. This capability makes them well-suited for threat engagement which can evolve as new threats emerge. However, such a use case requires addressing several limitations inherent to the design, technology, and implementation derived from the specific model used.
These limitations can be categorized by whether they're inherent to the design of the underlying transformer architecture or due to the model used. Both are explored more in-depth in the following subsections. With each limitation, a discussion of possible solutions to mitigate the severity when deploying honeypots using LLMs are provided.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# V. LIMITATIONS`,
                    msg: String.raw`<div class="markdown-body"><p>生成模型用于网络欺骗可以通过在提示中提供相关上下文，改变向攻击者提供的操作环境和威胁表面。这种能力使它们非常适合于随着新威胁的出现而发展的威胁参与。然而，这种用例需要解决几个固有于设计、技术和特定模型实现的限制。
这些限制可以根据它们是固有于底层变压器架构的设计还是由于所使用的模型而进行分类。接下来的各小节会更深入地探讨这两种限制。针对每个限制，本文提供了在部署使用LLM模型的蜜罐时减轻其严重性的可能解决方案讨论。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# A. DESIGN LIMITATIONS`,
                    msg: String.raw`<div class="markdown-body"><p>These limitations are inherent to the use of models that generate output using the self-attention mechanism and will be present no matter which model is used. Each limitation presented is summarized and accompanied with mitigations to limit or eliminate their impact.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# A. DESIGN LIMITATIONS`,
                    msg: String.raw`<div class="markdown-body"><p>这些限制是由使用自注意力机制生成输出的模型固有的，并且不论使用哪种模型都会存在。每个提出的限制都被总结，并伴随着限制或消除它们影响的缓解方法。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 1) DETERMINISTIC OUTPUT`,
                    msg: String.raw`<div class="markdown-body"><p>Using LLMs for something deterministic where for an input, the output should be the same can lead to some issues. This is due to how LLMs calculate the next token in a sequence using few-shot learning [30], [45]. If token probabilities are statistically close, variations in output may occur. This difference can cause further changes down the line as future inputs will use that discrepancy when calculating subsequent sequences in that output [46].
To mitigate this shortcoming, executing the same command multiple times and taking the most common output will find the most common occurrence in the parts of the output that have discrepancies. However, this method comes with increased cost and reduced performance and responsiveness. Alternatively, finding where discrepancies are common i.e. in version numbers, and masking those based on the previous context will increase the deterministicness of the honeypot. Li et al. take this approach by guiding the model to learn the deterministic relationship between masked content and the rest of the content to capture factual knowledge [47].</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 1) DETERMINISTIC OUTPUT`,
                    msg: String.raw`<div class="markdown-body"><p>使用LLMs进行确定性输出时，对于一个输入，输出应该是相同的可能会出现一些问题。这是因为LLMs使用少样本学习来计算序列中的下一个标记 [30], [45]。如果标记的概率在统计上接近，可能会导致输出的变化。这种差异会在未来的输入中使用，影响计算该输出中后续序列的变化 [46]。</p>
<p>为了减少这种缺陷，多次执行相同命令并选择最常见的输出可以找出具有差异的输出部分中最常见的出现情况。然而，这种方法会增加成本并降低性能和响应速度。另一种方法是找出差异通常存在的地方，例如版本号，并基于先前的上下文对其进行屏蔽，从而增加蜜罐的确定性。Li等人采用这种方法，通过引导模型学习被屏蔽内容与其余内容之间的确定性关系来捕捉事实知识 [47]。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 2) HALLUCINATION, RAMBLING, AND MISBEHAVING`,
                    msg: String.raw`<div class="markdown-body"><p>Another limitation can be occasionally found in the text generation for longer outputs which can lead to detection. These come up in the form of ''hallucinatory rambling,'' which occurs when the model makes something up and becomes stuck in an output loop until the token limit is reached. In addition to detecting wrong output, it takes much longer for the LLM to output a rambling answer. To mitigate this, the long response time can be used to detect when rambling occurs and reset with altered context.
Additionally, in testing with GPT3.5, the model would occasionally misbehave with certain questions where it added additional notes or comments at the end of answers when told not to in the prompt. If this occurs and that answer is used as context for future questions, future answers are likely to contain those notes or comments as well. A Sanitization step like the one used in Fig. 3 to remove notes or comments is crucial for maintaining the deception.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 2) HALLUCINATION, RAMBLING, AND MISBEHAVING`,
                    msg: String.raw`<div class="markdown-body"><p>另一个限制偶尔出现在生成较长输出的文本中，可能导致被检测到。这种情况表现为“幻觉性胡言乱语”，即模型编造内容并在达到标记限制之前陷入输出循环。除了检测错误的输出外，长时间才能输出胡言乱语的答案。为了减轻这一问题，可以利用长时间的响应来检测胡言乱语的发生，并通过修改上下文来进行重置。</p>
<p>此外，在对GPT3.5进行测试时，模型偶尔会在某些问题上表现不当，例如在不应该在提示中添加额外注释或备注的情况下，仍然在答案结尾添加。如果发生这种情况并且该答案被用作未来问题的上下文，未来的答案很可能也包含那些注释或备注。像图3中使用的清理步骤一样，去除注释或备注对于保持欺骗性至关重要。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 3) EXTERNAL COMMUNICATION`,
                    msg: String.raw`<div class="markdown-body"><p>Another possibility of detection comes with the interaction between the honeypot and some observable infrastructure under the attacker's control. Such scenarios include starting a session with a C&amp;C server, downloading malware, or exfiltrating data. Since no traffic is generated, the attacker can determine that their commands are not being executed and that they are in a honeypot. These detection scenarios can't be addressed without help from other technologies such as communication handlers and sandboxes. To mitigate this, a communication handler can be used to spoof the communication on behalf of the generating model.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 3) EXTERNAL COMMUNICATION`,
                    msg: String.raw`<div class="markdown-body"><p>另一种检测可能性涉及蜜罐与攻击者控制下的某些可观察基础设施之间的交互。这些情景包括与C&amp;C服务器启动会话、下载恶意软件或外泄数据。由于没有产生流量，攻击者可以确定他们的命令未被执行，从而确认自己处于一个蜜罐中。这些检测场景如果没有其他技术（如通信处理器和沙箱）的帮助，无法解决。为了减轻这一问题，可以使用通信处理器代表生成模型欺骗通信。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# B. IMPLEMENTATION LIMITATIONS`,
                    msg: String.raw`<div class="markdown-body"><p>These limitations were encountered due to the specific model used and may or may not be present when using different models. GPT3.5 is implemented as an example use case in Section III. However, there are some constraints to using OpenAI's chat completion models that can have implications that bring to light further limitations.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# B. IMPLEMENTATION LIMITATIONS`,
                    msg: String.raw`<div class="markdown-body"><p>这些限制是由于使用的具体模型而遇到的，使用不同模型时可能存在也可能不存在。GPT3.5被作为第III节的一个示例用例来实现。然而，使用OpenAI的聊天完成模型存在一些限制，这些限制可能会揭示出进一步的局限性。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 1) COST`,
                    msg: String.raw`<div class="markdown-body"><p>Compared to GPT3.5, GPT4 costs 15x-30x for prompts and 30x-60x as much for completion. For a research deployment where thousands of attacks can be observed daily, costs can quickly add up when each command can possibly return thousands of tokens. With this in mind, it's imperative to appear as a real system because if an attacker determines the honeypot is implemented using a pay-as-you-go model, they can maximize context by executing commands that have large outputs such as file reading or recursive directory listing.
When effectively utilized, the proposed honeypot exhibits a considerable reduction in deployment expenses, as evidenced by the lower token usage observed in the attack scenarios in Section IV. However, if the goal is to attract a large number of hosts, the cost per token is still a limiting factor. To overcome this, a locally trained and hosted language model could be used.
In total, around 240K tokens were used to develop the FEI using gpt-3.5-turbo. Evaluation consumed significantly more tokens due to each instruction being executed 10 times to compute the average Levenshtein distances. On the other hand, testing the five different MITRE ATT&amp;CK scenarios only consumed 50,840 tokens. Of these, the FEI-assisted model used 47.34% fewer tokens than the base LLM honeypot.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 1) COST`,
                    msg: String.raw`<div class="markdown-body"><p>相较于GPT-3.5，使用GPT-4进行提示的成本增加了15到30倍，完成任务的成本增加了30到60倍。在每日观察数千次攻击的研究部署中，当每个命令可能返回成千上万个令牌时，成本会迅速累积。因此，重要的是要表现得像一个真实系统，因为如果攻击者确定蜜罐是使用按需付费模型实现的，他们可以通过执行输出大的命令（如文件读取或递归目录列表）来最大化上下文信息。</p>
<p>有效利用时，所提出的蜜罐在部署费用上表现出显著降低，正如第四部分中攻击场景中观察到的较低令牌使用量所证实的。然而，如果目标是吸引大量主机，每个令牌的成本仍然是一个限制因素。为了克服这一问题，可以使用本地训练和托管的语言模型。</p>
<p>总计，使用gpt-3.5-turbo开发FEI（欺骗环境信息）大约使用了240,000个令牌。由于每条指令执行10次以计算平均Levenshtein距离，评估消耗了更多的令牌。另一方面，测试五种不同的MITRE ATT&amp;CK场景仅消耗了50,840个令牌。其中，FEI辅助模型使用的令牌比基础LLM蜜罐少了47.34%。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 2) MEMORY LIMIT`,
                    msg: String.raw`<div class="markdown-body"><p>OpenAI's chat completion models have a limit on how many tokens can be supplied per query. This is due to memory limitations inherent to transformer-based models [48]. This limit applies to both prompt and response so some space is required to be left to not cut off the response. Models with larger memory are generally more expensive to train, meaning it's more expensive for the user as seen with GPT-4 being more expensive than GPT-3.5 [33], [49]. Using a reduced memory model, if only relevant context is supplied per each command as detailed in Section III, this should cut down on costs and extend the session by a large margin.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 2) MEMORY LIMIT`,
                    msg: String.raw`<div class="markdown-body"><p>OpenAI的聊天完成模型在每个查询中可以提供的令牌数量上有限制。这是由于基于Transformer的模型固有的内存限制 [48]。这个限制同时适用于提示和响应，因此需要留出一些空间以避免截断响应。通常情况下，具有更大内存的模型训练成本更高，这意味着用户成本更高，正如GPT-4比GPT-3.5更昂贵所示 [33], [49]。如第III节所述，如果每个命令只提供相关上下文来使用减少内存的模型，这应该能大幅降低成本并延长会话时间。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 3) RESPONSINVENESS`,
                    msg: String.raw`<div class="markdown-body"><p>To ensure efficient live usage and maintaining of deception, it is crucial to minimize the time required to generate output for certain inputs. Currently, generating output for specific inputs can take seconds or even minutes, which is impractical for real-time interactions. Therefore, it is essential to significantly reduce the response time to less than a second.
To accomplish this, a caching mechanism within the Front-End Interface (FEI) could be used. By caching large outputs in the FEI, the system can swiftly retrieve and send the pre-generated responses instead of waiting for the LLM to generate them on the spot. Another solution is to use a local model with characteristics put in place in the training stage to limit long outputs.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 3) RESPONSINVENESS`,
                    msg: String.raw`<div class="markdown-body"><p>为了确保高效的实时使用和维持欺骗，关键在于减少特定输入的输出生成时间。目前，生成特定输入的输出可能需要几秒甚至几分钟，这对于实时交互来说是不切实际的。因此，显著减少响应时间至少降低到一秒以下是至关重要的。</p>
<p>为了实现这一目标，在前端界面（FEI）中可以使用缓存机制。通过在FEI中缓存大型输出，系统可以迅速检索并发送预生成的响应，而不是等待LLM即时生成。另一个解决方案是使用本地模型，在训练阶段设置特征以限制长时间输出。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 4) TRAINING BIAS`,
                    msg: String.raw`<div class="markdown-body"><p>GPT3.5 has only been trained with data up to 2021 [50]. The impact of this cutoff is that any bias present in the training data will be inherited by the model such as outdated log contents or lack of modern interactions [10]. This bias can lead to a breakdown in deception if an attacker uses a modern package or tool with critical usage implemented after that cutoff. So long as these biases are internally consistent, this can be managed. However, it may limit what devices and technologies the honeypot can masquerade as. Because of this limit, evolution with the attacker is also hampered. To mitigate this, a more up-to-date model can be used.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 4) TRAINING BIAS`,
                    msg: String.raw`<div class="markdown-body"><p>GPT3.5仅使用了截至2021年的数据进行训练[50]。这一截止日期的影响是，训练数据中存在的任何偏差都会被模型继承，例如过时的日志内容或缺乏现代交互 [10]。如果攻击者使用了在此截止日期之后实施的现代软件包或工具，这种偏差可能导致欺骗失败。只要这些偏差在内部保持一致，就可以进行管理。然而，这可能限制了蜜罐可以伪装成的设备和技术。由于这种限制，与攻击者的演变也受到阻碍。为了减轻这一问题，可以使用更为更新的模型。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# 5) ETHICAL CONSTRAINTS`,
                    msg: String.raw`<div class="markdown-body"><p>OpenAI has implemented safeguards to prevent its models from returning malicious or otherwise harmful responses. Though the use case of a honeypot is not intentionally malicious, questions from the attacker may be categorized as such. When attempting to view log files that contain sensitive information, the deception may also fail as seen in Table 11 TABLE 11. gpt-3.5-turbo Terminal breakdown -sensitive output [11] [The warning is unable to be suppressed even with an appropriately worded system prompt.].
due to moderation policies put in place. To mitigate this, sanitizing self-reporting output and returning error codes can be done to present the illusion of a faulty system rather than a honeypot.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# 5) ETHICAL CONSTRAINTS`,
                    msg: String.raw`<div class="markdown-body"><p>OpenAI已经采取了措施，防止其模型返回恶意或其他有害的响应。虽然蜜罐的使用案例并非故意恶意，但攻击者的问题可能被归类为恶意。当尝试查看包含敏感信息的日志文件时，如表11所示，欺骗也可能会失败。表11显示了gpt-3.5-turbo终端敏感输出的细节分解 [11] [即使使用适当措辞的系统提示，警告也无法被压制] 。由于制定了管理政策，因此可能会出现这种情况。为了减轻这种情况，可以对自我报告的输出进行消毒，并返回错误代码，以呈现一个有故障的系统的错觉，而不是一个蜜罐。</p></div>`,
                }
            },
        
            {
                primary_col: {
                    header: String.raw`# VI. CONCLUSION`,
                    msg: String.raw`<div class="markdown-body"><p>Generative models such as LLMs have exploded in popularity in non-research sectors since the release of ChatGPT, a conversational large language model. With this explosion comes an exploration by users of alternate use cases that generative models can fulfill by changing their personalities based on the provided prompt. One of the use cases is as an interactive cyber deception asset to learn the tactics, techniques, and procedures (TTPs) of attackers.
An implementation of a honeypot using input curation for generative models to generate terminal output is proposed. The proposed honeypot takes inspiration from other dynamic interaction honeypots in its design, incorporating a context-aware approach to engage with potential threats without increasing risk. This implementation filters past inputs to limit token usage while maintaining interactivity and deception.
The effectiveness of this implementation is evaluated for both singular commands in its output similarity and extended sessions in its ability to maintain deception. This evaluation compared the proposed language model honeypot to a static medium-interaction honeypot of a similar risk level, Cowrie. The proposed honeypot's output was more similar than Cowrie's 70% of the time, with an average similarity score 16% higher. The proposed honeypot compares favorably to a non-curating implementation where the proposed architecture reduced token use by up to 77% by saving only the relevant context.
Section V discusses the limitations encountered and how they may be overcome. While language models offer interesting possibilities as honeypots, they do have limitations. The limitations include responsiveness, nondeterministic output, and non-verifiable output. Further research is needed before honeypots using this technology can be deployed effectively in the wild.
Future work is to be done in the extension of the input curation mechanism. More robust methods of context selection and handling of input edge cases are left for future work. Alternatively, research in the model used for output generation could be explored to find which of a series of different models with different feature sets are the most effective for cyber deception. The development and deployment or extensive tuning of a generative model for the use of cyber deception is also considered to replace the general-purpose model used.
Threat engagement is a constantly evolving issue due to the evolutionary nature of cyberattacks. This requires a technology that can evolve with it and be able to adapt to new issues as they come up. Generative models fulfill this need as one of a still-expanding set of use cases. To this end, the efficient handling of input for these models is paramount and has demonstrated improvements to a base deployment in token use reduction without compromising deception.</p></div>`,
                },
                secondary_rol: {
                    header: String.raw`# VI. CONCLUSION`,
                    msg: String.raw`<div class="markdown-body"><p>VI. 结论</p>
<p>自ChatGPT发布以来，生成模型如LLMs在非研究领域中大受欢迎并得到广泛应用。随着这一趋势的发展，用户开始探索生成模型在改变其个性化响应提示的情况下可以实现的其他用例。其中之一是作为交互式网络诱饵，以了解攻击者的战术、技术和程序（TTPs）。</p>
<p>本文提出了一种利用输入策划生成模型生成终端输出的蜜罐的实现方案。所提出的蜜罐从其他动态交互式蜜罐中汲取灵感，通过上下文感知方法与潜在威胁进行交互，同时不增加风险。该实现方案通过过滤过去的输入来限制标记使用，同时保持互动性和欺骗性。</p>
<p>本实施方案在输出相似性方面进行了评估，包括其在单个命令的输出相似性以及在延长会话中维持欺骗的能力。这项评估将所提出的语言模型蜜罐与风险水平相似的静态中互动蜜罐Cowrie进行了比较。结果显示，所提出的蜜罐在输出相似性方面超过Cowrie的70%，平均相似性得分高出16%。所提出的蜜罐与不进行策划的实施方案相比表现优越，通过仅保存相关上下文，其架构在标记使用方面减少了高达77%。</p>
<p>第五节讨论了所遇到的限制及其可能的克服方法。尽管语言模型作为蜜罐提供了有趣的可能性，但它们确实存在一些限制，包括响应性、非确定性输出和无法验证的输出。在这些技术能够有效部署于实际环境之前，需要进一步的研究。</p>
<p>未来的工作将继续扩展输入策划机制。将更加鲁棒的上下文选择方法和处理输入边缘情况的方法留给未来研究。或者，还可以研究用于输出生成的模型，以找出哪种具有不同特征集的一系列不同模型对于网络欺骗最有效。此外，还考虑开发和部署或广泛调整用于网络欺骗的生成模型，以替代目前使用的通用模型。</p>
<p>由于威胁的不断演变，威胁交互是一个持续进化的问题。这要求一种能够随之演变并能够适应新问题的技术。生成模型作为一种仍在不断扩展其用例的技术之一，满足了这一需求。因此，对于这些模型的输入的高效处理至关重要，并已经证明在减少标记使用的基础部署中提升了欺骗性能。</p></div>`,
                }
            },
        
        ],
        // 控制状态切换的变量
        num: 0
      }
    },
    computed: {
      text() {
        // 对控制状态的变量进行判断处理
        switch (this.num) {
          case 0:
            return '切换英文' //0 ->  英文
            break;
          case 1:
            return '切换中文' // 1-> 中文
            break;
          default:
            return '展示全部' // 其他的 -> '全部'
            break;
        }
      }
    },
    methods: {
      // 切换状态的逻辑
      showStatus() {
        if (this.num >= 2) { //如果变量>=2  就将变量置回去
          this.num = 0
        } else {
          this.num++
        }
      }
    },
  })
</script>
